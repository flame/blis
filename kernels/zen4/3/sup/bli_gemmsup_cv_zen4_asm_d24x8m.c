/*

   BLIS
   An object-based framework for developing high-performance BLAS-like
   libraries.

   Copyright (C) 2023 - 2025, Advanced Micro Devices, Inc. All rights reserved.

   Redistribution and use in source and binary forms, with or without
   modification, are permitted provided that the following conditions are
   met:
    - Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    - Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in the
      documentation and/or other materials provided with the distribution.
    - Neither the name(s) of the copyright holder(s) nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
   HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

*/
#include "blis.h"
#define BLIS_ASM_SYNTAX_ATT
#include "bli_x86_asm_macros.h"
#define TAIL_NITER 3


/**
 * helper macros
*/

/**
 * @brief : PREPARE_SCRATCHPAD
 * Initialize accumulator vector register(zmm6 to zmm29) to zero.
 * Every IR iteration needs clear set of following zmm vector registers
 * to hold FMA result during K loop.
*/
#define PREPARE_SCRATCHPAD          \
    vxorpd(zmm6, zmm6, zmm6)        \
    vmovapd(zmm6, zmm7)             \
    vmovapd(zmm6, zmm28)            \
    vmovapd(zmm6, zmm8)             \
    vmovapd(zmm6, zmm9)             \
    vmovapd(zmm6, zmm29)            \
    vmovapd(zmm6, zmm10)            \
    vmovapd(zmm6, zmm11)            \
    vmovapd(zmm6, zmm26)            \
    vmovapd(zmm6, zmm12)            \
    vmovapd(zmm6, zmm13)            \
    vmovapd(zmm6, zmm27)            \
    vmovapd(zmm6, zmm14)            \
    vmovapd(zmm6, zmm15)            \
    vmovapd(zmm6, zmm24)            \
    vmovapd(zmm6, zmm16)            \
    vmovapd(zmm6, zmm17)            \
    vmovapd(zmm6, zmm25)            \
    vmovapd(zmm6, zmm18)            \
    vmovapd(zmm6, zmm19)            \
    vmovapd(zmm6, zmm22)            \
    vmovapd(zmm6, zmm20)            \
    vmovapd(zmm6, zmm21)            \
    vmovapd(zmm6, zmm23)

/**
 * @brief : PREPARE_M_LOOP
 * m_iter computes how many iterations need to be
 * computed for given m.
 * Loads the address of A and C matrix address into
 * register rax and rcx.
*/
#define PREPARE_M_LOOP                                                            \
    mov(var(m_iter), r11)   /* m_iter loop starts */                              \
    mov(var(a), rax)        /* rax = load address of a matrix */                  \
    mov(var(a), r15)        /* r15 = load address of a matrix */                  \
    mov(var(c), rcx)        /* rcx = load address of c matrix */                  \
    cmp(imm(0), r11)        /* check i via logical AND */                         \
    je(.MLEFT)              /* jump to m_left case */                             \
    label(.M_ITER_LOOP)                                                           \
    mov(r11, var(m_iter))   /* m_iter is updated with updated iterations of m */  \
    mov(var(a), rax)        /* restore updated A matrix offset to rax */          \

/**
 * @brief : PRE_K_LOOP
 * Here it loads all the necessary parameters and matrix
 * offsets needed for computing GEMM, prefetching into
 * general purpose registers.
*/
#define PRE_K_LOOP                                                                              \
    mov(var(cs_a0), r10)        /* r10 = column stride of a matrix; r10 = cs_a0 */              \
    mov(var(b), rbx)            /* rbx = load address of b matrix */                            \
    mov(var(rs_b0), r8)         /* r8 = row stride of b matrix; r8 = rs_b0 */                   \
    mov(var(cs_b0), r9)         /* r9 = column stride of b matrix; r9 = cs_b0 */                \
    mov(var(cs_c0), rdi)        /* rdi = column stride of c matrix; rdi = cs_c0 */              \
    lea(mem(, r8, 8), r8)       /* r8 = row stride of b matrix in bytes; r8 = rs_b0 * 8 */      \
    lea(mem(, r9, 8), r9)       /* r9 = column stride of b matrix in bytes; r9 = cs_b0 * 8 */   \
    lea(mem(, r10, 8), r10)     /* r10 = column stride of a matrix in bytes; r10 = cs_a0 * 8 */ \
    lea(mem(, rdi, 8), rdi)     /* rdi = column stride of c matrix in bytes; rdi = cs_c0 * 8 */ \
    lea(mem(r9, r9, 2 ), r13)   /* r13 = offset to 3rd column b matrix in bytes */              \
    lea(mem(rbx, r9, 4), r12)   /* r12 = 4th column b matrix in bytes */                        \
                                /* r12 = b + 4*cs_b */                                          \
                                                                                                \
    /* - Calculate the address for prefetching C into rdx.*/                                    \
    /* - rcx: Base address of matrix C.*/                                                       \
    /* - 7*8: Offset to the next set of elements for prefetching */                             \
    /* (assuming each element is 8 bytes and we are moving 7 elements ahead).*/                 \
    lea(mem(rcx, 7*8), rdx)

/**
 * @brief : SHUFFLE_DATA
 * Shuffle 2 double-precision elements selected by imm8 from S1 and S2,
 * and store the results in D1
 * S1 : 1  9 3 11 5 13 7 15
 * S2 : 2 10 4 12 6 14 8 16
 * D1 : 1  9  5  13  2  10  6  14
 * D2 : 3 11  7  15  4  12  8  16
*/
#define SHUFFLE_DATA(S1, S2, D1, D2, S3, S4, D3, D4) \
    VSHUFF64X2(IMM(0x88), ZMM(S1), ZMM(S2), ZMM(D1)) \
    VSHUFF64X2(IMM(0xDD), ZMM(S1), ZMM(S2), ZMM(D2)) \
    VSHUFF64X2(IMM(0x88), ZMM(S3), ZMM(S4), ZMM(D3)) \
    VSHUFF64X2(IMM(0xDD), ZMM(S3), ZMM(S4), ZMM(D4)) \

/**
 * @brief : UNPACK_LO_HIGH
 * Unpacks and interleave low half and high half of each
 * 128-bit lane in S1 and S2 and store into D1 and D2
 * respectively.
 * S1 : 1  2  3  4  5  6  7  8
 * S2 : 9 10 11 12 13 14 15 16
 * D1 : 1  9 3 11 5 13 7 15
 * D2 : 2 10 4 12 6 14 8 16
*/
#define UNPACK_LO_HIGH(S1, S2, D1, D2, S3, S4, D3, D4)  \
    vunpcklpd( zmm(S1),  zmm(S2),  zmm(D1))             \
    vunpckhpd( zmm(S1),  zmm(S2),  zmm(D2))             \
    vunpcklpd( zmm(S3),  zmm(S4),  zmm(D3))             \
    vunpckhpd( zmm(S3),  zmm(S4),  zmm(D4))

/**
 * @brief : UPDATE_C
 * Loads elements from C rows, Scales it with Beta
 * and adds FMA result to it.
 * Stores back the C rows.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0, zmm4, zmm2, zmm6, zmm1, zmm5, zmm3, zmm8 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
 * Stores result Beta * C[1,0] + zmm4(Alpha * A * B) back to C[1,0]
 * Stores result Beta * C[2,0] + zmm2(Alpha * A * B) back to C[2,0]
 * Stores result Beta * C[3,0] + zmm6(Alpha * A * B) back to C[3,0]
 * Stores result Beta * C[4,0] + zmm1(Alpha * A * B) back to C[4,0]
 * Stores result Beta * C[5,0] + zmm5(Alpha * A * B) back to C[5,0]
 * Stores result Beta * C[6,0] + zmm3(Alpha * A * B) back to C[6,0]
 * Stores result Beta * C[7,0] + zmm8(Alpha * A * B) back to C[7,0]
 */
#define UPDATE_C                                                                        \
    vfmadd231pd( mem(rcx), zmm31, zmm0 )                                                \
    vmovupd( zmm0, (rcx) )                          /* Stores result back to C[0,0] */  \
    vfmadd231pd( mem(rcx, rsi, 1), zmm31, zmm4 )                                        \
    vmovupd( zmm4, (rcx, rsi, 1) )                  /* Stores result back to C[1,0] */  \
    vfmadd231pd( mem(rcx, rsi, 2), zmm31, zmm2 )                                        \
    vmovupd( zmm2, (rcx, rsi, 2) )                  /* Stores result back to C[2,0] */  \
    vfmadd231pd( mem(rcx, r12, 1), zmm31, zmm6 )                                        \
    vmovupd( zmm6, (rcx, r12, 1) )                  /* Stores result back to C[3,0] */  \
    vfmadd231pd( mem(rcx, rsi, 4), zmm31, zmm1 )                                        \
    vmovupd( zmm1, (rcx, rsi, 4) )                  /* Stores result back to C[4,0] */  \
    vfmadd231pd( mem(rcx, r13, 1), zmm31, zmm5 )                                        \
    vmovupd( zmm5, (rcx, r13, 1) )                  /* Stores result back to C[5,0] */  \
    vfmadd231pd( mem(rcx, r12, 2), zmm31, zmm3 )                                        \
    vmovupd( zmm3, (rcx, r12, 2) )                  /* Stores result back to C[6,0] */  \
    vfmadd231pd( mem(rcx, rdx, 1), zmm31, zmm8 )                                        \
    vmovupd( zmm8, (rcx, rdx, 1) )                  /* Stores result back to C[7,0] */  \
    add(r14, rcx)


/**
 * @brief : UPDATE_C_BZ
 * stores FMA result to C rows.
 *
 * rcx = Address of C matrix
 * zmm0, zmm4, zmm2, zmm6, zmm1, zmm5, zmm3, zmm8 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
 * Stores result zmm4(Alpha * A * B) back to C[1,0]
 * Stores result zmm2(Alpha * A * B) back to C[2,0]
 * Stores result zmm6(Alpha * A * B) back to C[3,0]
 * Stores result zmm1(Alpha * A * B) back to C[4,0]
 * Stores result zmm5(Alpha * A * B) back to C[5,0]
 * Stores result zmm3(Alpha * A * B) back to C[6,0]
 * Stores result zmm8(Alpha * A * B) back to C[7,0]
 */
#define UPDATE_C_BZ                                     \
    vmovupd( zmm0, (rcx) )                              \
    vmovupd( zmm4, (rcx, rsi, 1) )                      \
    vmovupd( zmm2, (rcx, rsi, 2) )                      \
    vmovupd( zmm6, (rcx, r12, 1) )                      \
    vmovupd( zmm1, (rcx, rsi, 4) )                      \
    vmovupd( zmm5, (rcx, r13, 1) )                      \
    vmovupd( zmm3, (rcx, r12, 2) )                      \
    vmovupd( zmm8, (rcx, rdx, 1) )                      \
    add(r14, rcx)

/**
 * @brief : UPDATE_MASKED_C
 * Loads elements from C row only if correspondnig bits in
 * mask register is set, Scales it with Beta and adds FMA result to it
 * Stores back the C rows.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0, zmm4, zmm2, zmm6, zmm1, zmm5, zmm3, zmm8 holds Alpha*A*B
 * of corresponding rows of C.
 * MASK_KZ(2) = masked for loading-storing of n-left elements.
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 * r14 = column stride of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
 * Stores result Beta * C[1,0] + zmm4(Alpha * A * B) back to C[1,0]
 * Stores result Beta * C[2,0] + zmm2(Alpha * A * B) back to C[2,0]
 * Stores result Beta * C[3,0] + zmm6(Alpha * A * B) back to C[3,0]
 * Stores result Beta * C[4,0] + zmm1(Alpha * A * B) back to C[4,0]
 * Stores result Beta * C[5,0] + zmm5(Alpha * A * B) back to C[5,0]
 * Stores result Beta * C[6,0] + zmm3(Alpha * A * B) back to C[6,0]
 * Stores result Beta * C[7,0] + zmm8(Alpha * A * B) back to C[7,0]
*/
#define UPDATE_MASKED_C                                 \
    vmovupd( mem(rcx), zmm30 MASK_KZ(2) )               \
    vfmadd231pd( zmm31, zmm30, zmm0 )                   \
    vmovupd( mem(rcx, rsi, 1, 0), zmm10 MASK_KZ(2) )    \
    vfmadd231pd( zmm31, zmm10, zmm4 )                   \
    vmovupd( mem(rcx, rsi, 2, 0), zmm12 MASK_KZ(2) )    \
    vfmadd231pd( zmm31, zmm12, zmm2 )                   \
    vmovupd( mem(rcx, r12, 1, 0), zmm16 MASK_KZ(2) )    \
    vfmadd231pd( zmm31, zmm16, zmm6 )                   \
    vmovupd( mem(rcx, rsi, 4, 0), zmm14 MASK_KZ(2) )    \
    vfmadd231pd( zmm31, zmm14, zmm1 )                   \
    vmovupd( mem(rcx, r13, 1, 0), zmm18 MASK_KZ(2) )    \
    vfmadd231pd( zmm31, zmm18, zmm5 )                   \
    vmovupd( mem(rcx, r12, 2, 0), zmm10 MASK_KZ(2) )    \
    vfmadd231pd( zmm31, zmm10, zmm3 )                   \
    vmovupd( mem(rcx, rdx, 1, 0), zmm12 MASK_KZ(2) )    \
    vfmadd231pd( zmm31, zmm12, zmm8 )                   \
                                                        \
    vmovupd( zmm0, (rcx) MASK_(k(2)))                   \
    vmovupd( zmm4, (rcx, rsi, 1) MASK_(k(2)))           \
    vmovupd( zmm2, (rcx, rsi, 2) MASK_(k(2)))           \
    vmovupd( zmm6, (rcx, r12, 1) MASK_(k(2)))           \
    vmovupd( zmm1, (rcx, rsi, 4) MASK_(k(2)))           \
    vmovupd( zmm5, (rcx, r13, 1) MASK_(k(2)))           \
    vmovupd( zmm3, (rcx, r12, 2) MASK_(k(2)))           \
    vmovupd( zmm8, (rcx, rdx, 1) MASK_(k(2)))           \
    add(r14, rcx)

/**
 * @brief : UPDATE_MASKED_C_BZ
 * mask register is set, stores FMA result to C.
 *
 * rcx = Address of C matrix
 * zmm0, zmm4, zmm2, zmm6, zmm1, zmm5, zmm3, zmm8 holds Alpha*A*B
 * of corresponding rows of C.
 * MASK_KZ(2) = masked for loading-storing of n-left elements.
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 * r14 = column stride of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
 * Stores result zmm4(Alpha * A * B) back to C[1,0]
 * Stores result zmm2(Alpha * A * B) back to C[2,0]
 * Stores result zmm6(Alpha * A * B) back to C[3,0]
 * Stores result zmm1(Alpha * A * B) back to C[4,0]
 * Stores result zmm5(Alpha * A * B) back to C[5,0]
 * Stores result zmm3(Alpha * A * B) back to C[6,0]
 * Stores result zmm8(Alpha * A * B) back to C[7,0]
*/
#define UPDATE_MASKED_C_BZ                          \
    vmovupd( zmm0, mem(rcx) MASK_(k(2)))            \
    vmovupd( zmm4, mem(rcx, rsi, 1) MASK_(k(2)))    \
    vmovupd( zmm2, mem(rcx, rsi, 2) MASK_(k(2)) )   \
    vmovupd( zmm6, mem(rcx, r12, 1) MASK_(k(2)) )   \
    vmovupd( zmm1, mem(rcx, rsi, 4) MASK_(k(2)))    \
    vmovupd( zmm5, mem(rcx, r13, 1) MASK_(k(2)))    \
    vmovupd( zmm3, mem(rcx, r12, 2) MASK_(k(2)))    \
    vmovupd( zmm8, mem(rcx, rdx, 1) MASK_(k(2)))    \
    add(r14, rcx)

/**
 * @brief : UPDATE_C_8_BZ
 * Stores fma result back to C
 *
 * rcx = Address of C matrix
 * zmm0, zmm4, zmm2, zmm6, zmm1, zmm5, zmm3, zmm8 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
 * Stores result zmm4(Alpha * A * B) back to C[1,0]
 * Stores result zmm2(Alpha * A * B) back to C[2,0]
 * Stores result zmm6(Alpha * A * B) back to C[3,0]
 * Stores result zmm1(Alpha * A * B) back to C[4,0]
 * Stores result zmm5(Alpha * A * B) back to C[5,0]
 * Stores result zmm3(Alpha * A * B) back to C[6,0]
 * Stores result zmm8(Alpha * A * B) back to C[7,0]
*/
#define UPDATE_C_8_BZ \
    vmovupd( zmm0, (rcx) )              \
    vmovupd( zmm4, (rcx, rsi, 1) )      \
    vmovupd( zmm2, (rcx, rsi, 2) )      \
    vmovupd( zmm6, (rcx, r12, 1) )      \
    vmovupd( zmm1, (rcx, rsi, 4) )      \
    vmovupd( zmm5, (rcx, r13, 1) )      \
    vmovupd( zmm3, (rcx, r12, 2) )      \
    vmovupd( zmm8, (rcx, rdx, 1) )      \
    add(r14, rcx)

/**
 * @brief : UPDATE_C_7_BZ
 * Stores fma result back to C
 *
 * rcx = Address of C matrix
 * zmm0, zmm4, zmm2, zmm6, zmm1, zmm5, zmm3 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
 * Stores result zmm4(Alpha * A * B) back to C[1,0]
 * Stores result zmm2(Alpha * A * B) back to C[2,0]
 * Stores result zmm6(Alpha * A * B) back to C[3,0]
 * Stores result zmm1(Alpha * A * B) back to C[4,0]
 * Stores result zmm5(Alpha * A * B) back to C[5,0]
 * Stores result zmm3(Alpha * A * B) back to C[6,0]
*/
#define UPDATE_C_7_BZ \
    vmovupd( zmm0, (rcx) )          \
    vmovupd( zmm4, (rcx, rsi, 1) )  \
    vmovupd( zmm2, (rcx, rsi, 2) )  \
    vmovupd( zmm6, (rcx, r12, 1) )  \
    vmovupd( zmm1, (rcx, rsi, 4) )  \
    vmovupd( zmm5, (rcx, r13, 1) )  \
    vmovupd( zmm3, (rcx, r12, 2) )

/**
 * @brief : UPDATE_C_6_BZ
 * Stores fma result back to C
 *
 * rcx = Address of C matrix
 * zmm0, zmm4, zmm2, zmm6, zmm1, zmm5 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
 * Stores result zmm4(Alpha * A * B) back to C[1,0]
 * Stores result zmm2(Alpha * A * B) back to C[2,0]
 * Stores result zmm6(Alpha * A * B) back to C[3,0]
 * Stores result zmm1(Alpha * A * B) back to C[4,0]
 * Stores result zmm5(Alpha * A * B) back to C[5,0]
*/
#define UPDATE_C_6_BZ \
    vmovupd( zmm0, (rcx) )              \
    vmovupd( zmm4, (rcx, rsi, 1) )      \
    vmovupd( zmm2, (rcx, rsi, 2) )      \
    vmovupd( zmm6, (rcx, r12, 1) )      \
    vmovupd( zmm1, (rcx, rsi, 4) )      \
    vmovupd( zmm5, (rcx, r13, 1) )

/**
 * @brief : UPDATE_C_5_BZ
 * Stores fma result back to C
 *
 * rcx = Address of C matrix
 * zmm0, zmm4, zmm2, zmm6, zmm1 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
 * Stores result zmm4(Alpha * A * B) back to C[1,0]
 * Stores result zmm2(Alpha * A * B) back to C[2,0]
 * Stores result zmm6(Alpha * A * B) back to C[3,0]
 * Stores result zmm1(Alpha * A * B) back to C[4,0]
*/
#define UPDATE_C_5_BZ \
    vmovupd( zmm0, (rcx) )          \
    vmovupd( zmm4, (rcx, rsi, 1) )  \
    vmovupd( zmm2, (rcx, rsi, 2) )  \
    vmovupd( zmm6, (rcx, r12, 1) )  \
    vmovupd( zmm1, (rcx, rsi, 4) )

/**
 * @brief : UPDATE_C_4_BZ
 * Stores fma result back to C
 *
 * rcx = Address of C matrix
 * zmm0, zmm4, zmm2, zmm6 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
 * Stores result zmm4(Alpha * A * B) back to C[1,0]
 * Stores result zmm2(Alpha * A * B) back to C[2,0]
 * Stores result zmm6(Alpha * A * B) back to C[3,0]
*/
#define UPDATE_C_4_BZ \
    vmovupd( zmm0, (rcx) )          \
    vmovupd( zmm4, (rcx, rsi, 1) )  \
    vmovupd( zmm2, (rcx, rsi, 2) )  \
    vmovupd( zmm6, (rcx, r12, 1) )

/**
 * @brief : UPDATE_C_3_BZ
 * Stores fma result back to C
 *
 * rcx = Address of C matrix
 * zmm0, zmm4, zmm2 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
 * Stores result zmm4(Alpha * A * B) back to C[1,0]
 * Stores result zmm2(Alpha * A * B) back to C[2,0]
*/
#define UPDATE_C_3_BZ \
    vmovupd( zmm0, (rcx) )              \
    vmovupd( zmm4, (rcx, rsi, 1) )      \
    vmovupd( zmm2, (rcx, rsi, 2) )

/**
 * @brief : UPDATE_C_2_BZ
 * Stores fma result back to C
 *
 * rcx = Address of C matrix
 * zmm0, zmm4 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
 * Stores result zmm4(Alpha * A * B) back to C[1,0]
*/
#define UPDATE_C_2_BZ \
    vmovupd( zmm0, (rcx) )              \
    vmovupd( zmm4, (rcx, rsi, 1) )

/**
 * @brief : UPDATE_C_1_BZ
 * Stores fma result back to C
 *
 * rcx = Address of C matrix
 * zmm0 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
*/
#define UPDATE_C_1_BZ \
    vmovupd( zmm0, (rcx) )              \

/**
 * @brief : UPDATE_C_8
 * Loads elements from C row, Scales it with Beta
 * and adds FMA result to it.
 * Stores back the C row.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0, zmm4, zmm2, zmm6, zmm1, zmm5, zmm3, zmm8 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
 * Stores result Beta * C[1,0] + zmm4(Alpha * A * B) back to C[1,0]
 * Stores result Beta * C[2,0] + zmm2(Alpha * A * B) back to C[2,0]
 * Stores result Beta * C[3,0] + zmm6(Alpha * A * B) back to C[3,0]
 * Stores result Beta * C[4,0] + zmm1(Alpha * A * B) back to C[4,0]
 * Stores result Beta * C[5,0] + zmm5(Alpha * A * B) back to C[5,0]
 * Stores result Beta * C[6,0] + zmm3(Alpha * A * B) back to C[6,0]
 * Stores result Beta * C[7,0] + zmm8(Alpha * A * B) back to C[7,0]
*/
#define UPDATE_C_8                                  \
    vfmadd231pd( mem(rcx), zmm31,zmm0 )             \
    vmovupd( zmm0, (rcx) )                          \
    vfmadd231pd( mem(rcx, rsi, 1), zmm31, zmm4 )    \
    vmovupd( zmm4, (rcx, rsi, 1) )                  \
    vfmadd231pd( mem(rcx, rsi, 2), zmm31, zmm2 )    \
    vmovupd( zmm2, (rcx, rsi, 2) )                  \
    vfmadd231pd( mem(rcx, r12, 1), zmm31, zmm6 )    \
    vmovupd( zmm6, (rcx, r12, 1) )                  \
    vfmadd231pd( mem(rcx, rsi, 4), zmm31, zmm1 )    \
    vmovupd( zmm1, (rcx, rsi, 4) )                  \
    vfmadd231pd( mem(rcx, r13, 1), zmm31, zmm5 )    \
    vmovupd( zmm5, (rcx, r13, 1) )                  \
    vfmadd231pd( mem(rcx, r12, 2), zmm31, zmm3 )    \
    vmovupd( zmm3, (rcx, r12, 2) )                  \
    vfmadd231pd( mem(rcx, rdx, 1), zmm31, zmm8 )    \
    vmovupd( zmm8, (rcx, rdx, 1) )                  \
    add(r14, rcx)

/**
 * @brief : UPDATE_C_7
 * Loads elements from C row, Scales it with Beta
 * and adds FMA result to it.
 * Stores back the C row.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0, zmm4, zmm2, zmm6, zmm1, zmm5, zmm3 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
 * Stores result Beta * C[1,0] + zmm4(Alpha * A * B) back to C[1,0]
 * Stores result Beta * C[2,0] + zmm2(Alpha * A * B) back to C[2,0]
 * Stores result Beta * C[3,0] + zmm6(Alpha * A * B) back to C[3,0]
 * Stores result Beta * C[4,0] + zmm1(Alpha * A * B) back to C[4,0]
 * Stores result Beta * C[5,0] + zmm5(Alpha * A * B) back to C[5,0]
 * Stores result Beta * C[6,0] + zmm3(Alpha * A * B) back to C[6,0]
*/
#define UPDATE_C_7 \
    vfmadd231pd( mem(rcx), zmm31, zmm0 )            \
    vmovupd( zmm0, (rcx) )                          \
    vfmadd231pd( mem(rcx, rsi, 1), zmm31, zmm4 )    \
    vmovupd( zmm4, (rcx, rsi, 1) )                  \
    vfmadd231pd( mem(rcx, rsi, 2), zmm31, zmm2 )    \
    vmovupd( zmm2, (rcx, rsi, 2) )                  \
    vfmadd231pd( mem(rcx, r12, 1), zmm31, zmm6 )    \
    vmovupd( zmm6, (rcx, r12, 1) )                  \
    vfmadd231pd( mem(rcx, rsi, 4), zmm31, zmm1 )    \
    vmovupd( zmm1, (rcx, rsi, 4) )                  \
    vfmadd231pd( mem(rcx, r13, 1), zmm31, zmm5 )    \
    vmovupd( zmm5, (rcx, r13, 1) )                  \
    vfmadd231pd( mem(rcx, r12, 2), zmm31, zmm3 )    \
    vmovupd( zmm3, (rcx, r12, 2) )

/**
 * @brief : UPDATE_C_6
 * Loads elements from C row, Scales it with Beta
 * and adds FMA result to it.
 * Stores back the C row.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0, zmm4, zmm2, zmm6, zmm1, zmm5 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
 * Stores result Beta * C[1,0] + zmm4(Alpha * A * B) back to C[1,0]
 * Stores result Beta * C[2,0] + zmm2(Alpha * A * B) back to C[2,0]
 * Stores result Beta * C[3,0] + zmm6(Alpha * A * B) back to C[3,0]
 * Stores result Beta * C[4,0] + zmm1(Alpha * A * B) back to C[4,0]
 * Stores result Beta * C[5,0] + zmm5(Alpha * A * B) back to C[5,0]
*/
#define UPDATE_C_6 \
    vfmadd231pd( mem(rcx), zmm31,zmm0 )             \
    vmovupd( zmm0, (rcx) )                          \
    vfmadd231pd( mem(rcx, rsi, 1), zmm31, zmm4 )    \
    vmovupd( zmm4, (rcx, rsi, 1) )                  \
    vfmadd231pd( mem(rcx, rsi, 2), zmm31, zmm2 )    \
    vmovupd( zmm2, (rcx, rsi, 2) )                  \
    vfmadd231pd( mem(rcx, r12, 1), zmm31, zmm6 )    \
    vmovupd( zmm6, (rcx, r12, 1) )                  \
    vfmadd231pd( mem(rcx, rsi, 4), zmm31, zmm1 )    \
    vmovupd( zmm1, (rcx, rsi, 4) )                  \
    vfmadd231pd( mem(rcx, r13, 1), zmm31, zmm5 )    \
    vmovupd( zmm5, (rcx, r13, 1) )

/**
 * @brief : UPDATE_C_5
 * Loads elements from C row, Scales it with Beta
 * and adds FMA result to it.
 * Stores back the C row.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0, zmm4, zmm2, zmm6, zmm1 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
 * Stores result Beta * C[1,0] + zmm4(Alpha * A * B) back to C[1,0]
 * Stores result Beta * C[2,0] + zmm2(Alpha * A * B) back to C[2,0]
 * Stores result Beta * C[3,0] + zmm6(Alpha * A * B) back to C[3,0]
 * Stores result Beta * C[4,0] + zmm1(Alpha * A * B) back to C[4,0]
*/
#define UPDATE_C_5 \
\
    vfmadd231pd( mem(rcx), zmm31, zmm0 )                \
    vmovupd( zmm0, (rcx) )                              \
    vfmadd231pd( mem(rcx, rsi, 1), zmm31, zmm4 )        \
    vmovupd( zmm4, (rcx, rsi, 1) )                      \
    vfmadd231pd( mem(rcx, rsi, 2), zmm31, zmm2 )        \
    vmovupd( zmm2, (rcx, rsi, 2) )                      \
    vfmadd231pd( mem(rcx, r12, 1), zmm31, zmm6 )        \
    vmovupd( zmm6, (rcx, r12, 1) )                      \
    vfmadd231pd( mem(rcx, rsi, 4), zmm31, zmm1 )        \
    vmovupd( zmm1, (rcx, rsi, 4) )

/**
 * @brief : UPDATE_C_4
 * Loads elements from C row, Scales it with Beta
 * and adds FMA result to it.
 * Stores back the C row.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0, zmm4, zmm2, zmm6 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
 * Stores result Beta * C[1,0] + zmm4(Alpha * A * B) back to C[1,0]
 * Stores result Beta * C[2,0] + zmm2(Alpha * A * B) back to C[2,0]
 * Stores result Beta * C[3,0] + zmm6(Alpha * A * B) back to C[3,0]
*/
#define UPDATE_C_4 \
    vfmadd231pd( mem(rcx), zmm31, zmm0 )            \
    vmovupd( zmm0, (rcx) )                          \
    vfmadd231pd( mem(rcx, rsi, 1), zmm31, zmm4 )    \
    vmovupd( zmm4, (rcx, rsi, 1) )                  \
    vfmadd231pd( mem(rcx, rsi, 2), zmm31, zmm2 )    \
    vmovupd( zmm2, (rcx, rsi, 2) )                  \
    vfmadd231pd( mem(rcx, r12, 1), zmm31, zmm6 )    \
    vmovupd( zmm6, (rcx, r12, 1) )

/**
 * @brief : UPDATE_C_3
 * Loads elements from C row, Scales it with Beta
 * and adds FMA result to it.
 * Stores back the C row.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0, zmm4, zmm2 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
 * Stores result Beta * C[1,0] + zmm4(Alpha * A * B) back to C[1,0]
 * Stores result Beta * C[2,0] + zmm2(Alpha * A * B) back to C[2,0]
*/
#define UPDATE_C_3 \
    vfmadd231pd( mem(rcx), zmm31, zmm0 )            \
    vmovupd( zmm0, (rcx) )                          \
    vfmadd231pd( mem(rcx, rsi, 1), zmm31, zmm4 )    \
    vmovupd( zmm4, (rcx, rsi, 1) )                  \
    vfmadd231pd( mem(rcx, rsi, 2), zmm31, zmm2 )    \
    vmovupd( zmm2, (rcx, rsi, 2) )

/**
 * @brief : UPDATE_C_2
 * Loads elements from C row, Scales it with Beta
 * and adds FMA result to it.
 * Stores back the C row.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0, zmm4 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
 * Stores result Beta * C[1,0] + zmm4(Alpha * A * B) back to C[1,0]
*/
#define UPDATE_C_2 \
    vfmadd231pd( mem(rcx), zmm31, zmm0 )                \
    vmovupd( zmm0, (rcx) )                              \
    vfmadd231pd( mem(rcx, rsi, 1), zmm31, zmm4 )        \
    vmovupd( zmm4, (rcx, rsi, 1) )

/**
 * @brief : UPDATE_C_1
 * Loads elements from C row, Scales it with Beta
 * and adds FMA result to it.
 * Stores back the C row.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
*/
#define UPDATE_C_1 \
    vfmadd231pd( mem(rcx), zmm31, zmm0 )     \
    vmovupd( zmm0, (rcx) )


/**
 * @brief : UPDATE_MASKED_C_8_BZ
 * mask register is set, stores the fma result back to C
 *
 * rcx = Address of C matrix
 * zmm0, zmm4, zmm2, zmm6, zmm1, zmm5, zmm3, zmm8 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
 * Stores result zmm4(Alpha * A * B) back to C[1,0]
 * Stores result zmm2(Alpha * A * B) back to C[2,0]
 * Stores result zmm6(Alpha * A * B) back to C[3,0]
 * Stores result zmm1(Alpha * A * B) back to C[4,0]
 * Stores result zmm5(Alpha * A * B) back to C[5,0]
 * Stores result zmm3(Alpha * A * B) back to C[6,0]
 * Stores result zmm8(Alpha * A * B) back to C[7,0]
*/
#define UPDATE_MASKED_C_8_BZ \
    vmovupd( zmm0, mem(rcx) MASK_(k(3)))            \
    vmovupd( zmm4, mem(rcx, rsi, 1) MASK_(k(3)))    \
    vmovupd( zmm2, mem(rcx, rsi, 2) MASK_(k(3)) )   \
    vmovupd( zmm6, mem(rcx, r12, 1) MASK_(k(3)) )   \
    vmovupd( zmm1, mem(rcx, rsi, 4) MASK_(k(3)))    \
    vmovupd( zmm5, mem(rcx, r13, 1) MASK_(k(3)))    \
    vmovupd( zmm3, mem(rcx, r12, 2) MASK_(k(3)))    \
    vmovupd( zmm8, mem(rcx, rdx, 1) MASK_(k(3)))    \
    add(r14, rcx)

/**
 * @brief : UPDATE_MASKED_C_7_BZ
 * mask register is set, stores the fma result back to C
 *
 * rcx = Address of C matrix
 * zmm0, zmm4, zmm2, zmm6, zmm1, zmm5, zmm3 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
 * Stores result zmm4(Alpha * A * B) back to C[1,0]
 * Stores result zmm2(Alpha * A * B) back to C[2,0]
 * Stores result zmm6(Alpha * A * B) back to C[3,0]
 * Stores result zmm1(Alpha * A * B) back to C[4,0]
 * Stores result zmm5(Alpha * A * B) back to C[5,0]
 * Stores result zmm3(Alpha * A * B) back to C[6,0]
*/
#define UPDATE_MASKED_C_7_BZ \
    vmovupd( zmm0, mem(rcx) MASK_(k(3)))            \
    vmovupd( zmm4, mem(rcx, rsi, 1) MASK_(k(3)))    \
    vmovupd( zmm2, mem(rcx, rsi, 2) MASK_(k(3)) )   \
    vmovupd( zmm6, mem(rcx, r12, 1) MASK_(k(3)) )   \
    vmovupd( zmm1, mem(rcx, rsi, 4) MASK_(k(3)))    \
    vmovupd( zmm5, mem(rcx, r13, 1) MASK_(k(3)))    \
    vmovupd( zmm3, mem(rcx, r12, 2) MASK_(k(3)))

/**
 * @brief : UPDATE_MASKED_C_6_BZ
 * mask register is set, stores the fma result back to C
 *
 * rcx = Address of C matrix
 * zmm0, zmm4, zmm2, zmm6, zmm1, zmm5 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
 * Stores result zmm4(Alpha * A * B) back to C[1,0]
 * Stores result zmm2(Alpha * A * B) back to C[2,0]
 * Stores result zmm6(Alpha * A * B) back to C[3,0]
 * Stores result zmm1(Alpha * A * B) back to C[4,0]
 * Stores result zmm5(Alpha * A * B) back to C[5,0]
*/
#define UPDATE_MASKED_C_6_BZ \
    vmovupd( zmm0, mem(rcx) MASK_(k(3)))            \
    vmovupd( zmm4, mem(rcx, rsi, 1) MASK_(k(3)))    \
    vmovupd( zmm2, mem(rcx, rsi, 2) MASK_(k(3)) )   \
    vmovupd( zmm6, mem(rcx, r12, 1) MASK_(k(3)) )   \
    vmovupd( zmm1, mem(rcx, rsi, 4) MASK_(k(3)))    \
    vmovupd( zmm5, mem(rcx, r13, 1) MASK_(k(3)))

/**
 * @brief : UPDATE_MASKED_C_5_BZ
 * mask register is set, stores the fma result back to C
 *
 * rcx = Address of C matrix
 * zmm0, zmm4, zmm2, zmm6, zmm1 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
 * Stores result zmm4(Alpha * A * B) back to C[1,0]
 * Stores result zmm2(Alpha * A * B) back to C[2,0]
 * Stores result zmm6(Alpha * A * B) back to C[3,0]
 * Stores result zmm1(Alpha * A * B) back to C[4,0]
*/
#define UPDATE_MASKED_C_5_BZ \
    vmovupd( zmm0, mem(rcx) MASK_(k(3)))            \
    vmovupd( zmm4, mem(rcx, rsi, 1) MASK_(k(3)))    \
    vmovupd( zmm2, mem(rcx, rsi, 2) MASK_(k(3)) )   \
    vmovupd( zmm6, mem(rcx, r12, 1) MASK_(k(3)) )   \
    vmovupd( zmm1, mem(rcx, rsi, 4) MASK_(k(3)))

/**
 * @brief : UPDATE_MASKED_C_4_BZ
 * mask register is set, stores the fma result back to C
 *
 * rcx = Address of C matrix
 * zmm0, zmm4, zmm2, zmm6 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
 * Stores result zmm4(Alpha * A * B) back to C[1,0]
 * Stores result zmm2(Alpha * A * B) back to C[2,0]
 * Stores result zmm6(Alpha * A * B) back to C[3,0]
*/
#define UPDATE_MASKED_C_4_BZ \
    vmovupd( zmm0, mem(rcx) MASK_(k(3)))            \
    vmovupd( zmm4, mem(rcx, rsi, 1) MASK_(k(3)))    \
    vmovupd( zmm2, mem(rcx, rsi, 2) MASK_(k(3)) )   \
    vmovupd( zmm6, mem(rcx, r12, 1) MASK_(k(3)) )

/**
 * @brief : UPDATE_MASKED_C_3_BZ
 * mask register is set, stores the fma result back to C
 *
 * rcx = Address of C matrix
 * zmm0, zmm4, zmm2 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
 * Stores result zmm4(Alpha * A * B) back to C[1,0]
 * Stores result zmm2(Alpha * A * B) back to C[2,0]
*/
#define UPDATE_MASKED_C_3_BZ \
    vmovupd( zmm0, mem(rcx) MASK_(k(3)))            \
    vmovupd( zmm4, mem(rcx, rsi, 1) MASK_(k(3)))    \
    vmovupd( zmm2, mem(rcx, rsi, 2) MASK_(k(3)) )

/**
 * @brief : UPDATE_MASKED_C_2_BZ
 * mask register is set, stores the fma result back to C
 *
 * rcx = Address of C matrix
 * zmm0, zmm4 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
 * Stores result zmm4(Alpha * A * B) back to C[1,0]
*/
#define UPDATE_MASKED_C_2_BZ \
    vmovupd( zmm0, mem(rcx) MASK_(k(3)))            \
    vmovupd( zmm4, mem(rcx, rsi, 1) MASK_(k(3)))

/**
 * @brief : UPDATE_MASKED_C_1_BZ
 * mask register is set, stores the fma result back to C
 *
 * rcx = Address of C matrix
 * zmm0 holds Alpha*A*B
 * of corresponding rows of C.
 *
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result zmm0(Alpha * A * B) back to C[0,0]
*/
#define UPDATE_MASKED_C_1_BZ \
    vmovupd( zmm0, mem(rcx) MASK_(k(3)))

/**
 * @brief : UPDATE_MASKED_C_8
 * Loads elements from C row only if correspondnig bits in
 * mask register is set, Scales it with Beta and adds FMA result to it.
 * Stores back the C row.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0, zmm4, zmm2, zmm6, zmm1, zmm5, zmm3, zmm8 holds Alpha*A*B
 * of corresponding rows of C.
 * MASK_KZ(3) = masked for loading-storing of n-left elements.
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 * r14 = column stride of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
 * Stores result Beta * C[1,0] + zmm4(Alpha * A * B) back to C[1,0]
 * Stores result Beta * C[2,0] + zmm2(Alpha * A * B) back to C[2,0]
 * Stores result Beta * C[3,0] + zmm6(Alpha * A * B) back to C[3,0]
 * Stores result Beta * C[4,0] + zmm1(Alpha * A * B) back to C[4,0]
 * Stores result Beta * C[5,0] + zmm5(Alpha * A * B) back to C[5,0]
 * Stores result Beta * C[6,0] + zmm3(Alpha * A * B) back to C[6,0]
 * Stores result Beta * C[7,0] + zmm8(Alpha * A * B) back to C[7,0]
*/
#define UPDATE_MASKED_C_8 \
    vmovupd( mem(rcx), zmm30 MASK_KZ(3) )               \
    vfmadd231pd( zmm31, zmm30, zmm0 )                   \
    vmovupd( mem(rcx, rsi, 1, 0), zmm10 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm10, zmm4 )                   \
    vmovupd( mem(rcx, rsi, 2, 0), zmm12 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm12, zmm2 )                   \
    vmovupd( mem(rcx, r12, 1, 0), zmm16 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm16, zmm6 )                   \
    vmovupd( mem(rcx, rsi, 4, 0), zmm14 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm14, zmm1 )                   \
    vmovupd( mem(rcx, r13, 1, 0), zmm18 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm18, zmm5 )                   \
    vmovupd( mem(rcx, r12, 2, 0), zmm10 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm10, zmm3 )                   \
    vmovupd( mem(rcx, rdx, 1, 0), zmm12 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm12, zmm8 )                   \
    vmovupd( zmm0, (rcx) MASK_(k(3)))                   \
    vmovupd( zmm4, (rcx, rsi, 1) MASK_(k(3)))           \
    vmovupd( zmm2, (rcx, rsi, 2) MASK_(k(3)))           \
    vmovupd( zmm6, (rcx, r12, 1) MASK_(k(3)))           \
    vmovupd( zmm1, (rcx, rsi, 4) MASK_(k(3)))           \
    vmovupd( zmm5, (rcx, r13, 1) MASK_(k(3)))           \
    vmovupd( zmm3, (rcx, r12, 2) MASK_(k(3)))           \
    vmovupd( zmm8, (rcx, rdx, 1) MASK_(k(3)))           \
    add(r14, rcx)

/**
 * @brief : UPDATE_MASKED_C_7
 * Loads elements from C row only if correspondnig bits in
 * mask register is set, Scales it with Beta and adds FMA result to it.
 * Stores back the C row.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0, zmm4, zmm2, zmm6, zmm1, zmm5, zmm3 holds Alpha*A*B
 * of corresponding rows of C.
 * MASK_KZ(3) = masked for loading-storing of n-left elements.
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 * r14 = column stride of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
 * Stores result Beta * C[1,0] + zmm4(Alpha * A * B) back to C[1,0]
 * Stores result Beta * C[2,0] + zmm2(Alpha * A * B) back to C[2,0]
 * Stores result Beta * C[3,0] + zmm6(Alpha * A * B) back to C[3,0]
 * Stores result Beta * C[4,0] + zmm1(Alpha * A * B) back to C[4,0]
 * Stores result Beta * C[5,0] + zmm5(Alpha * A * B) back to C[5,0]
 * Stores result Beta * C[6,0] + zmm3(Alpha * A * B) back to C[6,0]
*/
#define UPDATE_MASKED_C_7 \
    vmovupd( mem(rcx), zmm30 MASK_KZ(3) )               \
    vfmadd231pd( zmm31, zmm30,zmm0 )                    \
    vmovupd( mem(rcx, rsi, 1, 0), zmm10 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm10, zmm4 )                   \
    vmovupd( mem(rcx, rsi, 2, 0), zmm12 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm12, zmm2 )                   \
    vmovupd( mem(rcx, r12, 1, 0), zmm16 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm16, zmm6 )                   \
    vmovupd( mem(rcx, rsi, 4, 0), zmm14 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm14, zmm1 )                   \
    vmovupd( mem(rcx, r13, 1, 0), zmm18 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm18, zmm5 )                   \
    vmovupd( mem(rcx, r12, 2, 0), zmm10 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm10, zmm3 )                   \
    vmovupd( zmm0, (rcx) MASK_(k(3)))                   \
    vmovupd( zmm4, (rcx, rsi, 1) MASK_(k(3)))           \
    vmovupd( zmm2, (rcx, rsi, 2) MASK_(k(3)))           \
    vmovupd( zmm6, (rcx, r12, 1) MASK_(k(3)))           \
    vmovupd( zmm1, (rcx, rsi, 4) MASK_(k(3)))           \
    vmovupd( zmm5, (rcx, r13, 1) MASK_(k(3)))           \
    vmovupd( zmm3, (rcx, r12, 2) MASK_(k(3)))

/**
 * @brief : UPDATE_MASKED_C_6
 * Loads elements from C row only if correspondnig bits in
 * mask register is set, Scales it with Beta and adds FMA result to it.
 * Stores back the C row.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0, zmm4, zmm2, zmm6, zmm1, zmm5 holds Alpha*A*B
 * of corresponding rows of C.
 * MASK_KZ(3) = masked for loading-storing of n-left elements.
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 * r14 = column stride of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
 * Stores result Beta * C[1,0] + zmm4(Alpha * A * B) back to C[1,0]
 * Stores result Beta * C[2,0] + zmm2(Alpha * A * B) back to C[2,0]
 * Stores result Beta * C[3,0] + zmm6(Alpha * A * B) back to C[3,0]
 * Stores result Beta * C[4,0] + zmm1(Alpha * A * B) back to C[4,0]
 * Stores result Beta * C[5,0] + zmm5(Alpha * A * B) back to C[5,0]
*/
#define UPDATE_MASKED_C_6 \
    vmovupd( mem(rcx), zmm30 MASK_KZ(3) )               \
    vfmadd231pd( zmm31, zmm30, zmm0 )                   \
    vmovupd( mem(rcx, rsi, 1, 0), zmm10 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm10, zmm4 )                   \
    vmovupd( mem(rcx, rsi, 2, 0), zmm12 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm12, zmm2 )                   \
    vmovupd( mem(rcx, r12, 1, 0), zmm16 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm16, zmm6 )                   \
    vmovupd( mem(rcx, rsi, 4, 0), zmm14 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm14, zmm1 )                   \
    vmovupd( mem(rcx, r13, 1, 0), zmm18 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm18, zmm5 )                   \
    vmovupd( zmm0, (rcx) MASK_(k(3)))                   \
    vmovupd( zmm4, (rcx, rsi, 1) MASK_(k(3)))           \
    vmovupd( zmm2, (rcx, rsi, 2) MASK_(k(3)))           \
    vmovupd( zmm6, (rcx, r12, 1) MASK_(k(3)))           \
    vmovupd( zmm1, (rcx, rsi, 4) MASK_(k(3)))           \
    vmovupd( zmm5, (rcx, r13, 1) MASK_(k(3)))

/**
 * @brief : UPDATE_MASKED_C_5
 * Loads elements from C row only if correspondnig bits in
 * mask register is set, Scales it with Beta and adds FMA result to it.
 * Stores back the C row.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0, zmm4, zmm2, zmm6, zmm1 holds Alpha*A*B
 * of corresponding rows of C.
 * MASK_KZ(3) = masked for loading-storing of n-left elements.
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 * r14 = column stride of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
 * Stores result Beta * C[1,0] + zmm4(Alpha * A * B) back to C[1,0]
 * Stores result Beta * C[2,0] + zmm2(Alpha * A * B) back to C[2,0]
 * Stores result Beta * C[3,0] + zmm6(Alpha * A * B) back to C[3,0]
 * Stores result Beta * C[4,0] + zmm1(Alpha * A * B) back to C[4,0]
*/
#define UPDATE_MASKED_C_5 \
    vmovupd( mem(rcx), zmm30 MASK_KZ(3) )                   \
    vfmadd231pd( zmm31, zmm30, zmm0 )                       \
    vmovupd( mem(rcx, rsi, 1, 0), zmm10 MASK_KZ(3) )        \
    vfmadd231pd( zmm31, zmm10, zmm4 )                       \
    vmovupd( mem(rcx, rsi, 2, 0), zmm12 MASK_KZ(3) )        \
    vfmadd231pd( zmm31, zmm12, zmm2 )                       \
    vmovupd( mem(rcx, r12, 1, 0), zmm16 MASK_KZ(3) )        \
    vfmadd231pd( zmm31, zmm16, zmm6 )                       \
    vmovupd( mem(rcx, rsi, 4, 0), zmm14 MASK_KZ(3) )        \
    vfmadd231pd( zmm31, zmm14, zmm1 )                       \
    vmovupd( zmm0, (rcx) MASK_(k(3)))                       \
    vmovupd( zmm4, (rcx, rsi, 1) MASK_(k(3)))               \
    vmovupd( zmm2, (rcx, rsi, 2) MASK_(k(3)))               \
    vmovupd( zmm6, (rcx, r12, 1) MASK_(k(3)))               \
    vmovupd( zmm1, (rcx, rsi, 4) MASK_(k(3)))

/**
 * @brief : UPDATE_MASKED_C_4
 * Loads elements from C row only if correspondnig bits in
 * mask register is set, Scales it with Beta and adds FMA result to it.
 * Stores back the C row.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0, zmm4, zmm2, zmm6 holds Alpha*A*B
 * of corresponding rows of C.
 * MASK_KZ(3) = masked for loading-storing of n-left elements.
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 * r14 = column stride of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
 * Stores result Beta * C[1,0] + zmm4(Alpha * A * B) back to C[1,0]
 * Stores result Beta * C[2,0] + zmm2(Alpha * A * B) back to C[2,0]
 * Stores result Beta * C[3,0] + zmm6(Alpha * A * B) back to C[3,0]
*/
#define UPDATE_MASKED_C_4 \
    vmovupd( mem(rcx), zmm30 MASK_KZ(3) )               \
    vfmadd231pd( zmm31, zmm30, zmm0 )                   \
    vmovupd( mem(rcx, rsi, 1, 0), zmm10 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm10, zmm4 )                   \
    vmovupd( mem(rcx, rsi, 2, 0), zmm12 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm12, zmm2 )                   \
    vmovupd( mem(rcx, r12, 1, 0), zmm16 MASK_KZ(3) )    \
    vfmadd231pd( zmm31, zmm16, zmm6 )                   \
    vmovupd( zmm0, (rcx) MASK_(k(3)))                   \
    vmovupd( zmm4, (rcx, rsi, 1) MASK_(k(3)))           \
    vmovupd( zmm2, (rcx, rsi, 2) MASK_(k(3)))           \
    vmovupd( zmm6, (rcx, r12, 1) MASK_(k(3)))

/**
 * @brief : UPDATE_MASKED_C_3
 * Loads elements from C row only if correspondnig bits in
 * mask register is set, Scales it with Beta and adds FMA result to it.
 * Stores back the C row.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0, zmm4, zmm2 holds Alpha*A*B
 * of corresponding rows of C.
 * MASK_KZ(3) = masked for loading-storing of n-left elements.
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 * r14 = column stride of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
 * Stores result Beta * C[1,0] + zmm4(Alpha * A * B) back to C[1,0]
 * Stores result Beta * C[2,0] + zmm2(Alpha * A * B) back to C[2,0]
*/
#define UPDATE_MASKED_C_3 \
    vmovupd( mem(rcx), zmm30 MASK_KZ(3) )                   \
    vfmadd231pd( zmm31, zmm30, zmm0 )                       \
    vmovupd( mem(rcx, rsi, 1, 0), zmm10 MASK_KZ(3) )        \
    vfmadd231pd( zmm31, zmm10, zmm4 )                       \
    vmovupd( mem(rcx, rsi, 2, 0), zmm12 MASK_KZ(3) )        \
    vfmadd231pd( zmm31, zmm12, zmm2 )                       \
    vmovupd( zmm0, (rcx) MASK_(k(3)))                       \
    vmovupd( zmm4, (rcx, rsi, 1) MASK_(k(3)))               \
    vmovupd( zmm2, (rcx, rsi, 2) MASK_(k(3)))

/**
 * @brief : UPDATE_MASKED_C_2
 * Loads elements from C row only if correspondnig bits in
 * mask register is set, Scales it with Beta and adds FMA result to it.
 * Stores back the C row.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0, zmm4 holds Alpha*A*B
 * of corresponding rows of C.
 * MASK_KZ(3) = masked for loading-storing of n-left elements.
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 * r14 = column stride of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
 * Stores result Beta * C[1,0] + zmm4(Alpha * A * B) back to C[1,0]
*/
#define UPDATE_MASKED_C_2 \
    vmovupd( mem(rcx), zmm30 MASK_KZ(3) )                   \
    vfmadd231pd( zmm31, zmm30, zmm0 )                       \
    vmovupd( mem(rcx, rsi, 1, 0), zmm10 MASK_KZ(3) )        \
    vfmadd231pd( zmm31, zmm10, zmm4 )                       \
    vmovupd( zmm0, (rcx) MASK_(k(3)))                       \
    vmovupd( zmm4, (rcx, rsi, 1) MASK_(k(3)))

/**
 * @brief : UPDATE_MASKED_C_1
 * Loads elements from C row only if correspondnig bits in
 * mask register is set, Scales it with Beta and adds FMA result to it.
 * Stores back the C row.
 *
 * rcx = Address of C matrix
 * zmm31 = Beta
 * zmm0 holds Alpha*A*B
 * of corresponding rows of C.
 * MASK_KZ(3) = masked for loading-storing of n-left elements.
 * rsi = row stride of C matrix in bytes
 * r12 = 3rd row of C matrix in bytes
 * r13 = 5th row of C matrix in bytes
 * r14 = column stride of C matrix in bytes
 *
 * Following snipper updates data as following.
 * Stores result Beta * C[0,0] + zmm0(Alpha * A * B) back to C[0,0]
*/
#define UPDATE_MASKED_C_1 \
    vmovupd( mem(rcx), zmm30 MASK_KZ(3) )   \
    vfmadd231pd( zmm31, zmm30, zmm0 )       \
    vmovupd( zmm0, (rcx) MASK_(k(3)))


/**
 * @brief STORE_COLSTORED_C24Bn
 *
 * This macro performs a series of operations on three input vectors (d1, d2, d3)
 * and stores the results in the corresponding memory locations (mem1, mem2, mem3).
 *
 * Operations:
 * 1. Each input vector (d1, d2, d3) is multiplied by the vector zmm30 (Alpha).
 * 2. The result is then fused with another vector (zmm31, Beta) and added to the original input vector.
 * 3. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2, d3: Input vectors to be processed.
 * - mem1, mem2, mem3: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 * - zmm31: Vector containing the Beta value used in the fused multiply-add operation.
 */
#define STORE_COLSTORED_C24Bn(d1, d2, d3, mem1, mem2, mem3) \
    vmulpd( zmm30, zmm(d1), zmm(d1) )                       \
    vfmadd231pd( mem1, zmm31, zmm(d1))                      \
    vmovupd( zmm(d1), mem1)                                 \
                                                            \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                       \
    vfmadd231pd( mem2, zmm31, zmm(d2))                      \
    vmovupd( zmm(d2), mem2)                                 \
                                                            \
    vmulpd( zmm30, zmm(d3), zmm(d3) )                       \
    vfmadd231pd( mem3, zmm31, zmm(d3))                      \
    vmovupd( zmm(d3), mem3)                                 \

/**
 * @brief STORE_COLSTORED_C20Bn
 *
 * This macro performs a series of operations on three input vectors (d1, d2, d3)
 * and stores the results in the corresponding memory locations (mem1, mem2, mem3).
 *
 * Operations:
 * 1. Each input vector (d1, d2, d3) is multiplied by the vector zmm30 (Alpha).
 * 2. The result is then fused with another vector (zmm31, Beta) and added to the original input vector.
 * 3. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2, d3: Input vectors to be processed.
 * - mem1, mem2, mem3: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 * - zmm31: Vector containing the Beta value used in the fused multiply-add operation.
 */
#define STORE_COLSTORED_C20Bn(d1, d2, d3, mem1, mem2, mem3)                                                         \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */      \
    vfmadd231pd( mem1, zmm31, zmm(d1))  /* Fused multiply-add: zmm31 * mem1 + zmm(d1), store result in zmm(d1) */   \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */                 \
                                                                                                                    \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                               \
    vfmadd231pd( mem2, zmm31, zmm(d2))                                                                              \
    vmovupd( zmm(d2), mem2)                                                                                         \
                                                                                                                    \
    vmulpd( ymm30, ymm(d3), ymm(d3) )                                                                               \
    vfmadd231pd( mem3, ymm31, ymm(d3))                                                                              \
    vmovupd( ymm(d3), mem3)

/**
 * @brief STORE_COLSTORED_C18Bn
 *
 * This macro performs a series of operations on three input vectors (d1, d2, d3)
 * and stores the results in the corresponding memory locations (mem1, mem2, mem3).
 *
 * Operations:
 * 1. Each input vector (d1, d2, d3) is multiplied by the vector zmm30 (Alpha).
 * 2. The result is then fused with another vector (zmm31, Beta) and added to the original input vector.
 * 3. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2, d3: Input vectors to be processed.
 * - mem1, mem2, mem3: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 * - zmm31: Vector containing the Beta value used in the fused multiply-add operation.
 */
#define STORE_COLSTORED_C18Bn(d1, d2, d3, mem1, mem2, mem3)                                                         \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */      \
    vmovupd( mem1, zmm0)                                                                                            \
    vfmadd231pd( zmm0, zmm31, zmm(d1))  /* Fused multiply-add: zmm31 * mem1 + zmm(d1), store result in zmm(d1) */   \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */                 \
                                                                                                                    \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                               \
    vmovupd( mem2, zmm1)                                                                                            \
    vfmadd231pd( zmm1, zmm31, zmm(d2))                                                                              \
    vmovupd( zmm(d2), mem2)                                                                                         \
                                                                                                                    \
    vmulpd( xmm30, xmm(d3), xmm(d3) )                                                                               \
    vmovupd( mem3, xmm2 )                                                                                           \
    vfmadd231pd( xmm2, xmm31, xmm(d3))                                                                              \
    vmovupd( xmm(d3), mem3 )

/**
 * @brief STORE_COLSTORED_C17Bn
 *
 * This macro performs a series of operations on three input vectors (d1, d2, d3)
 * and stores the results in the corresponding memory locations (mem1, mem2, mem3).
 *
 * Operations:
 * 1. Each input vector (d1, d2, d3) is multiplied by the vector zmm30 (Alpha).
 * 2. The result is then fused with another vector (zmm31, Beta) and added to the original input vector.
 * 3. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2, d3: Input vectors to be processed.
 * - mem1, mem2, mem3: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 * - zmm31: Vector containing the Beta value used in the fused multiply-add operation.
 */
#define STORE_COLSTORED_C17Bn(d1, d2, d3, mem1, mem2, mem3)                                                         \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */      \
    vmovupd( mem1, zmm0)                                                                                            \
    vfmadd231pd( zmm0, zmm31, zmm(d1))  /* Fused multiply-add: zmm31 * mem1 + zmm(d1), store result in zmm(d1) */   \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */                 \
                                                                                                                    \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                               \
    vmovupd( mem2, zmm1)                                                                                            \
    vfmadd231pd( zmm1, zmm31, zmm(d2))                                                                              \
    vmovupd( zmm(d2), mem2)                                                                                         \
                                                                                                                    \
    vmulpd( xmm30, xmm(d3), xmm(d3) )                                                                               \
    vmovsd( mem3, xmm2 )                                                                                            \
    vfmadd231pd( xmm2, xmm31, xmm(d3))                                                                              \
    vmovsd( xmm(d3), mem3 )

/**
 * @brief STORE_COLSTORED_C16Bn
 *
 * This macro performs a series of operations on three input vectors (d1, d2)
 * and stores the results in the corresponding memory locations (mem1, mem2).
 *
 * Operations:
 * 1. Each input vector (d1, d2) is multiplied by the vector zmm30 (Alpha).
 * 2. The result is then fused with another vector (zmm31, Beta) and added to the original input vector.
 * 3. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2 Input vectors to be processed.
 * - mem1, mem2: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 * - zmm31: Vector containing the Beta value used in the fused multiply-add operation.
 */
#define STORE_COLSTORED_C16Bn(d1, d2, mem1, mem2)                                                                   \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */      \
    vfmadd231pd( mem1, zmm31, zmm(d1))  /* Fused multiply-add: zmm31 * mem1 + zmm(d1), store result in zmm(d1) */   \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */                 \
                                                                                                                    \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                               \
    vfmadd231pd( mem2, zmm31, zmm(d2))                                                                              \
    vmovupd( zmm(d2), mem2 )

/**
 * @brief STORE_COLSTORED_C24MASKBn
 *
 * This macro performs a series of operations on three input vectors (d1, d2, d3)
 * and stores the masked results in the corresponding memory locations (mem1, mem2, mem3).
 *
 * Operations:
 * 1. Each input vector (d1, d2, d3) is multiplied by the vector zmm30 (Alpha).
 * 2. The result is then fused with another vector (zmm31, Beta) and added to the original input vector.
 * 3. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2, d3: Input vectors to be processed.
 * - mem1, mem2, mem3: Memory locations where the processed masked results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 * - zmm31: Vector containing the Beta value used in the fused multiply-add operation.
 */
#define STORE_COLSTORED_C24MASKBn(d1, d2, d3, mem1, mem2, mem3)                                                     \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */      \
    vmovupd( mem1, zmm0)                                                                                            \
    vfmadd231pd( zmm0, zmm31, zmm(d1))  /* Fused multiply-add: zmm31 * mem1 + zmm(d1), store result in zmm(d1) */   \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */                 \
                                                                                                                    \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                               \
    vmovupd( mem2, zmm1)                                                                                            \
    vfmadd231pd( zmm1, zmm31, zmm(d2))                                                                              \
    vmovupd( zmm(d2), mem2)                                                                                         \
                                                                                                                    \
    vmulpd( zmm30, zmm(d3), zmm(d3) )                                                                               \
    vmovupd( mem3, zmm2 MASK_KZ(2))                                                                                 \
    vfmadd231pd( zmm2, zmm31, zmm(d3))                                                                              \
    vmovupd( zmm(d3), mem3 MASK_(k(2)))

/**
 * @brief STORE_COLSTORED_C12Bn
 *
 * This macro performs a series of operations on three input vectors (d1, d2)
 * and stores the results in the corresponding memory locations (mem1, mem2).
 *
 * Operations:
 * 1. Each input vector (d1, d2) is multiplied by the vector zmm30 (Alpha).
 * 2. The result is then fused with another vector (zmm31, Beta) and added to the original input vector.
 * 3. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2 Input vectors to be processed.
 * - mem1, mem2: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 * - zmm31: Vector containing the Beta value used in the fused multiply-add operation.
 */
#define STORE_COLSTORED_C12Bn(d1, d2, mem1, mem2)                                                                   \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */      \
    vfmadd231pd( mem1, zmm31, zmm(d1))  /* Fused multiply-add: zmm31 * mem1 + zmm(d1), store result in zmm(d1) */   \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */                 \
                                                                                                                    \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                               \
    vmovupd( mem2, ymm1 )                                                                                           \
    vfmadd231pd( ymm1, ymm31, ymm(d2))                                                                              \
    vmovupd( ymm(d2), mem2)

/**
 * @brief STORE_COLSTORED_C11Bn
 *
 * This macro performs a series of operations on three input vectors (d1, d2, d3)
 * and stores the results in the corresponding memory locations (mem1, mem2, mem3).
 *
 * Operations:
 * 1. Each input vector (d1, d2, d3) is multiplied by the vector zmm30 (Alpha).
 * 2. The result is then fused with another vector (zmm31, Beta) and added to the original input vector.
 * 3. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2, d3: Input vectors to be processed.
 * - mem1, mem2, mem3: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 * - zmm31: Vector containing the Beta value used in the fused multiply-add operation.
 */
#define STORE_COLSTORED_C11Bn(d1, d2, d3, mem1, mem2, mem3)                                                         \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */      \
    vfmadd231pd( mem1, zmm31, zmm(d1))  /* Fused multiply-add: zmm31 * mem1 + zmm(d1), store result in zmm(d1) */   \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */                 \
                                                                                                                    \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                               \
    vmovupd( mem2, xmm1)                                                                                            \
    vfmadd231pd( xmm1, xmm31, xmm(d2))                                                                              \
    vmovupd( xmm(d2), mem2)                                                                                         \
                                                                                                                    \
    vmulpd( zmm30, zmm(d3), zmm(d3) )                                                                               \
    vmovsd( mem3, xmm2 )                                                                                            \
    vfmadd231pd( xmm2, xmm31, xmm(d3))                                                                              \
    vmovsd( xmm(d3), mem3 )

/**
 * @brief STORE_COLSTORED_C10Bn
 *
 * This macro performs a series of operations on three input vectors (d1, d2)
 * and stores the results in the corresponding memory locations (mem1, mem2).
 *
 * Operations:
 * 1. Each input vector (d1, d2) is multiplied by the vector zmm30 (Alpha).
 * 2. The result is then fused with another vector (zmm31, Beta) and added to the original input vector.
 * 3. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2 Input vectors to be processed.
 * - mem1, mem2: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 * - zmm31: Vector containing the Beta value used in the fused multiply-add operation.
 */
#define STORE_COLSTORED_C10Bn(d1, d2, mem1, mem2)                                                                   \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */      \
    vfmadd231pd( mem1, zmm31, zmm(d1))  /* Fused multiply-add: zmm31 * mem1 + zmm(d1), store result in zmm(d1) */   \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */                 \
                                                                                                                    \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                               \
    vmovupd( mem2, xmm1 )                                                                                           \
    vfmadd231pd( xmm1, xmm31, xmm(d2))                                                                              \
    vmovupd( xmm(d2), mem2)

/**
 * @brief STORE_COLSTORED_C9Bn
 *
 * This macro performs a series of operations on three input vectors (d1, d2)
 * and stores the results in the corresponding memory locations (mem1, mem2).
 *
 * Operations:
 * 1. Each input vector (d1, d2) is multiplied by the vector zmm30 (Alpha).
 * 2. The result is then fused with another vector (zmm31, Beta) and added to the original input vector.
 * 3. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2 Input vectors to be processed.
 * - mem1, mem2: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 * - zmm31: Vector containing the Beta value used in the fused multiply-add operation.
 */
#define STORE_COLSTORED_C9Bn(d1, d2, mem1, mem2)                                                                    \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */      \
    vfmadd231pd( mem1, zmm31, zmm(d1))  /* Fused multiply-add: zmm31 * mem1 + zmm(d1), store result in zmm(d1) */   \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */                 \
                                                                                                                    \
    vmulpd( xmm30, xmm(d2), xmm(d2) )                                                                               \
    vmovsd( mem2, xmm1)                                                                                             \
    vfmadd231pd( xmm1, xmm31, xmm(d2))                                                                              \
    vmovsd( xmm(d2), mem2 )


#define STORE_COLSTORED_C8Bn(d1, mem1)          \
    vmulpd( zmm30, zmm(d1), zmm(d1) )           \
    vfmadd231pd( mem1, zmm31, zmm(d1))          \
    vmovupd( zmm(d1), mem1 )

/**
 * @brief STORE_COLSTORED_C16MASKBn
 *
 * This macro performs a series of operations on three input vectors (d1, d2)
 * and stores the masked results in the corresponding memory locations (mem1, mem2).
 *
 * Operations:
 * 1. Each input vector (d1, d2) is multiplied by the vector zmm30 (Alpha).
 * 2. The result is then fused with another vector (zmm31, Beta) and added to the original input vector.
 * 3. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2 Input vectors to be processed.
 * - mem1, mem2: Memory locations where the processed masked results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 * - zmm31: Vector containing the Beta value used in the fused multiply-add operation.
 */
#define STORE_COLSTORED_C16MASKBn(d1, d2, mem1, mem2)                                                               \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */      \
    vfmadd231pd( mem1, zmm31, zmm(d1))  /* Fused multiply-add: zmm31 * mem1 + zmm(d1), store result in zmm(d1) */   \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */                 \
                                                                                                                    \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                               \
    vmovupd( mem2, zmm1 MASK_KZ(2))                                                                                 \
    vfmadd231pd( zmm1, zmm31, zmm(d2))                                                                              \
    vmovupd( zmm(d2), mem2 MASK_(k(2)))

/**
 * @brief STORE_COLSTORED_C4Bn
 *
 * This macro performs a series of operations on three input vectors (d1)
 * and stores the results in the corresponding memory locations (mem1).
 *
 * Operations:
 * 1. Each input vector (d1) is multiplied by the vector zmm30 (Alpha).
 * 2. The result is then fused with another vector (zmm31, Beta) and added to the original input vector.
 * 3. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1 Input vector to be processed.
 * - mem1 Memory location where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 * - zmm31: Vector containing the Beta value used in the fused multiply-add operation.
 */
#define STORE_COLSTORED_C4Bn(s, d1, mem1)                                                                               \
    vmulpd( ymm30, ymm(d1), ymm(d1) )       /* Multiply ymm(d1) by ymm30(ALPHA) and store the result in ymm(d1) */      \
    vmovupd( mem1, ymm(s) )                                                                                             \
    vfmadd231pd( ymm(s), ymm31, ymm(d1))    /* Fused multiply-add: ymm31 * mem1 + ymm(d1), store result in ymm(d1) */   \
    vmovupd( ymm(d1), mem1 )                /* Store the result from zmm(d1) to memory location mem1 */

/**
 * @brief STORE_COLSTORED_C2Bn
 *
 * This macro performs a series of operations on three input vectors (d1)
 * and stores the results in the corresponding memory locations (mem1).
 *
 * Operations:
 * 1. Each input vector (d1) is multiplied by the vector zmm30 (Alpha).
 * 2. The result is then fused with another vector (zmm31, Beta) and added to the original input vector.
 * 3. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1 Input vector to be processed.
 * - mem1 Memory location where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 * - zmm31: Vector containing the Beta value used in the fused multiply-add operation.
 */
#define STORE_COLSTORED_C2Bn(s, d1, mem1)                                                                               \
    vmulpd( xmm30, xmm(d1), xmm(d1) )       /* Multiply xmm(d1) by xmm30(ALPHA) and store the result in xmm(d1) */      \
    vmovupd( mem1, xmm(s) )                                                                                             \
    vfmadd231pd( xmm(s), xmm31, xmm(d1))    /* Fused multiply-add: xmm31 * mem1 + xmm(d1), store result in xmm(d1) */   \
    vmovupd( xmm(d1), mem1 )                /* Store the result from zmm(d1) to memory location mem1 */

/**
 * @brief STORE_COLSTORED_C1Bn
 *
 * This macro performs a series of operations on three input vectors (d1)
 * and stores the results in the corresponding memory locations (mem1).
 *
 * Operations:
 * 1. Each input vector (d1) is multiplied by the vector zmm30 (Alpha).
 * 2. The result is then fused with another vector (zmm31, Beta) and added to the original input vector.
 * 3. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1 Input vector to be processed.
 * - mem1 Memory location where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 * - zmm31: Vector containing the Beta value used in the fused multiply-add operation.
 */
#define STORE_COLSTORED_C1Bn(s, d1, mem1)                                                                               \
    vmulpd( xmm30, xmm(d1), xmm(d1) )       /* Multiply xmm(d1) by xmm30(ALPHA) and store the result in xmm(d1) */      \
    vmovsd( mem1, xmm(s))                                                                                               \
    vfmadd231pd( xmm(s), xmm31, xmm(d1))    /* Fused multiply-add: xmm31 * mem1 + xmm(d1), store result in xmm(d1) */   \
    vmovsd( xmm(d1), mem1)                  /* Store the result from zmm(d1) to memory location mem1 */

/**
 * @brief STORE_COLSTORED_C8MASKBn
 *
 * This macro performs a series of operations on three input vectors (d1)
 * and stores the results in the corresponding memory locations (mem1).
 *
 * Operations:
 * 1. Each input vector (d1) is multiplied by the vector zmm30 (Alpha).
 * 2. The result is then fused with another vector (zmm31, Beta) and added to the original input vector.
 * 3. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1 Input vector to be processed.
 * - mem1 Memory location where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 * - zmm31: Vector containing the Beta value used in the fused multiply-add operation.
 */
#define STORE_COLSTORED_C8MASKBn(d1, mem1)  \
    vmulpd( zmm30, zmm(d1), zmm(d1) )       \
    vmovupd( mem1, zmm0 MASK_KZ(2))         \
    vfmadd231pd( zmm0, zmm31, zmm(d1))      \
    vmovupd( zmm(d1), mem1 MASK_(k(2)))

/**
 * @brief STORE_COLSTORED_C24B0
 *
 * This macro performs a series of operations on three input vectors (d1, d2, d3)
 * and stores the results in the corresponding memory locations (mem1, mem2, mem3).
 *
 * Operations:
 * 1. Each input vector (d1, d2, d3) is multiplied by the vector zmm30 (Alpha).
 * 2. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2, d3: Input vectors to be processed.
 * - mem1, mem2, mem3: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 */
#define STORE_COLSTORED_C24B0(d1, d2, d3, mem1, mem2, mem3)                                                     \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */  \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */             \
                                                                                                                \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                           \
    vmovupd( zmm(d2), mem2)                                                                                     \
                                                                                                                \
    vmulpd( zmm30, zmm(d3), zmm(d3) )                                                                           \
    vmovupd( zmm(d3), mem3)                                                                                     \

/**
 * @brief STORE_COLSTORED_C20B0
 *
 * This macro performs a series of operations on three input vectors (d1, d2, d3)
 * and stores the results in the corresponding memory locations (mem1, mem2, mem3).
 *
 * Operations:
 * 1. Each input vector (d1, d2, d3) is multiplied by the vector zmm30 (Alpha).
 * 2. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2, d3: Input vectors to be processed.
 * - mem1, mem2, mem3: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 */
#define STORE_COLSTORED_C20B0(d1, d2, d3, mem1, mem2, mem3)                                                     \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */  \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */             \
                                                                                                                \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                           \
    vmovupd( zmm(d2), mem2)                                                                                     \
                                                                                                                \
    vmulpd( ymm30, ymm(d3), ymm(d3) )                                                                           \
    vmovupd( ymm(d3), mem3)                                                                                     \

/**
 * @brief STORE_COLSTORED_C18B0
 *
 * This macro performs a series of operations on three input vectors (d1, d2, d3)
 * and stores the results in the corresponding memory locations (mem1, mem2, mem3).
 *
 * Operations:
 * 1. Each input vector (d1, d2, d3) is multiplied by the vector zmm30 (Alpha).
 * 2. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2, d3: Input vectors to be processed.
 * - mem1, mem2, mem3: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 */
#define STORE_COLSTORED_C18B0(d1, d2, d3, mem1, mem2, mem3)                                                     \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */  \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */             \
                                                                                                                \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                           \
    vmovupd( zmm(d2), mem2)                                                                                     \
                                                                                                                \
    vmulpd( xmm30, xmm(d3), xmm(d3) )                                                                           \
    vmovupd( xmm(d3), mem3 )

/**
 * @brief STORE_COLSTORED_C17B0
 *
 * This macro performs a series of operations on three input vectors (d1, d2, d3)
 * and stores the results in the corresponding memory locations (mem1, mem2, mem3).
 *
 * Operations:
 * 1. Each input vector (d1, d2, d3) is multiplied by the vector zmm30 (Alpha).
 * 2. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2, d3: Input vectors to be processed.
 * - mem1, mem2, mem3: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 */
#define STORE_COLSTORED_C17B0(d1, d2, d3, mem1, mem2, mem3)                                                     \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */  \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */             \
                                                                                                                \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                           \
    vmovupd( zmm(d2), mem2)                                                                                     \
                                                                                                                \
    vmulpd( xmm30, xmm(d3), xmm(d3) )                                                                           \
    vmovsd( xmm(d3), mem3 )

/**
 * @brief STORE_COLSTORED_C16B0
 *
 * This macro performs a series of operations on three input vectors (d1, d2)
 * and stores the results in the corresponding memory locations (mem1, mem2).
 *
 * Operations:
 * 1. Each input vector (d1, d2) is multiplied by the vector zmm30 (Alpha).
 * 2. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2 Input vectors to be processed.
 * - mem1, mem2: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 */
#define STORE_COLSTORED_C16B0(d1, d2, mem1, mem2)                                                               \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */  \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */             \
                                                                                                                \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                           \
    vmovupd( zmm(d2), mem2 )

/**
 * @brief STORE_COLSTORED_C24MASKB0
 *
 * This macro performs a series of operations on three input vectors (d1, d2, d3)
 * and stores the results in the corresponding memory locations (mem1, mem2, mem3).
 *
 * Operations:
 * 1. Each input vector (d1, d2, d3) is multiplied by the vector zmm30 (Alpha).
 * 2. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2, d3: Input vectors to be processed.
 * - mem1, mem2, mem3: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 */
#define STORE_COLSTORED_C24MASKB0(d1, d2, d3, mem1, mem2, mem3)                                                 \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */  \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */             \
                                                                                                                \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                           \
    vmovupd( zmm(d2), mem2)                                                                                     \
                                                                                                                \
    vmulpd( zmm30, zmm(d3), zmm(d3) )                                                                           \
    vmovupd( zmm(d3), mem3 MASK_(k(2)))

/**
 * @brief STORE_COLSTORED_C12B0
 *
 * This macro performs a series of operations on three input vectors (d1, d2)
 * and stores the results in the corresponding memory locations (mem1, mem2).
 *
 * Operations:
 * 1. Each input vector (d1, d2) is multiplied by the vector zmm30 (Alpha).
 * 2. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2 Input vectors to be processed.
 * - mem1, mem2: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 */
#define STORE_COLSTORED_C12B0(d1, d2, mem1, mem2)                                                               \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */  \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */             \
                                                                                                                \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                           \
    vmovupd( ymm(d2), mem2)

/**
 * @brief STORE_COLSTORED_C11B0
 *
 * This macro performs a series of operations on three input vectors (d1, d2, d3)
 * and stores the results in the corresponding memory locations (mem1, mem2, mem3).
 *
 * Operations:
 * 1. Each input vector (d1, d2, d3) is multiplied by the vector zmm30 (Alpha).
 * 2. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2, d3: Input vectors to be processed.
 * - mem1, mem2, mem3: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 */
#define STORE_COLSTORED_C11B0(d1, d2, d3, mem1, mem2, mem3)                                                     \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */  \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */             \
                                                                                                                \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                           \
    vmovupd( xmm(d2), mem2)                                                                                     \
                                                                                                                \
    vmulpd( zmm30, zmm(d3), zmm(d3) )                                                                           \
    vmovsd( xmm(d3), mem3 )

/**
 * @brief STORE_COLSTORED_C10B0
 *
 * This macro performs a series of operations on three input vectors (d1, d2)
 * and stores the results in the corresponding memory locations (mem1, mem2).
 *
 * Operations:
 * 1. Each input vector (d1, d2) is multiplied by the vector zmm30 (Alpha).
 * 2. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2 Input vectors to be processed.
 * - mem1, mem2: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 */
#define STORE_COLSTORED_C10B0(d1, d2, mem1, mem2)                                                               \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */  \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */             \
                                                                                                                \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                           \
    vmovupd( xmm(d2), mem2)

/**
 * @brief STORE_COLSTORED_C9B0
 *
 * This macro performs a series of operations on three input vectors (d1, d2)
 * and stores the results in the corresponding memory locations (mem1, mem2).
 *
 * Operations:
 * 1. Each input vector (d1, d2) is multiplied by the vector zmm30 (Alpha).
 * 2. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2 Input vectors to be processed.
 * - mem1, mem2: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 */
#define STORE_COLSTORED_C9B0(d1, d2, mem1, mem2)                                                                \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */  \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */             \
                                                                                                                \
    vmulpd( xmm30, xmm(d2), xmm(d2) )                                                                           \
    vmovsd( xmm(d2), mem2 )

#define STORE_COLSTORED_C8B0(d1, mem1)\
	/* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */  \
    vmulpd( zmm30, zmm(d1), zmm(d1) )                                       \
	/* Store the result from zmm(d1) to memory location mem1 */             \
    vmovupd( zmm(d1), mem1 )

/**
 * @brief STORE_COLSTORED_C16MASKB0
 *
 * This macro performs a series of operations on three input vectors (d1, d2)
 * and stores the results in the corresponding memory locations (mem1, mem2).
 *
 * Operations:
 * 1. Each input vector (d1, d2) is multiplied by the vector zmm30 (Alpha).
 * 2. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1, d2 Input vectors to be processed.
 * - mem1, mem2: Memory locations where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 */
#define STORE_COLSTORED_C16MASKB0(d1, d2, mem1, mem2)                                                           \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */  \
    vmovupd( zmm(d1), mem1)             /* Store the result from zmm(d1) to memory location mem1 */             \
                                                                                                                \
    vmulpd( zmm30, zmm(d2), zmm(d2) )                                                                           \
    vmovupd( zmm(d2), mem2 MASK_(k(2)))

/**
 * @brief STORE_COLSTORED_C4B0
 *
 * This macro performs a series of operations on three input vectors (d1)
 * and stores the results in the corresponding memory locations (mem1).
 *
 * Operations:
 * 1. Each input vector (d1) is multiplied by the vector zmm30 (Alpha).
 * 2. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1 Input vector to be processed.
 * - mem1 Memory location where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 */
#define STORE_COLSTORED_C4B0(d1, mem1)                                                                          \
    vmulpd( ymm30, ymm(d1), ymm(d1) )   /* Multiply ymm(d1) by ymm30(ALPHA) and store the result in ymm(d1) */  \
    vmovupd( ymm(d1), mem1 )            /* Store the result from zmm(d1) to memory location mem1 */

/**
 * @brief STORE_COLSTORED_C2B0
 *
 * This macro performs a series of operations on three input vectors (d1)
 * and stores the results in the corresponding memory locations (mem1).
 *
 * Operations:
 * 1. Each input vector (d1) is multiplied by the vector zmm30 (Alpha).
 * 2. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1 Input vector to be processed.
 * - mem1 Memory location where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 */
#define STORE_COLSTORED_C2B0(d1, mem1)                                                                          \
    vmulpd( xmm30, xmm(d1), xmm(d1) )   /* Multiply xmm(d1) by xmm30(ALPHA) and store the result in xmm(d1) */  \
    vmovupd( xmm(d1), mem1 )            /* Store the result from zmm(d1) to memory location mem1 */

/**
 * @brief STORE_COLSTORED_C1B0
 *
 * This macro performs a series of operations on three input vectors (d1)
 * and stores the results in the corresponding memory locations (mem1).
 *
 * Operations:
 * 1. Each input vector (d1) is multiplied by the vector zmm30 (Alpha).
 * 2. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1 Input vector to be processed.
 * - mem1 Memory location where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 */
#define STORE_COLSTORED_C1B0(d1, mem1)                                                                          \
    vmulpd( xmm30, xmm(d1), xmm(d1) )   /* Multiply xmm(d1) by xmm30(ALPHA) and store the result in xmm(d1) */  \
    vmovsd( xmm(d1), mem1)              /* Store the result from zmm(d1) to memory location mem1 */

/**
 * @brief STORE_COLSTORED_C8MASKB0
 *
 * This macro performs a series of operations on three input vectors (d1)
 * and stores the results in the corresponding memory locations (mem1).
 *
 * Operations:
 * 1. Each input vector (d1) is multiplied by the vector zmm30 (Alpha).
 * 2. The final result is stored back into the respective memory location.
 *
 * Parameters:
 * - d1 Input vector to be processed.
 * - mem1 Memory location where the processed results are stored.
 * - zmm30: Vector containing the Alpha value used for multiplication.
 */
#define STORE_COLSTORED_C8MASKB0(d1, mem1)                                                                      \
    vmulpd( zmm30, zmm(d1), zmm(d1) )   /* Multiply zmm(d1) by zmm30(ALPHA) and store the result in zmm(d1) */  \
    vmovupd( zmm(d1), mem1 MASK_(k(2))) /* Store the result from zmm(d1) to memory location mem1 */

/**************************START COLSTORED C STORE Bn x8********************************/
/**
 * @brief STORE_COLSTORED_C_24x8_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C24Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 * - zmm20, 21, 23 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_24x8_Bn                                                               \
    STORE_COLSTORED_C24Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
    STORE_COLSTORED_C24Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
    STORE_COLSTORED_C24Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
    STORE_COLSTORED_C24Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
    STORE_COLSTORED_C24Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
    STORE_COLSTORED_C24Bn(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))   \
    STORE_COLSTORED_C24Bn(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))   \
    STORE_COLSTORED_C24Bn(20, 21, 23, mem(rdx, r13, 1), 0x40(rdx, r13, 1), 0x80(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_20x8_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C20Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 * - zmm20, 21, 23 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_20x8_Bn                                                               \
    STORE_COLSTORED_C20Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
    STORE_COLSTORED_C20Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
    STORE_COLSTORED_C20Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
    STORE_COLSTORED_C20Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
    STORE_COLSTORED_C20Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
    STORE_COLSTORED_C20Bn(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))   \
    STORE_COLSTORED_C20Bn(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))   \
    STORE_COLSTORED_C20Bn(20, 21, 23, mem(rdx, r13, 1), 0x40(rdx, r13, 1), 0x80(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_18x8_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C18Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 * - zmm20, 21, 23 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_18x8_Bn                                                               \
    STORE_COLSTORED_C18Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
    STORE_COLSTORED_C18Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
    STORE_COLSTORED_C18Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
    STORE_COLSTORED_C18Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
    STORE_COLSTORED_C18Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
    STORE_COLSTORED_C18Bn(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))   \
    STORE_COLSTORED_C18Bn(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))   \
    STORE_COLSTORED_C18Bn(20, 21, 23, mem(rdx, r13, 1), 0x40(rdx, r13, 1), 0x80(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_17x8_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C17Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 * - zmm20, 21, 23 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_17x8_Bn                                                               \
    STORE_COLSTORED_C17Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
    STORE_COLSTORED_C17Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
    STORE_COLSTORED_C17Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
    STORE_COLSTORED_C17Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
    STORE_COLSTORED_C17Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
    STORE_COLSTORED_C17Bn(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))   \
    STORE_COLSTORED_C17Bn(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))   \
    STORE_COLSTORED_C17Bn(20, 21, 23, mem(rdx, r13, 1), 0x40(rdx, r13, 1), 0x80(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_16x8_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C16Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 * - zmm18, 19 holds A * B for C[0, 6]
 * - zmm20, 21 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_16x8_Bn                                       \
    STORE_COLSTORED_C16Bn(6, 7, mem(rcx), 0x40(rcx))                    \
    STORE_COLSTORED_C16Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
    STORE_COLSTORED_C16Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
    STORE_COLSTORED_C16Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
    STORE_COLSTORED_C16Bn(14, 15, mem(rdx), 0x40(rdx))                  \
    STORE_COLSTORED_C16Bn(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))  \
    STORE_COLSTORED_C16Bn(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))  \
    STORE_COLSTORED_C16Bn(20, 21, mem(rdx, r13, 1), 0x40(rdx, r13, 1))

#define STORE_COLSTORED_C_24MASKx8_Bn                   \
    vmulpd( zmm30, zmm6, zmm6 )                         \
    vfmadd231pd( mem(rcx), zmm31, zmm6 )                \
    vmovupd( zmm6, mem(rcx) )                           \
                                                        \
    vmulpd( zmm30, zmm7, zmm7 )                         \
    vfmadd231pd( 0x40(rcx), zmm31, zmm7 )               \
    vmovupd( zmm7, 0x40(rcx) )                          \
                                                        \
    vmulpd( zmm30, zmm8, zmm8 )                         \
    vfmadd231pd( mem(rcx, rdi, 1), zmm31, zmm8 )        \
    vmovupd( zmm8, mem(rcx, rdi, 1) )                   \
                                                        \
    vmulpd( zmm30, zmm9, zmm9 )                         \
    vfmadd231pd( 0x40(rcx, rdi, 1), zmm31, zmm9 )       \
    vmovupd( zmm9, 0x40(rcx, rdi, 1) )                  \
                                                        \
    vmulpd( zmm30, zmm10, zmm10 )                       \
    vfmadd231pd( mem(rcx, rdi, 2), zmm31, zmm10 )       \
    vmovupd( zmm10, mem(rcx, rdi, 2) )                  \
                                                        \
    vmulpd( zmm30, zmm11, zmm11 )                       \
    vfmadd231pd( 0x40(rcx, rdi, 2), zmm31, zmm11 )      \
    vmovupd( zmm11, 0x40(rcx, rdi, 2) )                 \
                                                        \
    vmulpd( zmm30, zmm12, zmm12 )                       \
    vfmadd231pd( mem(rcx, r13, 1), zmm31, zmm12 )       \
    vmovupd( zmm12, mem(rcx, r13, 1) )                  \
                                                        \
    vmulpd( zmm30, zmm13, zmm13 )                       \
    vfmadd231pd( 0x40(rcx, r13, 1), zmm31, zmm13 )      \
    vmovupd( zmm13, 0x40(rcx, r13, 1) )                 \
                                                        \
    vmulpd( zmm30, zmm14, zmm14 )                       \
    vfmadd231pd( mem(rdx), zmm31, zmm14 )               \
    vmovupd( zmm14, mem(rdx) )                          \
                                                        \
    vmulpd( zmm30, zmm15, zmm15 )                       \
    vfmadd231pd( 0x40(rdx), zmm31, zmm15 )              \
    vmovupd( zmm15, 0x40(rdx) )                         \
                                                        \
    vmulpd( zmm30, zmm16, zmm16 )                       \
    vfmadd231pd( mem(rdx, rdi, 1), zmm31, zmm16 )       \
    vmovupd( zmm16, mem(rdx, rdi, 1) )                  \
                                                        \
    vmulpd( zmm30, zmm17, zmm17 )                       \
    vfmadd231pd( 0x40(rdx, rdi, 1), zmm31, zmm17 )      \
    vmovupd( zmm17, 0x40(rdx, rdi, 1) )                 \
                                                        \
    vmulpd( zmm30, zmm18, zmm18 )                       \
    vfmadd231pd( mem(rdx, rdi, 2), zmm31, zmm18 )       \
    vmovupd( zmm18, mem(rdx, rdi, 2) )                  \
                                                        \
    vmulpd( zmm30, zmm19, zmm19 )                       \
    vfmadd231pd( 0x40(rdx, rdi, 2), zmm31, zmm19 )      \
    vmovupd( zmm19, 0x40(rdx, rdi, 2) )                 \
                                                        \
    vmulpd( zmm30, zmm20, zmm20 )                       \
    vfmadd231pd( mem(rdx, r13, 1), zmm31, zmm20 )       \
    vmovupd( zmm20, mem(rdx, r13, 1) )                  \
                                                        \
    vmulpd( zmm30, zmm21, zmm21 )                       \
    vfmadd231pd( 0x40(rdx, r13, 1), zmm31, zmm21 )      \
    vmovupd( zmm21, 0x40(rdx, r13, 1) )                 \
                                                        \
    vmulpd( zmm30, zmm28, zmm28 )                       \
    vmovupd( 0x80(rcx), zmm0 MASK_KZ(2) )               \
    vfmadd231pd( zmm0, zmm31, zmm28 )                   \
    vmovupd( zmm28, 0x80(rcx) MASK_(k(2)) )             \
    vmulpd( zmm30, zmm29, zmm29 )                       \
    vmovupd( 0x80(rcx, rdi, 1), zmm1 MASK_KZ(2) )       \
    vfmadd231pd( zmm1, zmm31, zmm29 )                   \
    vmovupd( zmm29, 0x80(rcx, rdi, 1) MASK_(k(2)) )     \
    vmulpd( zmm30, zmm26, zmm26 )                       \
    vmovupd( 0x80(rcx, rdi, 2), zmm2 MASK_KZ(2) )       \
    vfmadd231pd( zmm2, zmm31, zmm26 )                   \
    vmovupd( zmm26, 0x80(rcx, rdi, 2) MASK_(k(2)) )     \
    vmulpd( zmm30, zmm27, zmm27 )                       \
    vmovupd( 0x80(rcx, r13, 1), zmm3 MASK_KZ(2) )       \
    vfmadd231pd( zmm3, zmm31, zmm27 )                   \
    vmovupd( zmm27, 0x80(rcx, r13, 1) MASK_(k(2)) )     \
    vmulpd( zmm30, zmm24, zmm24 )                       \
    vmovupd( 0x80(rdx), zmm4 MASK_KZ(2) )               \
    vfmadd231pd( zmm4, zmm31, zmm24 )                   \
    vmovupd( zmm24, 0x80(rdx) MASK_(k(2)) )             \
    vmulpd( zmm30, zmm25, zmm25 )                       \
    vmovupd( 0x80(rdx, rdi, 1), zmm5 MASK_KZ(2) )       \
    vfmadd231pd( zmm5, zmm31, zmm25 )                   \
    vmovupd( zmm25, 0x80(rdx, rdi, 1) MASK_(k(2)) )     \
    vmulpd( zmm30, zmm22, zmm22 )                       \
    vmovupd( 0x80(rdx, rdi, 2), zmm6 MASK_KZ(2) )       \
    vfmadd231pd( zmm6, zmm31, zmm22 )                   \
    vmovupd( zmm22, 0x80(rdx, rdi, 2) MASK_(k(2)) )     \
    vmulpd( zmm30, zmm23, zmm23 )                       \
    vmovupd( 0x80(rdx, r13, 1), zmm8 MASK_KZ(2) )       \
    vfmadd231pd( zmm8, zmm31, zmm23 )                   \
    vmovupd( zmm23, 0x80(rdx, r13, 1) MASK_(k(2)) )

/**
 * @brief STORE_COLSTORED_C_12x8_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C12Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 * - zmm18, 19 holds A * B for C[0, 6]
 * - zmm20, 21 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_12x8_Bn                                       \
    STORE_COLSTORED_C12Bn(6, 7, mem(rcx), 0x40(rcx))                    \
    STORE_COLSTORED_C12Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
    STORE_COLSTORED_C12Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
    STORE_COLSTORED_C12Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
    STORE_COLSTORED_C12Bn(14, 15, mem(rdx), 0x40(rdx))                  \
    STORE_COLSTORED_C12Bn(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))  \
    STORE_COLSTORED_C12Bn(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))  \
    STORE_COLSTORED_C12Bn(20, 21, mem(rdx, r13, 1), 0x40(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_11x8_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C11Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - 0x80(rcx) = C[10, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - 0x80(rdx) = 4th Column C[10, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 * - zmm20, 21, 23 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_11x8_Bn                                                               \
    STORE_COLSTORED_C11Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x50(rcx))                             \
    STORE_COLSTORED_C11Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x50(rcx, rdi, 1))     \
    STORE_COLSTORED_C11Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x50(rcx, rdi, 2))   \
    STORE_COLSTORED_C11Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x50(rcx, r13, 1))   \
    STORE_COLSTORED_C11Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x50(rdx))                           \
    STORE_COLSTORED_C11Bn(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x50(rdx, rdi, 1))   \
    STORE_COLSTORED_C11Bn(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x50(rdx, rdi, 2))   \
    STORE_COLSTORED_C11Bn(20, 21, 23, mem(rdx, r13, 1), 0x40(rdx, r13, 1), 0x50(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_10x8_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C10Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 * - zmm18, 19 holds A * B for C[0, 6]
 * - zmm20, 21 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_10x8_Bn                                           \
    STORE_COLSTORED_C10Bn(6, 7, mem(rcx), 0x40(rcx))                        \
    STORE_COLSTORED_C10Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))        \
    STORE_COLSTORED_C10Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))      \
    STORE_COLSTORED_C10Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))      \
    STORE_COLSTORED_C10Bn(14, 15, mem(rdx), 0x40(rdx))                      \
    STORE_COLSTORED_C10Bn(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))      \
    STORE_COLSTORED_C10Bn(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))      \
    STORE_COLSTORED_C10Bn(20, 21, mem(rdx, r13, 1), 0x40(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_9x8_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C9Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 * - zmm18, 19 holds A * B for C[0, 6]
 * - zmm20, 21 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_9x8_Bn                                        \
    STORE_COLSTORED_C9Bn(6, 7, mem(rcx), 0x40(rcx))                     \
    STORE_COLSTORED_C9Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))     \
    STORE_COLSTORED_C9Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))   \
    STORE_COLSTORED_C9Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))   \
    STORE_COLSTORED_C9Bn(14, 15, mem(rdx), 0x40(rdx))                   \
    STORE_COLSTORED_C9Bn(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))   \
    STORE_COLSTORED_C9Bn(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))   \
    STORE_COLSTORED_C9Bn(20, 21, mem(rdx, r13, 1), 0x40(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_8x8_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 * - zmm18 holds A * B for C[0, 6]
 * - zmm20 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_8x8_Bn                    \
    STORE_COLSTORED_C8Bn(6, mem(rcx))               \
    STORE_COLSTORED_C8Bn(8, mem(rcx, rdi, 1))       \
    STORE_COLSTORED_C8Bn(10, mem(rcx, rdi, 2))      \
    STORE_COLSTORED_C8Bn(12, mem(rcx, r13, 1))      \
    STORE_COLSTORED_C8Bn(14, mem(rdx))              \
    STORE_COLSTORED_C8Bn(16, mem(rdx, rdi, 1))      \
    STORE_COLSTORED_C8Bn(18, mem(rdx, rdi, 2))      \
    STORE_COLSTORED_C8Bn(20, mem(rdx, r13, 1))

#define STORE_COLSTORED_C_16MASKx8_Bn               \
    vmulpd( zmm30, zmm6, zmm6 )                     \
    vfmadd231pd( mem(rcx), zmm31, zmm6)             \
    vmovupd( zmm6, mem(rcx))                        \
                                                    \
    vmulpd( zmm30, zmm8, zmm8 )                     \
    vfmadd231pd( mem(rcx, rdi, 1), zmm31, zmm8)     \
    vmovupd( zmm8, mem(rcx, rdi, 1))                \
                                                    \
    vmulpd( zmm30, zmm10, zmm10 )                   \
    vfmadd231pd( mem(rcx, rdi, 2), zmm31, zmm10)    \
    vmovupd( zmm10, mem(rcx, rdi, 2))               \
                                                    \
    vmulpd( zmm30, zmm12, zmm12 )                   \
    vfmadd231pd( mem(rcx, r13, 1), zmm31, zmm12)    \
    vmovupd( zmm12, mem(rcx, r13, 1))               \
                                                    \
    vmulpd( zmm30, zmm14, zmm14 )                   \
    vfmadd231pd( mem(rdx), zmm31, zmm14)            \
    vmovupd( zmm14, mem(rdx))                       \
                                                    \
    vmulpd( zmm30, zmm16, zmm16 )                   \
    vfmadd231pd( mem(rdx, rdi, 1), zmm31, zmm16)    \
    vmovupd( zmm16, mem(rdx, rdi, 1))               \
                                                    \
    vmulpd( zmm30, zmm18, zmm18 )                   \
    vfmadd231pd( mem(rdx, rdi, 2), zmm31, zmm18)    \
    vmovupd( zmm18, mem(rdx, rdi, 2))               \
                                                    \
    vmulpd( zmm30, zmm20, zmm20 )                   \
    vfmadd231pd( mem(rdx, r13, 1), zmm31, zmm20)    \
    vmovupd( zmm20, mem(rdx, r13, 1))               \
                                                    \
    vmulpd( zmm30, zmm7, zmm7 )                     \
    vmovupd( 0x40(rcx), zmm0 MASK_KZ(2))            \
                                                    \
    vfmadd231pd( zmm0, zmm31, zmm7)                 \
    vmovupd( zmm7, 0x40(rcx) MASK_(k(2)))           \
                                                    \
    vmulpd( zmm30, zmm9, zmm9 )                     \
    vmovupd( 0x40(rcx, rdi, 1), zmm1 MASK_KZ(2))    \
                                                    \
    vfmadd231pd( zmm1, zmm31, zmm9)                 \
    vmovupd( zmm9, 0x40(rcx, rdi, 1) MASK_(k(2)))   \
                                                    \
    vmulpd( zmm30, zmm11, zmm11 )                   \
    vmovupd( 0x40(rcx, rdi, 2), zmm2 MASK_KZ(2))    \
                                                    \
    vfmadd231pd( zmm2, zmm31, zmm11)                \
    vmovupd( zmm11, 0x40(rcx, rdi, 2) MASK_(k(2)))  \
                                                    \
    vmulpd( zmm30, zmm13, zmm13 )                   \
    vmovupd( 0x40(rcx, r13, 1), zmm3 MASK_KZ(2))    \
                                                    \
    vfmadd231pd( zmm3, zmm31, zmm13)                \
    vmovupd( zmm13, 0x40(rcx, r13, 1) MASK_(k(2)))  \
                                                    \
    vmulpd( zmm30, zmm15, zmm15 )                   \
    vmovupd( 0x40(rdx), zmm4 MASK_KZ(2))            \
                                                    \
    vfmadd231pd( zmm4, zmm31, zmm15)                \
    vmovupd( zmm15, 0x40(rdx) MASK_(k(2)))          \
                                                    \
    vmulpd( zmm30, zmm17, zmm17 )                   \
    vmovupd( 0x40(rdx, rdi, 1), zmm5 MASK_KZ(2))    \
                                                    \
    vfmadd231pd( zmm5, zmm31, zmm17)                \
    vmovupd( zmm17, 0x40(rdx, rdi, 1) MASK_(k(2)))  \
                                                    \
    vmulpd( zmm30, zmm19, zmm19 )                   \
    vmovupd( 0x40(rdx, rdi, 2), zmm6 MASK_KZ(2))    \
                                                    \
    vfmadd231pd( zmm6, zmm31, zmm19)                \
    vmovupd( zmm19, 0x40(rdx, rdi, 2) MASK_(k(2)))  \
                                                    \
    vmulpd( zmm30, zmm21, zmm21 )                   \
    vmovupd( 0x40(rdx, r13, 1), zmm8 MASK_KZ(2))    \
                                                    \
    vfmadd231pd( zmm8, zmm31, zmm21)                \
    vmovupd( zmm21, 0x40(rdx, r13, 1) MASK_(k(2)))

/**
 * @brief STORE_COLSTORED_C_4x8_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C4Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 * - zmm18 holds A * B for C[0, 6]
 * - zmm20 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_4x8_Bn                    \
    STORE_COLSTORED_C4Bn(0, 6, mem(rcx))            \
    STORE_COLSTORED_C4Bn(1, 8, mem(rcx, rdi, 1))    \
    STORE_COLSTORED_C4Bn(2, 10, mem(rcx, rdi, 2))   \
    STORE_COLSTORED_C4Bn(3, 12, mem(rcx, r13, 1))   \
    STORE_COLSTORED_C4Bn(4, 14, mem(rdx))           \
    STORE_COLSTORED_C4Bn(5, 16, mem(rdx, rdi, 1))   \
    STORE_COLSTORED_C4Bn(6, 18, mem(rdx, rdi, 2))   \
    STORE_COLSTORED_C4Bn(7, 20, mem(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_2x8_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C2Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 * - zmm18 holds A * B for C[0, 6]
 * - zmm20 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_2x8_Bn                    \
    STORE_COLSTORED_C2Bn(0, 6, mem(rcx))            \
    STORE_COLSTORED_C2Bn(1, 8, mem(rcx, rdi, 1))    \
    STORE_COLSTORED_C2Bn(2, 10, mem(rcx, rdi, 2))   \
    STORE_COLSTORED_C2Bn(3, 12, mem(rcx, r13, 1))   \
    STORE_COLSTORED_C2Bn(4, 14, mem(rdx))           \
    STORE_COLSTORED_C2Bn(5, 16, mem(rdx, rdi, 1))   \
    STORE_COLSTORED_C2Bn(6, 18, mem(rdx, rdi, 2))   \
    STORE_COLSTORED_C2Bn(7, 20, mem(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_1x8_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C1Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 * - zmm18 holds A * B for C[0, 6]
 * - zmm20 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_1x8_Bn                    \
    STORE_COLSTORED_C1Bn(0, 6, mem(rcx))            \
    STORE_COLSTORED_C1Bn(1, 8, mem(rcx, rdi, 1))    \
    STORE_COLSTORED_C1Bn(2, 10, mem(rcx, rdi, 2))   \
    STORE_COLSTORED_C1Bn(3, 12, mem(rcx, r13, 1))   \
    STORE_COLSTORED_C1Bn(4, 14, mem(rdx))           \
    STORE_COLSTORED_C1Bn(5, 16, mem(rdx, rdi, 1))   \
    STORE_COLSTORED_C1Bn(6, 18, mem(rdx, rdi, 2))   \
    STORE_COLSTORED_C1Bn(7, 20, mem(rdx, r13, 1))

#define STORE_COLSTORED_C_8MASKx8_Bn                \
    vmulpd( zmm30, zmm6, zmm6 )                     \
    vmovupd( mem(rcx), zmm0 MASK_KZ(2))             \
                                                    \
    vmulpd( zmm30, zmm8, zmm8 )                     \
    vmovupd( mem(rcx, rdi, 1), zmm1 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm10, zmm10 )                   \
    vmovupd( mem(rcx, rdi, 2), zmm2 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm12, zmm12 )                   \
    vmovupd( mem(rcx, r13, 1), zmm3 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm14, zmm14 )                   \
    vmovupd( mem(rdx), zmm4 MASK_KZ(2))             \
                                                    \
    vmulpd( zmm30, zmm16, zmm16 )                   \
    vmovupd( mem(rdx, rdi, 1), zmm5 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm18, zmm18 )                   \
    vmovupd( mem(rdx, rdi, 2), zmm7 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm20, zmm20 )                   \
    vmovupd( mem(rdx, r13, 1), zmm9 MASK_KZ(2))     \
                                                    \
    vfmadd231pd( zmm0, zmm31, zmm6)                 \
    vmovupd( zmm6, mem(rcx) MASK_(k(2)))            \
                                                    \
    vfmadd231pd( zmm1, zmm31, zmm8)                 \
    vmovupd( zmm8, mem(rcx, rdi, 1) MASK_(k(2)))    \
                                                    \
    vfmadd231pd( zmm2, zmm31, zmm10)                \
    vmovupd( zmm10, mem(rcx, rdi, 2) MASK_(k(2)))   \
                                                    \
    vfmadd231pd( zmm3, zmm31, zmm12)                \
    vmovupd( zmm12, mem(rcx, r13, 1) MASK_(k(2)))   \
                                                    \
    vfmadd231pd( zmm4, zmm31, zmm14)                \
    vmovupd( zmm14, mem(rdx) MASK_(k(2)))           \
                                                    \
    vfmadd231pd( zmm5, zmm31, zmm16)                \
    vmovupd( zmm16, mem(rdx, rdi, 1) MASK_(k(2)))   \
                                                    \
    vfmadd231pd( zmm7, zmm31, zmm18)                \
    vmovupd( zmm18, mem(rdx, rdi, 2) MASK_(k(2)))   \
                                                    \
    vfmadd231pd( zmm9, zmm31, zmm20)                \
    vmovupd( zmm20, mem(rdx, r13, 1) MASK_(k(2)))

/*****************************************************************************/

/**
 * @brief STORE_COLSTORED_C_24x8_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C24B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 * - zmm20, 21, 23 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_24x8_B0                                                               \
	STORE_COLSTORED_C24B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C24B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C24B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C24B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C24B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C24B0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))   \
	STORE_COLSTORED_C24B0(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))   \
	STORE_COLSTORED_C24B0(20, 21, 23, mem(rdx, r13, 1), 0x40(rdx, r13, 1), 0x80(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_20x8_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C20B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 * - zmm20, 21, 23 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_20x8_B0                                                               \
	STORE_COLSTORED_C20B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
    STORE_COLSTORED_C20B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C20B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C20B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C20B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C20B0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))   \
	STORE_COLSTORED_C20B0(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))   \
	STORE_COLSTORED_C20B0(20, 21, 23, mem(rdx, r13, 1), 0x40(rdx, r13, 1), 0x80(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_18x8_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C18B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 * - zmm20, 21, 23 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_18x8_B0                                                               \
	STORE_COLSTORED_C18B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C18B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C18B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C18B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C18B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C18B0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))   \
	STORE_COLSTORED_C18B0(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))   \
	STORE_COLSTORED_C18B0(20, 21, 23, mem(rdx, r13, 1), 0x40(rdx, r13, 1), 0x80(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_17x8_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C17B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 * - zmm20, 21, 23 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_17x8_B0                                                               \
	STORE_COLSTORED_C17B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C17B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C17B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C17B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C17B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C17B0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))   \
	STORE_COLSTORED_C17B0(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))   \
	STORE_COLSTORED_C17B0(20, 21, 23, mem(rdx, r13, 1), 0x40(rdx, r13, 1), 0x80(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_16x8_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C16B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 * - zmm18, 19 holds A * B for C[0, 6]
 * - zmm20, 21 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_16x8_B0                                       \
	STORE_COLSTORED_C16B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C16B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C16B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C16B0(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C16B0(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))  \
	STORE_COLSTORED_C16B0(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))  \
	STORE_COLSTORED_C16B0(20, 21, mem(rdx, r13, 1), 0x40(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_24MASKx8_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C24MASKB0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next m_left elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next m_left elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 * - zmm20, 21, 23 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_24MASKx8_B0                                                               \
	STORE_COLSTORED_C24MASKB0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C24MASKB0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C24MASKB0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C24MASKB0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C24MASKB0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C24MASKB0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))   \
	STORE_COLSTORED_C24MASKB0(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))   \
	STORE_COLSTORED_C24MASKB0(20, 21, 23, mem(rdx, r13, 1), 0x40(rdx, r13, 1), 0x80(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_12x8_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C12B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 * - zmm18, 19 holds A * B for C[0, 6]
 * - zmm20, 21 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_12x8_B0                                       \
	STORE_COLSTORED_C12B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C12B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C12B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C12B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C12B0(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C12B0(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))  \
	STORE_COLSTORED_C12B0(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))  \
	STORE_COLSTORED_C12B0(20, 21, mem(rdx, r13, 1), 0x40(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_11x8_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C11B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - 0x50(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - 0x50(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 * - zmm20, 21, 23 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_11x8_B0                                                               \
	STORE_COLSTORED_C11B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x50(rcx))                             \
	STORE_COLSTORED_C11B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x50(rcx, rdi, 1))     \
	STORE_COLSTORED_C11B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x50(rcx, rdi, 2))   \
	STORE_COLSTORED_C11B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x50(rcx, r13, 1))   \
	STORE_COLSTORED_C11B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x50(rdx))                           \
	STORE_COLSTORED_C11B0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x50(rdx, rdi, 1))   \
	STORE_COLSTORED_C11B0(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x50(rdx, rdi, 2))   \
	STORE_COLSTORED_C11B0(20, 21, 23, mem(rdx, r13, 1), 0x40(rdx, r13, 1), 0x50(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_10x8_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C10B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 * - zmm18, 19 holds A * B for C[0, 6]
 * - zmm20, 21 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_10x8_B0                                       \
	STORE_COLSTORED_C10B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C10B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C10B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C10B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C10B0(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C10B0(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))  \
	STORE_COLSTORED_C10B0(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))  \
	STORE_COLSTORED_C10B0(20, 21, mem(rdx, r13, 1), 0x40(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_9x8_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C9B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 * - zmm18, 19 holds A * B for C[0, 6]
 * - zmm20, 21 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_9x8_B0                                        \
	STORE_COLSTORED_C9B0(6, 7, mem(rcx), 0x40(rcx))                     \
	STORE_COLSTORED_C9B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))     \
	STORE_COLSTORED_C9B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))   \
	STORE_COLSTORED_C9B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))   \
	STORE_COLSTORED_C9B0(14, 15, mem(rdx), 0x40(rdx))                   \
	STORE_COLSTORED_C9B0(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))   \
	STORE_COLSTORED_C9B0(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))   \
	STORE_COLSTORED_C9B0(20, 21, mem(rdx, r13, 1), 0x40(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_8x8_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx, rdx: Base registers for memory addresses.
 * - rdi, r13: Offset registers for memory addresses.
 */
#define STORE_COLSTORED_C_8x8_B0                \
	STORE_COLSTORED_C8B0(6, mem(rcx))           \
	STORE_COLSTORED_C8B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C8B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C8B0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C8B0(14, mem(rdx))          \
	STORE_COLSTORED_C8B0(16, mem(rdx, rdi, 1))  \
	STORE_COLSTORED_C8B0(18, mem(rdx, rdi, 2))  \
	STORE_COLSTORED_C8B0(20, mem(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_16MASKx8_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C16MASKB0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next m_left elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next m_left elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 * - zmm18, 19 holds A * B for C[0, 6]
 * - zmm20, 21 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_16MASKx8_B0                                       \
	STORE_COLSTORED_C16MASKB0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16MASKB0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C16MASKB0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C16MASKB0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C16MASKB0(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C16MASKB0(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))  \
	STORE_COLSTORED_C16MASKB0(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))  \
	STORE_COLSTORED_C16MASKB0(20, 21, mem(rdx, r13, 1), 0x40(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_4x8_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C4B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 * - zmm18 holds A * B for C[0, 6]
 * - zmm20 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_4x8_B0                \
	STORE_COLSTORED_C4B0(6, mem(rcx))           \
	STORE_COLSTORED_C4B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C4B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C4B0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C4B0(14, mem(rdx))          \
	STORE_COLSTORED_C4B0(16, mem(rdx, rdi, 1))  \
	STORE_COLSTORED_C4B0(18, mem(rdx, rdi, 2))  \
	STORE_COLSTORED_C4B0(20, mem(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_2x8_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C2B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 * - zmm18 holds A * B for C[0, 6]
 * - zmm20 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_2x8_B0                \
    STORE_COLSTORED_C2B0(6, mem(rcx))           \
    STORE_COLSTORED_C2B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C2B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C2B0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C2B0(14, mem(rdx))          \
	STORE_COLSTORED_C2B0(16, mem(rdx, rdi, 1))  \
	STORE_COLSTORED_C2B0(18, mem(rdx, rdi, 2))  \
	STORE_COLSTORED_C2B0(20, mem(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_1x8_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C1B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 * - zmm18 holds A * B for C[0, 6]
 * - zmm20 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_1x8_B0                \
	STORE_COLSTORED_C1B0(6, mem(rcx))           \
	STORE_COLSTORED_C1B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C1B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C1B0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C1B0(14, mem(rdx))          \
	STORE_COLSTORED_C1B0(16, mem(rdx, rdi, 1))  \
	STORE_COLSTORED_C1B0(18, mem(rdx, rdi, 2))  \
	STORE_COLSTORED_C1B0(20, mem(rdx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_8MASKx8_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8MASKB0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] m_left elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] m_left elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 * - zmm18 holds A * B for C[0, 6]
 * - zmm20 holds A * B for C[0, 7]
 */
#define STORE_COLSTORED_C_8MASKx8_B0                \
	STORE_COLSTORED_C8MASKB0(6, mem(rcx))           \
	STORE_COLSTORED_C8MASKB0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C8MASKB0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C8MASKB0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C8MASKB0(14, mem(rdx))          \
	STORE_COLSTORED_C8MASKB0(16, mem(rdx, rdi, 1))  \
	STORE_COLSTORED_C8MASKB0(18, mem(rdx, rdi, 2))  \
	STORE_COLSTORED_C8MASKB0(20, mem(rdx, r13, 1))

/**************************END COLSTORED C STORE Bn x8********************************/

/**************************START COLSTORED C STORE Bn x7********************************/
/**
 * @brief STORE_COLSTORED_C_24x7_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C24Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_24x7_Bn                                                                   \
	STORE_COLSTORED_C24Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                                 \
	STORE_COLSTORED_C24Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))         \
	STORE_COLSTORED_C24Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))       \
	STORE_COLSTORED_C24Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))       \
	STORE_COLSTORED_C24Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                               \
	STORE_COLSTORED_C24Bn(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))       \
	STORE_COLSTORED_C24Bn(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_20x7_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C20Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_20x7_Bn                                                               \
	STORE_COLSTORED_C20Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C20Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C20Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C20Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C20Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C20Bn(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))   \
	STORE_COLSTORED_C20Bn(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_18x7_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C18Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_18x7_Bn                                                               \
	STORE_COLSTORED_C18Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C18Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C18Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C18Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C18Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C18Bn(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))   \
	STORE_COLSTORED_C18Bn(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_17x7_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C17Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_17x7_Bn                                                               \
	STORE_COLSTORED_C17Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C17Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C17Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C17Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C17Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C17Bn(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))   \
	STORE_COLSTORED_C17Bn(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_16x7_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C16Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 * - zmm18, 19 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_16x7_Bn                                       \
	STORE_COLSTORED_C16Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C16Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C16Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C16Bn(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C16Bn(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))  \
	STORE_COLSTORED_C16Bn(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))


#define STORE_COLSTORED_C_24MASKx7_Bn                   \
	vmulpd( zmm30, zmm6, zmm6 )                         \
	vfmadd231pd( mem(rcx), zmm31, zmm6)                 \
	vmovupd( zmm6, mem(rcx))                            \
                                                        \
	vmulpd( zmm30, zmm7, zmm7 )                         \
	vfmadd231pd( 0x40(rcx), zmm31, zmm7)                \
	vmovupd( zmm7, 0x40(rcx))                           \
                                                        \
	vmulpd( zmm30, zmm8, zmm8 )                         \
	vfmadd231pd( mem(rcx, rdi, 1), zmm31, zmm8)         \
	vmovupd( zmm8, mem(rcx, rdi, 1))                    \
                                                        \
	vmulpd( zmm30, zmm9, zmm9 )                         \
	vfmadd231pd( 0x40(rcx, rdi, 1), zmm31, zmm9)        \
	vmovupd( zmm9, 0x40(rcx, rdi, 1))                   \
                                                        \
	vmulpd( zmm30, zmm10, zmm10 )                       \
	vfmadd231pd( mem(rcx, rdi, 2), zmm31, zmm10)        \
	vmovupd( zmm10, mem(rcx, rdi, 2))                   \
                                                        \
	vmulpd( zmm30, zmm11, zmm11 )                       \
	vfmadd231pd( 0x40(rcx, rdi, 2), zmm31, zmm11)       \
	vmovupd( zmm11, 0x40(rcx, rdi, 2))                  \
                                                        \
	vmulpd( zmm30, zmm12, zmm12 )                       \
	vfmadd231pd( mem(rcx, r13, 1), zmm31, zmm12)        \
	vmovupd( zmm12, mem(rcx, r13, 1))                   \
                                                        \
	vmulpd( zmm30, zmm13, zmm13 )                       \
	vfmadd231pd( 0x40(rcx, r13, 1), zmm31, zmm13)       \
	vmovupd( zmm13, 0x40(rcx, r13, 1))                  \
                                                        \
	vmulpd( zmm30, zmm14, zmm14 )                       \
	vfmadd231pd( mem(rdx), zmm31, zmm14)                \
	vmovupd( zmm14, mem(rdx))                           \
                                                        \
	vmulpd( zmm30, zmm15, zmm15 )                       \
	vfmadd231pd( 0x40(rdx), zmm31, zmm15)               \
	vmovupd( zmm15, 0x40(rdx))                          \
                                                        \
	vmulpd( zmm30, zmm16, zmm16 )                       \
	vfmadd231pd( mem(rdx, rdi, 1), zmm31, zmm16)        \
	vmovupd( zmm16, mem(rdx, rdi, 1))                   \
                                                        \
	vmulpd( zmm30, zmm17, zmm17 )                       \
	vfmadd231pd( 0x40(rdx, rdi, 1), zmm31, zmm17)       \
	vmovupd( zmm17, 0x40(rdx, rdi, 1))                  \
                                                        \
	vmulpd( zmm30, zmm18, zmm18 )                       \
	vfmadd231pd( mem(rdx, rdi, 2), zmm31, zmm18)        \
	vmovupd( zmm18, mem(rdx, rdi, 2))                   \
                                                        \
	vmulpd( zmm30, zmm19, zmm19 )                       \
	vfmadd231pd( 0x40(rdx, rdi, 2), zmm31, zmm19)       \
	vmovupd( zmm19, 0x40(rdx, rdi, 2))                  \
                                                        \
	vmulpd( zmm30, zmm28, zmm28 )                       \
	vmovupd( 0x80(rcx), zmm0 MASK_KZ(2))                \
                                                        \
	vfmadd231pd( zmm0, zmm31, zmm28)                    \
	vmovupd( zmm28, 0x80(rcx) MASK_(k(2)))              \
                                                        \
	vmulpd( zmm30, zmm29, zmm29 )                       \
	vmovupd( 0x80(rcx, rdi, 1), zmm1 MASK_KZ(2))        \
                                                        \
	vfmadd231pd( zmm1, zmm31, zmm29)                    \
	vmovupd( zmm29, 0x80(rcx, rdi, 1) MASK_(k(2)))      \
                                                        \
	vmulpd( zmm30, zmm26, zmm26 )                       \
	vmovupd( 0x80(rcx, rdi, 2),zmm2 MASK_KZ(2))         \
                                                        \
	vfmadd231pd( zmm2, zmm31, zmm26)                    \
	vmovupd( zmm26, 0x80(rcx, rdi, 2) MASK_(k(2)))      \
                                                        \
	vmulpd( zmm30, zmm27, zmm27 )                       \
	vmovupd( 0x80(rcx, r13, 1), zmm3 MASK_KZ(2))        \
                                                        \
	vfmadd231pd( zmm3, zmm31, zmm27)                    \
	vmovupd( zmm27, 0x80(rcx, r13, 1) MASK_(k(2)))      \
                                                        \
	vmulpd( zmm30, zmm24, zmm24 )                       \
	vmovupd( 0x80(rdx), zmm4 MASK_KZ(2))                \
                                                        \
	vfmadd231pd( zmm4, zmm31, zmm24)                    \
	vmovupd( zmm24, 0x80(rdx) MASK_(k(2)))              \
                                                        \
	vmulpd( zmm30, zmm25, zmm25 )                       \
	vmovupd( 0x80(rdx, rdi, 1), zmm5 MASK_KZ(2))        \
                                                        \
	vfmadd231pd( zmm5, zmm31, zmm25)                    \
	vmovupd( zmm25, 0x80(rdx, rdi, 1) MASK_(k(2)))      \
                                                        \
	vmulpd( zmm30, zmm22, zmm22 )                       \
	vmovupd( 0x80(rdx, rdi, 2), zmm6 MASK_KZ(2))        \
                                                        \
	vfmadd231pd( zmm6, zmm31, zmm22)                    \
	vmovupd( zmm22, 0x80(rdx, rdi, 2) MASK_(k(2)))

/**
 * @brief STORE_COLSTORED_C_12x7_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C12Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 * - zmm18, 19 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_12x7_Bn                                       \
	STORE_COLSTORED_C12Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C12Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C12Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C12Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C12Bn(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C12Bn(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))  \
	STORE_COLSTORED_C12Bn(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_11x7_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C11Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - 0x50(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - 0x50(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_11x7_Bn                                                               \
	STORE_COLSTORED_C11Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x50(rcx))                             \
	STORE_COLSTORED_C11Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x50(rcx, rdi, 1))     \
	STORE_COLSTORED_C11Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x50(rcx, rdi, 2))   \
	STORE_COLSTORED_C11Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x50(rcx, r13, 1))   \
	STORE_COLSTORED_C11Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x50(rdx))                           \
	STORE_COLSTORED_C11Bn(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x50(rdx, rdi, 1))   \
	STORE_COLSTORED_C11Bn(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x50(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_10x7_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C10Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 * - zmm18, 19 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_10x7_Bn                                       \
	STORE_COLSTORED_C10Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C10Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C10Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C10Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C10Bn(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C10Bn(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))  \
	STORE_COLSTORED_C10Bn(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_9x7_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C9Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 * - zmm18, 19 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_9x7_Bn                                        \
	STORE_COLSTORED_C9Bn(6, 7, mem(rcx), 0x40(rcx))                     \
	STORE_COLSTORED_C9Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))     \
	STORE_COLSTORED_C9Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))   \
	STORE_COLSTORED_C9Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))   \
	STORE_COLSTORED_C9Bn(14, 15, mem(rdx), 0x40(rdx))                   \
	STORE_COLSTORED_C9Bn(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))   \
	STORE_COLSTORED_C9Bn(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_8x7_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 * - zmm18 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_8x7_Bn                \
	STORE_COLSTORED_C8Bn(6, mem(rcx))           \
	STORE_COLSTORED_C8Bn(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C8Bn(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C8Bn(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C8Bn(14, mem(rdx))          \
	STORE_COLSTORED_C8Bn(16, mem(rdx, rdi, 1))  \
	STORE_COLSTORED_C8Bn(18, mem(rdx, rdi, 2))

#define STORE_COLSTORED_C_16MASKx7_Bn               \
	vmulpd( zmm30, zmm6, zmm6 )                     \
	vfmadd231pd( mem(rcx), zmm31, zmm6)             \
	vmovupd( zmm6, mem(rcx))                        \
                                                    \
	vmulpd( zmm30, zmm8, zmm8 )                     \
	vfmadd231pd( mem(rcx, rdi, 1), zmm31, zmm8)     \
	vmovupd( zmm8, mem(rcx, rdi, 1))                \
                                                    \
	vmulpd( zmm30, zmm10, zmm10 )                   \
	vfmadd231pd( mem(rcx, rdi, 2), zmm31, zmm10)    \
	vmovupd( zmm10, mem(rcx, rdi, 2))               \
                                                    \
	vmulpd( zmm30, zmm12, zmm12 )                   \
	vfmadd231pd( mem(rcx, r13, 1), zmm31, zmm12)    \
	vmovupd( zmm12, mem(rcx, r13, 1))               \
                                                    \
	vmulpd( zmm30, zmm14, zmm14 )                   \
	vfmadd231pd( mem(rdx), zmm31, zmm14)            \
	vmovupd( zmm14, mem(rdx))                       \
                                                    \
	vmulpd( zmm30, zmm16, zmm16 )                   \
	vfmadd231pd( mem(rdx, rdi, 1), zmm31, zmm16)    \
	vmovupd( zmm16, mem(rdx, rdi, 1))               \
                                                    \
	vmulpd( zmm30, zmm18, zmm18 )                   \
	vfmadd231pd( mem(rdx, rdi, 2), zmm31, zmm18)    \
	vmovupd( zmm18, mem(rdx, rdi, 2))               \
                                                    \
	vmulpd( zmm30, zmm7, zmm7 )                     \
	vmovupd( 0x40(rcx), zmm0 MASK_KZ(2))            \
                                                    \
	vfmadd231pd( zmm0, zmm31, zmm7)                 \
	vmovupd( zmm7, 0x40(rcx) MASK_(k(2)))           \
                                                    \
	vmulpd( zmm30, zmm9, zmm9 )                     \
	vmovupd( 0x40(rcx, rdi, 1), zmm1 MASK_KZ(2))    \
                                                    \
	vfmadd231pd( zmm1, zmm31, zmm9)                 \
	vmovupd( zmm9, 0x40(rcx, rdi, 1) MASK_(k(2)))   \
                                                    \
	vmulpd( zmm30, zmm11, zmm11 )                   \
	vmovupd( 0x40(rcx, rdi, 2), zmm2 MASK_KZ(2))    \
                                                    \
	vfmadd231pd( zmm2, zmm31, zmm11)                \
	vmovupd( zmm11, 0x40(rcx, rdi, 2) MASK_(k(2)))  \
                                                    \
	vmulpd( zmm30, zmm13, zmm13 )                   \
	vmovupd( 0x40(rcx, r13, 1),zmm3 MASK_KZ(2))     \
                                                    \
	vfmadd231pd( zmm3, zmm31, zmm13)                \
	vmovupd( zmm13, 0x40(rcx, r13, 1) MASK_(k(2)))  \
                                                    \
	vmulpd( zmm30, zmm15, zmm15 )                   \
	vmovupd( 0x40(rdx), zmm4 MASK_KZ(2))            \
                                                    \
	vfmadd231pd( zmm4, zmm31, zmm15)                \
	vmovupd( zmm15, 0x40(rdx) MASK_(k(2)))          \
                                                    \
	vmulpd( zmm30, zmm17, zmm17 )                   \
	vmovupd( 0x40(rdx, rdi, 1), zmm5 MASK_KZ(2))    \
                                                    \
	vfmadd231pd( zmm5, zmm31, zmm17)                \
	vmovupd( zmm17, 0x40(rdx, rdi, 1) MASK_(k(2)))  \
                                                    \
	vmulpd( zmm30, zmm19, zmm19 )                   \
	vmovupd( 0x40(rdx, rdi, 2), zmm6 MASK_KZ(2))    \
                                                    \
	vfmadd231pd( zmm6, zmm31, zmm19)                \
	vmovupd( zmm19, 0x40(rdx, rdi, 2) MASK_(k(2)))

/**
 * @brief STORE_COLSTORED_C_4x7_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C4Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 * - zmm18 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_4x7_Bn                    \
	STORE_COLSTORED_C4Bn(0, 6, mem(rcx))            \
	STORE_COLSTORED_C4Bn(1, 8, mem(rcx, rdi, 1))    \
	STORE_COLSTORED_C4Bn(2, 10, mem(rcx, rdi, 2))   \
	STORE_COLSTORED_C4Bn(3, 12, mem(rcx, r13, 1))   \
	STORE_COLSTORED_C4Bn(4, 14, mem(rdx))           \
	STORE_COLSTORED_C4Bn(5, 16, mem(rdx, rdi, 1))   \
	STORE_COLSTORED_C4Bn(6, 18, mem(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_2x7_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C2Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 * - zmm18 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_2x7_Bn                    \
	STORE_COLSTORED_C2Bn(0, 6, mem(rcx))            \
	STORE_COLSTORED_C2Bn(1, 8, mem(rcx, rdi, 1))    \
	STORE_COLSTORED_C2Bn(2, 10, mem(rcx, rdi, 2))   \
	STORE_COLSTORED_C2Bn(3, 12, mem(rcx, r13, 1))   \
	STORE_COLSTORED_C2Bn(4, 14, mem(rdx))           \
	STORE_COLSTORED_C2Bn(5, 16, mem(rdx, rdi, 1))   \
	STORE_COLSTORED_C2Bn(6, 18, mem(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_1x7_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C1Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 * - zmm18 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_1x7_Bn                    \
	STORE_COLSTORED_C1Bn(0, 6, mem(rcx))            \
	STORE_COLSTORED_C1Bn(1, 8, mem(rcx, rdi, 1))    \
	STORE_COLSTORED_C1Bn(2, 10, mem(rcx, rdi, 2))   \
	STORE_COLSTORED_C1Bn(3, 12, mem(rcx, r13, 1))   \
	STORE_COLSTORED_C1Bn(4, 14, mem(rdx))           \
	STORE_COLSTORED_C1Bn(5, 16, mem(rdx, rdi, 1))   \
	STORE_COLSTORED_C1Bn(6, 18, mem(rdx, rdi, 2))

#define STORE_COLSTORED_C_8MASKx7_Bn                \
    vmulpd( zmm30, zmm6, zmm6 )                     \
    vmovupd( mem(rcx), zmm0 MASK_KZ(2))             \
                                                    \
    vmulpd( zmm30, zmm8, zmm8 )                     \
    vmovupd( mem(rcx, rdi, 1), zmm1 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm10, zmm10 )                   \
    vmovupd( mem(rcx, rdi, 2), zmm2 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm12, zmm12 )                   \
    vmovupd( mem(rcx, r13, 1), zmm3 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm14, zmm14 )                   \
    vmovupd( mem(rdx), zmm4 MASK_KZ(2))             \
                                                    \
    vmulpd( zmm30, zmm16, zmm16 )                   \
    vmovupd( mem(rdx, rdi, 1), zmm5 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm18, zmm18 )                   \
    vmovupd( mem(rdx, rdi, 2), zmm7 MASK_KZ(2))     \
                                                    \
    vfmadd231pd( zmm0, zmm31, zmm6)                 \
    vmovupd( zmm6, mem(rcx) MASK_(k(2)))            \
                                                    \
    vfmadd231pd( zmm1, zmm31, zmm8)                 \
    vmovupd( zmm8, mem(rcx, rdi, 1) MASK_(k(2)))    \
                                                    \
    vfmadd231pd( zmm2, zmm31, zmm10)                \
    vmovupd( zmm10, mem(rcx, rdi, 2) MASK_(k(2)))   \
                                                    \
    vfmadd231pd( zmm3, zmm31, zmm12)                \
    vmovupd( zmm12, mem(rcx, r13, 1) MASK_(k(2)))   \
                                                    \
    vfmadd231pd( zmm4, zmm31, zmm14)                \
    vmovupd( zmm14, mem(rdx) MASK_(k(2)))           \
                                                    \
    vfmadd231pd( zmm5, zmm31, zmm16)                \
    vmovupd( zmm16, mem(rdx, rdi, 1) MASK_(k(2)))   \
                                                    \
    vfmadd231pd( zmm7, zmm31, zmm18)                \
    vmovupd( zmm18, mem(rdx, rdi, 2) MASK_(k(2)))

/************************************************************/
/**
 * @brief STORE_COLSTORED_C_24x7_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C24B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_24x7_B0                                                               \
	STORE_COLSTORED_C24B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C24B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C24B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C24B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C24B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C24B0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))   \
	STORE_COLSTORED_C24B0(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_20x7_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C20B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_20x7_B0                                                               \
	STORE_COLSTORED_C20B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C20B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C20B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C20B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C20B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C20B0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))   \
	STORE_COLSTORED_C20B0(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_18x7_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C18B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_18x7_B0                                                               \
	STORE_COLSTORED_C18B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C18B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C18B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C18B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C18B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C18B0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))   \
	STORE_COLSTORED_C18B0(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))


/**
 * @brief STORE_COLSTORED_C_17x7_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C17B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_17x7_B0                                                               \
	STORE_COLSTORED_C17B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C17B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C17B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C17B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C17B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C17B0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))   \
	STORE_COLSTORED_C17B0(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_16x7_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C16B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 * - zmm18, 19 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_16x7_B0                                       \
	STORE_COLSTORED_C16B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C16B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C16B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C16B0(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C16B0(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))  \
	STORE_COLSTORED_C16B0(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))

#define STORE_COLSTORED_C_24MASKx7_B0                                                               \
	STORE_COLSTORED_C24MASKB0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C24MASKB0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C24MASKB0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C24MASKB0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C24MASKB0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C24MASKB0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))   \
	STORE_COLSTORED_C24MASKB0(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x80(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_12x7_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C12B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 * - zmm18, 19 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_12x7_B0                                       \
	STORE_COLSTORED_C12B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C12B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C12B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C12B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C12B0(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C12B0(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))  \
	STORE_COLSTORED_C12B0(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_11x7_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C11B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - 0x80(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 * - zmm18, 19, 22 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_11x7_B0                                                               \
	STORE_COLSTORED_C11B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x50(rcx))                             \
	STORE_COLSTORED_C11B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x50(rcx, rdi, 1))     \
	STORE_COLSTORED_C11B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x50(rcx, rdi, 2))   \
	STORE_COLSTORED_C11B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x50(rcx, r13, 1))   \
	STORE_COLSTORED_C11B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x50(rdx))                           \
	STORE_COLSTORED_C11B0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x50(rdx, rdi, 1))   \
	STORE_COLSTORED_C11B0(18, 19, 22, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2), 0x50(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_10x7_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C10B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 * - zmm18, 19 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_10x7_B0                                       \
	STORE_COLSTORED_C10B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C10B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C10B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C10B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C10B0(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C10B0(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))  \
	STORE_COLSTORED_C10B0(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_9x7_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C9B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 * - zmm18, 19 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_9x7_B0                                        \
	STORE_COLSTORED_C9B0(6, 7, mem(rcx), 0x40(rcx))                     \
	STORE_COLSTORED_C9B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))     \
	STORE_COLSTORED_C9B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))   \
	STORE_COLSTORED_C9B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))   \
	STORE_COLSTORED_C9B0(14, 15, mem(rdx), 0x40(rdx))                   \
	STORE_COLSTORED_C9B0(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))   \
	STORE_COLSTORED_C9B0(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_8x7_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 * - zmm18 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_8x7_B0                \
	STORE_COLSTORED_C8B0(6, mem(rcx))           \
	STORE_COLSTORED_C8B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C8B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C8B0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C8B0(14, mem(rdx))          \
	STORE_COLSTORED_C8B0(16, mem(rdx, rdi, 1))  \
	STORE_COLSTORED_C8B0(18, mem(rdx, rdi, 2))

#define STORE_COLSTORED_C_16MASKx7_B0                                       \
	STORE_COLSTORED_C16MASKB0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16MASKB0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C16MASKB0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C16MASKB0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C16MASKB0(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C16MASKB0(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))  \
	STORE_COLSTORED_C16MASKB0(18, 19, mem(rdx, rdi, 2), 0x40(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_4x7_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C4B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 * - zmm18 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_4x7_B0                \
	STORE_COLSTORED_C4B0(6, mem(rcx))           \
	STORE_COLSTORED_C4B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C4B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C4B0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C4B0(14, mem(rdx))          \
	STORE_COLSTORED_C4B0(16, mem(rdx, rdi, 1))  \
	STORE_COLSTORED_C4B0(18, mem(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_2x7_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C2B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 * - zmm18 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_2x7_B0                \
	STORE_COLSTORED_C2B0(6, mem(rcx))           \
	STORE_COLSTORED_C2B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C2B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C2B0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C2B0(14, mem(rdx))          \
	STORE_COLSTORED_C2B0(16, mem(rdx, rdi, 1))  \
	STORE_COLSTORED_C2B0(18, mem(rdx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_1x7_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C1B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 * - zmm18 holds A * B for C[0, 6]
 */
#define STORE_COLSTORED_C_1x7_B0                \
	STORE_COLSTORED_C1B0(6, mem(rcx))           \
	STORE_COLSTORED_C1B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C1B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C1B0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C1B0(14, mem(rdx))          \
	STORE_COLSTORED_C1B0(16, mem(rdx, rdi, 1))  \
	STORE_COLSTORED_C1B0(18, mem(rdx, rdi, 2))

#define STORE_COLSTORED_C_8MASKx7_B0                \
	STORE_COLSTORED_C8MASKB0(6, mem(rcx))           \
	STORE_COLSTORED_C8MASKB0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C8MASKB0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C8MASKB0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C8MASKB0(14, mem(rdx))          \
	STORE_COLSTORED_C8MASKB0(16, mem(rdx, rdi, 1))  \
	STORE_COLSTORED_C8MASKB0(18, mem(rdx, rdi, 2))

/**************************END COLSTORED C STORE Bn x7********************************/

/**************************START COLSTORED C STORE Bn x6********************************/
/**
 * STORE_COLSTORED_C_24x6_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C24Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_24x6_Bn                                                               \
	STORE_COLSTORED_C24Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C24Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C24Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C24Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C24Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C24Bn(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_20x6_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C20Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_20x6_Bn                                                               \
	STORE_COLSTORED_C20Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C20Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C20Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C20Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C20Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C20Bn(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_18x6_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C18Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_18x6_Bn                                                               \
	STORE_COLSTORED_C18Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C18Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C18Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C18Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C18Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C18Bn(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_17x6_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C17Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_17x6_Bn                                                               \
	STORE_COLSTORED_C17Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C17Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C17Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C17Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C17Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C17Bn(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_16x6_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C16Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_16x6_Bn                                       \
	STORE_COLSTORED_C16Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C16Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C16Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C16Bn(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C16Bn(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))

#define STORE_COLSTORED_C_24MASKx6_Bn                   \
	vmulpd( zmm30, zmm6, zmm6 )                         \
	vfmadd231pd( mem(rcx), zmm31, zmm6)                 \
	vmovupd( zmm6, mem(rcx))                            \
                                                        \
	vmulpd( zmm30, zmm7, zmm7 )                         \
	vfmadd231pd( 0x40(rcx), zmm31, zmm7)                \
	vmovupd( zmm7, 0x40(rcx))                           \
                                                        \
	vmulpd( zmm30, zmm8, zmm8 )                         \
	vfmadd231pd( mem(rcx, rdi, 1), zmm31, zmm8)         \
	vmovupd( zmm8, mem(rcx, rdi, 1))                    \
                                                        \
	vmulpd( zmm30, zmm9, zmm9 )                         \
	vfmadd231pd( 0x40(rcx, rdi, 1), zmm31, zmm9)        \
	vmovupd( zmm9, 0x40(rcx, rdi, 1))                   \
                                                        \
	vmulpd( zmm30, zmm10, zmm10 )                       \
	vfmadd231pd( mem(rcx, rdi, 2), zmm31, zmm10)        \
	vmovupd( zmm10, mem(rcx, rdi, 2))                   \
                                                        \
	vmulpd( zmm30,zmm11, zmm11 )                        \
	vfmadd231pd( 0x40(rcx, rdi, 2),zmm31,zmm11)         \
	vmovupd( zmm11, 0x40(rcx, rdi, 2))                  \
                                                        \
	vmulpd( zmm30, zmm12, zmm12 )                       \
	vfmadd231pd( mem(rcx, r13, 1), zmm31, zmm12)        \
	vmovupd( zmm12, mem(rcx, r13, 1))                   \
                                                        \
	vmulpd( zmm30, zmm13, zmm13 )                       \
	vfmadd231pd( 0x40(rcx, r13, 1), zmm31, zmm13)       \
	vmovupd( zmm13, 0x40(rcx, r13, 1))                  \
                                                        \
	vmulpd( zmm30, zmm14, zmm14 )                       \
	vfmadd231pd( mem(rdx), zmm31, zmm14)                \
	vmovupd( zmm14, mem(rdx))                           \
                                                        \
	vmulpd( zmm30, zmm15, zmm15 )                       \
	vfmadd231pd( 0x40(rdx), zmm31, zmm15)               \
	vmovupd( zmm15, 0x40(rdx))                          \
                                                        \
	vmulpd( zmm30, zmm16, zmm16 )                       \
	vfmadd231pd( mem(rdx, rdi, 1), zmm31, zmm16)        \
	vmovupd( zmm16, mem(rdx, rdi, 1))                   \
                                                        \
	vmulpd( zmm30, zmm17, zmm17 )                       \
	vfmadd231pd( 0x40(rdx, rdi, 1), zmm31, zmm17)       \
	vmovupd( zmm17, 0x40(rdx, rdi, 1))                  \
                                                        \
	vmulpd( zmm30, zmm28, zmm28 )                       \
	vmovupd( 0x80(rcx), zmm0 MASK_KZ(2))                \
                                                        \
	vfmadd231pd( zmm0, zmm31, zmm28)                    \
	vmovupd( zmm28, 0x80(rcx) MASK_(k(2)))              \
                                                        \
	vmulpd( zmm30, zmm29, zmm29 )                       \
	vmovupd( 0x80(rcx, rdi, 1),zmm1 MASK_KZ(2))         \
                                                        \
	vfmadd231pd( zmm1, zmm31, zmm29)                    \
	vmovupd( zmm29, 0x80(rcx, rdi, 1) MASK_(k(2)))      \
                                                        \
	vmulpd( zmm30, zmm26, zmm26 )                       \
	vmovupd( 0x80(rcx, rdi, 2),zmm2 MASK_KZ(2))         \
                                                        \
	vfmadd231pd( zmm2, zmm31, zmm26)                    \
	vmovupd( zmm26, 0x80(rcx, rdi, 2) MASK_(k(2)))      \
                                                        \
	vmulpd( zmm30, zmm27, zmm27 )                       \
	vmovupd( 0x80(rcx, r13, 1),zmm3 MASK_KZ(2))         \
                                                        \
	vfmadd231pd( zmm3, zmm31, zmm27)                    \
	vmovupd( zmm27, 0x80(rcx, r13, 1) MASK_(k(2)))      \
                                                        \
	vmulpd( zmm30, zmm24, zmm24 )                       \
	vmovupd( 0x80(rdx), zmm4 MASK_KZ(2))                \
                                                        \
	vfmadd231pd( zmm4, zmm31, zmm24)                    \
	vmovupd( zmm24, 0x80(rdx) MASK_(k(2)))              \
                                                        \
	vmulpd( zmm30, zmm25, zmm25 )                       \
	vmovupd( 0x80(rdx, rdi, 1),zmm5 MASK_KZ(2))         \
                                                        \
	vfmadd231pd( zmm5, zmm31, zmm25)                    \
	vmovupd( zmm25, 0x80(rdx, rdi, 1) MASK_(k(2)))

/**
 * STORE_COLSTORED_C_12x6_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C12Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_12x6_Bn                                       \
	STORE_COLSTORED_C12Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C12Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C12Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C12Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C12Bn(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C12Bn(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_11x6_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C11Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - 0x50(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - 0x50(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_11x6_Bn                                                               \
	STORE_COLSTORED_C11Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x50(rcx))                             \
	STORE_COLSTORED_C11Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x50(rcx, rdi, 1))     \
	STORE_COLSTORED_C11Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x50(rcx, rdi, 2))   \
	STORE_COLSTORED_C11Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x50(rcx, r13, 1))   \
	STORE_COLSTORED_C11Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x50(rdx))                           \
	STORE_COLSTORED_C11Bn(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x50(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_10x6_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C10Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_10x6_Bn                                       \
	STORE_COLSTORED_C10Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C10Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C10Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C10Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C10Bn(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C10Bn(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_9x6_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C9Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_9x6_Bn                                        \
	STORE_COLSTORED_C9Bn(6, 7, mem(rcx), 0x40(rcx))                     \
	STORE_COLSTORED_C9Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))     \
	STORE_COLSTORED_C9Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))   \
	STORE_COLSTORED_C9Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))   \
	STORE_COLSTORED_C9Bn(14, 15, mem(rdx), 0x40(rdx))                   \
	STORE_COLSTORED_C9Bn(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_8x6_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_8x6_Bn                \
	STORE_COLSTORED_C8Bn(6, mem(rcx))           \
	STORE_COLSTORED_C8Bn(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C8Bn(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C8Bn(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C8Bn(14, mem(rdx))          \
	STORE_COLSTORED_C8Bn(16, mem(rdx, rdi, 1))

#define STORE_COLSTORED_C_16MASKx6_Bn                       \
	vmulpd( zmm30, zmm6, zmm6 )                             \
	vfmadd231pd( mem(rcx), zmm31, zmm6)                     \
	vmovupd( zmm6, mem(rcx))                                \
                                                            \
	vmulpd( zmm30, zmm8, zmm8 )                             \
	vfmadd231pd( mem(rcx, rdi, 1), zmm31, zmm8)             \
	vmovupd( zmm8, mem(rcx, rdi, 1))                        \
                                                            \
	vmulpd( zmm30, zmm10, zmm10 )                           \
	vfmadd231pd( mem(rcx, rdi, 2), zmm31, zmm10)            \
	vmovupd( zmm10, mem(rcx, rdi, 2))                       \
                                                            \
	vmulpd( zmm30, zmm12, zmm12 )                           \
	vfmadd231pd( mem(rcx, r13, 1), zmm31, zmm12)            \
	vmovupd( zmm12, mem(rcx, r13, 1))                       \
                                                            \
	vmulpd( zmm30, zmm14, zmm14 )                           \
	vfmadd231pd( mem(rdx), zmm31, zmm14)                    \
	vmovupd( zmm14, mem(rdx))                               \
                                                            \
	vmulpd( zmm30, zmm16, zmm16 )                           \
	vfmadd231pd( mem(rdx, rdi, 1), zmm31, zmm16)            \
	vmovupd( zmm16, mem(rdx, rdi, 1))                       \
                                                            \
	vmulpd( zmm30, zmm7, zmm7 )                             \
	vmovupd( 0x40(rcx), zmm0 MASK_KZ(2))                    \
	vfmadd231pd( zmm0, zmm31, zmm7)                         \
	vmovupd( zmm7, 0x40(rcx) MASK_(k(2)))                   \
                                                            \
	vmulpd( zmm30, zmm9, zmm9 )                             \
	vmovupd( 0x40(rcx, rdi, 1), zmm1 MASK_KZ(2))            \
	vfmadd231pd( zmm1, zmm31, zmm9)                         \
	vmovupd( zmm9, 0x40(rcx, rdi, 1) MASK_(k(2)))           \
                                                            \
	vmulpd( zmm30, zmm11, zmm11 )                           \
	vmovupd( 0x40(rcx, rdi, 2), zmm2 MASK_KZ(2))            \
	vfmadd231pd( zmm2, zmm31, zmm11)                        \
	vmovupd( zmm11, 0x40(rcx, rdi, 2) MASK_(k(2)))          \
                                                            \
	vmulpd( zmm30, zmm13, zmm13 )                           \
	vmovupd( 0x40(rcx, r13, 1), zmm3 MASK_KZ(2))            \
	vfmadd231pd( zmm3, zmm31, zmm13)                        \
	vmovupd( zmm13, 0x40(rcx, r13, 1) MASK_(k(2)))          \
                                                            \
	vmulpd( zmm30, zmm15, zmm15 )                           \
	vmovupd( 0x40(rdx), zmm4 MASK_KZ(2))                    \
	vfmadd231pd( zmm4, zmm31, zmm15)                        \
	vmovupd( zmm15, 0x40(rdx) MASK_(k(2)))                  \
                                                            \
	vmulpd( zmm30, zmm17, zmm17 )                           \
	vmovupd( 0x40(rdx, rdi, 1), zmm5 MASK_KZ(2))            \
	vfmadd231pd( zmm5, zmm31, zmm17)                        \
	vmovupd( zmm17, 0x40(rdx, rdi, 1) MASK_(k(2)))

/**
 * STORE_COLSTORED_C_4x6_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C4Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_4x6_Bn                    \
	STORE_COLSTORED_C4Bn(0, 6, mem(rcx))            \
	STORE_COLSTORED_C4Bn(1, 8, mem(rcx, rdi, 1))    \
	STORE_COLSTORED_C4Bn(2, 10, mem(rcx, rdi, 2))   \
	STORE_COLSTORED_C4Bn(3, 12, mem(rcx, r13, 1))   \
	STORE_COLSTORED_C4Bn(4, 14, mem(rdx))           \
	STORE_COLSTORED_C4Bn(5, 16, mem(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_2x6_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C2Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_2x6_Bn                    \
	STORE_COLSTORED_C2Bn(0, 6, mem(rcx))            \
	STORE_COLSTORED_C2Bn(1, 8, mem(rcx, rdi, 1))    \
	STORE_COLSTORED_C2Bn(2, 10, mem(rcx, rdi, 2))   \
	STORE_COLSTORED_C2Bn(3, 12, mem(rcx, r13, 1))   \
	STORE_COLSTORED_C2Bn(4, 14, mem(rdx))           \
	STORE_COLSTORED_C2Bn(5, 16, mem(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_1x6_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C1Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_1x6_Bn                    \
	STORE_COLSTORED_C1Bn(0, 6, mem(rcx))            \
	STORE_COLSTORED_C1Bn(1, 8, mem(rcx, rdi, 1))    \
	STORE_COLSTORED_C1Bn(2, 10, mem(rcx, rdi, 2))   \
	STORE_COLSTORED_C1Bn(3, 12, mem(rcx, r13, 1))   \
	STORE_COLSTORED_C1Bn(4, 14, mem(rdx))           \
	STORE_COLSTORED_C1Bn(5, 16, mem(rdx, rdi, 1))

#define STORE_COLSTORED_C_8MASKx6_Bn                \
    vmulpd( zmm30, zmm6, zmm6 )                     \
    vmovupd( mem(rcx), zmm0 MASK_KZ(2))             \
                                                    \
    vmulpd( zmm30, zmm8, zmm8 )                     \
    vmovupd( mem(rcx, rdi, 1), zmm1 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm10, zmm10 )                   \
    vmovupd( mem(rcx, rdi, 2), zmm2 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm12, zmm12 )                   \
    vmovupd( mem(rcx, r13, 1), zmm3 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm14, zmm14 )                   \
    vmovupd( mem(rdx), zmm4 MASK_KZ(2))             \
                                                    \
    vmulpd( zmm30, zmm16, zmm16 )                   \
    vmovupd( mem(rdx, rdi, 1), zmm5 MASK_KZ(2))     \
                                                    \
    vfmadd231pd( zmm0, zmm31, zmm6)                 \
    vmovupd( zmm6, mem(rcx) MASK_(k(2)))            \
                                                    \
    vfmadd231pd( zmm1, zmm31, zmm8)                 \
    vmovupd( zmm8, mem(rcx, rdi, 1) MASK_(k(2)))    \
                                                    \
    vfmadd231pd( zmm2, zmm31, zmm10)                \
    vmovupd( zmm10, mem(rcx, rdi, 2) MASK_(k(2)))   \
                                                    \
    vfmadd231pd( zmm3, zmm31, zmm12)                \
    vmovupd( zmm12, mem(rcx, r13, 1) MASK_(k(2)))   \
                                                    \
    vfmadd231pd( zmm4, zmm31, zmm14)                \
    vmovupd( zmm14, mem(rdx) MASK_(k(2)))           \
                                                    \
    vfmadd231pd( zmm5, zmm31, zmm16)                \
    vmovupd( zmm16, mem(rdx, rdi, 1) MASK_(k(2)))

/********************************************************/
/**
 * STORE_COLSTORED_C_24x6_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C24B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_24x6_B0                                                               \
	STORE_COLSTORED_C24B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C24B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C24B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C24B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C24B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C24B0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_20x6_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C20B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_20x6_B0                                                               \
	STORE_COLSTORED_C20B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C20B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C20B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C20B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C20B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C20B0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_18x6_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C18B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_18x6_B0                                                               \
	STORE_COLSTORED_C18B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C18B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C18B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C18B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C18B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C18B0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_17x6_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C17B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_17x6_B0                                                               \
	STORE_COLSTORED_C17B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C17B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C17B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C17B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C17B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C17B0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_16x6_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C16B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_16x6_B0                                       \
	STORE_COLSTORED_C16B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C16B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C16B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C16B0(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C16B0(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))

#define STORE_COLSTORED_C_24MASKx6_B0                                                               \
	STORE_COLSTORED_C24MASKB0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C24MASKB0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C24MASKB0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C24MASKB0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C24MASKB0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))                           \
	STORE_COLSTORED_C24MASKB0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x80(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_12x6_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C12B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_12x6_B0                                       \
	STORE_COLSTORED_C12B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C12B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C12B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C12B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C12B0(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C12B0(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_11x6_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C11B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - 0x50(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - 0x50(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 * - zmm16, 17, 25 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_11x6_B0                                                               \
	STORE_COLSTORED_C11B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x50(rcx))                             \
	STORE_COLSTORED_C11B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x50(rcx, rdi, 1))     \
	STORE_COLSTORED_C11B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x50(rcx, rdi, 2))   \
	STORE_COLSTORED_C11B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x50(rcx, r13, 1))   \
	STORE_COLSTORED_C11B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x50(rdx))                           \
	STORE_COLSTORED_C11B0(16, 17, 25, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1), 0x50(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_10x6_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C10B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_10x6_B0                                       \
	STORE_COLSTORED_C10B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C10B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C10B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C10B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C10B0(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C10B0(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_9x6_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C9B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 * - zmm16, 17 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_9x6_B0                                        \
	STORE_COLSTORED_C9B0(6, 7, mem(rcx), 0x40(rcx))                     \
	STORE_COLSTORED_C9B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))     \
	STORE_COLSTORED_C9B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))   \
	STORE_COLSTORED_C9B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))   \
	STORE_COLSTORED_C9B0(14, 15, mem(rdx), 0x40(rdx))                   \
	STORE_COLSTORED_C9B0(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_8x6_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_8x6_B0                \
	STORE_COLSTORED_C8B0(6, mem(rcx))           \
	STORE_COLSTORED_C8B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C8B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C8B0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C8B0(14, mem(rdx))          \
	STORE_COLSTORED_C8B0(16, mem(rdx, rdi, 1))

#define STORE_COLSTORED_C_16MASKx6_B0                                       \
	STORE_COLSTORED_C16MASKB0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16MASKB0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C16MASKB0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C16MASKB0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C16MASKB0(14, 15, mem(rdx), 0x40(rdx))                  \
	STORE_COLSTORED_C16MASKB0(16, 17, mem(rdx, rdi, 1), 0x40(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_4x6_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C4B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_4x6_B0                \
	STORE_COLSTORED_C4B0(6, mem(rcx))           \
	STORE_COLSTORED_C4B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C4B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C4B0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C4B0(14, mem(rdx))          \
	STORE_COLSTORED_C4B0(16, mem(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_2x6_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C2B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_2x6_B0                \
	STORE_COLSTORED_C2B0(6, mem(rcx))           \
	STORE_COLSTORED_C2B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C2B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C2B0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C2B0(14, mem(rdx))          \
	STORE_COLSTORED_C2B0(16, mem(rdx, rdi, 1))

/**
 * STORE_COLSTORED_C_1x6_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C1B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 * - zmm16 holds A * B for C[0, 5]
 */
#define STORE_COLSTORED_C_1x6_B0                \
	STORE_COLSTORED_C1B0(6, mem(rcx))           \
	STORE_COLSTORED_C1B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C1B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C1B0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C1B0(14, mem(rdx))          \
	STORE_COLSTORED_C1B0(16, mem(rdx, rdi, 1))

#define STORE_COLSTORED_C_8MASKx6_B0                \
	STORE_COLSTORED_C8MASKB0(6, mem(rcx))           \
	STORE_COLSTORED_C8MASKB0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C8MASKB0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C8MASKB0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C8MASKB0(14, mem(rdx))          \
	STORE_COLSTORED_C8MASKB0(16, mem(rdx, rdi, 1))

/**************************END COLSTORED C STORE Bn x6********************************/

/**************************START COLSTORED C STORE Bn x5********************************/
/**
 * @brief STORE_COLSTORED_C_24x5_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C24Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_24x5_Bn                                                               \
	STORE_COLSTORED_C24Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C24Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C24Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C24Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C24Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))

/**
 * @brief STORE_COLSTORED_C_20x5_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C20Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_20x5_Bn                                                               \
	STORE_COLSTORED_C20Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C20Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C20Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C20Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C20Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))

/**
 * @brief STORE_COLSTORED_C_18x5_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C18Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_18x5_Bn                                                               \
	STORE_COLSTORED_C18Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C18Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C18Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C18Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C18Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))

/**
 * @brief STORE_COLSTORED_C_17x5_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C17Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_17x5_Bn                                                               \
	STORE_COLSTORED_C17Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C17Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C17Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C17Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C17Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))

/**
 * @brief STORE_COLSTORED_C_16x5_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C16Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_16x5_Bn                                       \
	STORE_COLSTORED_C16Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C16Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C16Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C16Bn(14, 15, mem(rdx), 0x40(rdx))

#define STORE_COLSTORED_C_24MASKx5_Bn               \
	vmulpd( zmm30, zmm6, zmm6 )                     \
	vfmadd231pd( mem(rcx), zmm31, zmm6)             \
	vmovupd( zmm6, mem(rcx))                        \
                                                    \
	vmulpd( zmm30, zmm7, zmm7 )                     \
	vfmadd231pd( 0x40(rcx), zmm31, zmm7)            \
	vmovupd( zmm7, 0x40(rcx))                       \
                                                    \
	vmulpd( zmm30, zmm8, zmm8 )                     \
	vfmadd231pd( mem(rcx, rdi, 1), zmm31, zmm8)     \
	vmovupd( zmm8, mem(rcx, rdi, 1))                \
                                                    \
	vmulpd( zmm30, zmm9, zmm9 )                     \
	vfmadd231pd( 0x40(rcx, rdi, 1), zmm31, zmm9)    \
	vmovupd( zmm9, 0x40(rcx, rdi, 1))               \
                                                    \
	vmulpd( zmm30, zmm10, zmm10 )                   \
	vfmadd231pd( mem(rcx, rdi, 2), zmm31, zmm10)    \
	vmovupd( zmm10, mem(rcx, rdi, 2))               \
                                                    \
	vmulpd( zmm30, zmm11, zmm11 )                   \
	vfmadd231pd( 0x40(rcx, rdi, 2), zmm31, zmm11)   \
	vmovupd( zmm11, 0x40(rcx, rdi, 2))              \
                                                    \
	vmulpd( zmm30, zmm12, zmm12 )                   \
	vfmadd231pd( mem(rcx, r13, 1), zmm31, zmm12)    \
	vmovupd( zmm12, mem(rcx, r13, 1))               \
                                                    \
	vmulpd( zmm30, zmm13, zmm13 )                   \
	vfmadd231pd( 0x40(rcx, r13, 1), zmm31, zmm13)   \
	vmovupd( zmm13, 0x40(rcx, r13, 1))              \
                                                    \
	vmulpd( zmm30, zmm14, zmm14 )                   \
	vfmadd231pd( mem(rdx), zmm31, zmm14)            \
	vmovupd( zmm14, mem(rdx))                       \
                                                    \
	vmulpd( zmm30, zmm15, zmm15 )                   \
	vfmadd231pd( 0x40(rdx), zmm31, zmm15)           \
	vmovupd( zmm15, 0x40(rdx))                      \
                                                    \
	vmulpd( zmm30, zmm28, zmm28 )                   \
	vmovupd( 0x80(rcx), zmm0 MASK_KZ(2))            \
                                                    \
	vfmadd231pd( zmm0, zmm31, zmm28)                \
	vmovupd( zmm28, 0x80(rcx) MASK_(k(2)))          \
                                                    \
	vmulpd( zmm30, zmm29, zmm29 )                   \
	vmovupd( 0x80(rcx, rdi, 1), zmm1 MASK_KZ(2))    \
                                                    \
	vfmadd231pd( zmm1, zmm31, zmm29)                \
	vmovupd( zmm29, 0x80(rcx, rdi, 1) MASK_(k(2)))  \
                                                    \
	vmulpd( zmm30, zmm26, zmm26 )                   \
	vmovupd( 0x80(rcx, rdi, 2), zmm2 MASK_KZ(2))    \
                                                    \
	vfmadd231pd( zmm2, zmm31, zmm26)                \
	vmovupd( zmm26, 0x80(rcx, rdi, 2) MASK_(k(2)))  \
                                                    \
	vmulpd( zmm30, zmm27, zmm27 )                   \
	vmovupd( 0x80(rcx, r13, 1), zmm3 MASK_KZ(2))    \
                                                    \
	vfmadd231pd( zmm3, zmm31, zmm27)                \
	vmovupd( zmm27, 0x80(rcx, r13, 1) MASK_(k(2)))  \
                                                    \
	vmulpd( zmm30, zmm24, zmm24 )                   \
	vmovupd( 0x80(rdx), zmm4 MASK_KZ(2))            \
                                                    \
	vfmadd231pd( zmm4, zmm31, zmm24)                \
	vmovupd( zmm24, 0x80(rdx) MASK_(k(2)))

/**
 * @brief STORE_COLSTORED_C_12x5_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C12Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_12x5_Bn                                       \
	STORE_COLSTORED_C12Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C12Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C12Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C12Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C12Bn(14, 15, mem(rdx), 0x40(rdx))

/**
 * @brief STORE_COLSTORED_C_11x5_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C11Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - 0x50(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - 0x50(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_11x5_Bn                                                               \
	STORE_COLSTORED_C11Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x50(rcx))                             \
	STORE_COLSTORED_C11Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x50(rcx, rdi, 1))     \
	STORE_COLSTORED_C11Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x50(rcx, rdi, 2))   \
	STORE_COLSTORED_C11Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x50(rcx, r13, 1))   \
	STORE_COLSTORED_C11Bn(14, 15, 24, mem(rdx), 0x40(rdx), 0x50(rdx))

/**
 * @brief STORE_COLSTORED_C_10x5_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C10Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_10x5_Bn                                       \
	STORE_COLSTORED_C10Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C10Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C10Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C10Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C10Bn(14, 15, mem(rdx), 0x40(rdx))

/**
 * @brief STORE_COLSTORED_C_9x5_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C9Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_9x5_Bn                                        \
	STORE_COLSTORED_C9Bn(6, 7, mem(rcx), 0x40(rcx))                     \
	STORE_COLSTORED_C9Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))     \
	STORE_COLSTORED_C9Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))   \
	STORE_COLSTORED_C9Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))   \
	STORE_COLSTORED_C9Bn(14, 15, mem(rdx), 0x40(rdx))

/**
 * @brief STORE_COLSTORED_C_8x5_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_8x5_Bn                \
	STORE_COLSTORED_C8Bn(6, mem(rcx))           \
	STORE_COLSTORED_C8Bn(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C8Bn(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C8Bn(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C8Bn(14, mem(rdx))

#define STORE_COLSTORED_C_16MASKx5_Bn                   \
	vmulpd( zmm30, zmm6, zmm6 )                         \
	vfmadd231pd( mem(rcx), zmm31, zmm6)                 \
	vmovupd( zmm6, mem(rcx))                            \
                                                        \
	vmulpd( zmm30, zmm8, zmm8 )                         \
	vfmadd231pd( mem(rcx, rdi, 1), zmm31, zmm8)         \
	vmovupd( zmm8, mem(rcx, rdi, 1))                    \
                                                        \
	vmulpd( zmm30, zmm10, zmm10 )                       \
	vfmadd231pd( mem(rcx, rdi, 2), zmm31, zmm10)        \
	vmovupd( zmm10, mem(rcx, rdi, 2))                   \
                                                        \
	vmulpd( zmm30, zmm12, zmm12 )                       \
	vfmadd231pd( mem(rcx, r13, 1), zmm31, zmm12)        \
	vmovupd( zmm12, mem(rcx, r13, 1))                   \
                                                        \
	vmulpd( zmm30, zmm14, zmm14 )                       \
	vfmadd231pd( mem(rdx), zmm31, zmm14)                \
	vmovupd( zmm14, mem(rdx))                           \
                                                        \
	vmulpd( zmm30, zmm7, zmm7 )                         \
	vmovupd( 0x40(rcx), zmm0 MASK_KZ(2))                \
                                                        \
	vfmadd231pd( zmm0, zmm31, zmm7)                     \
	vmovupd( zmm7, 0x40(rcx) MASK_(k(2)))               \
                                                        \
	vmulpd( zmm30, zmm9, zmm9 )                         \
	vmovupd( 0x40(rcx, rdi, 1), zmm1 MASK_KZ(2))        \
                                                        \
	vfmadd231pd( zmm1, zmm31, zmm9)                     \
	vmovupd( zmm9, 0x40(rcx, rdi, 1) MASK_(k(2)))       \
                                                        \
	vmulpd( zmm30, zmm11, zmm11 )                       \
	vmovupd( 0x40(rcx, rdi, 2), zmm2 MASK_KZ(2))        \
                                                        \
	vfmadd231pd( zmm2, zmm31, zmm11)                    \
	vmovupd( zmm11, 0x40(rcx, rdi, 2) MASK_(k(2)))      \
                                                        \
	vmulpd( zmm30, zmm13, zmm13 )                       \
	vmovupd( 0x40(rcx, r13, 1), zmm3 MASK_KZ(2))        \
                                                        \
	vfmadd231pd( zmm3, zmm31, zmm13)                    \
	vmovupd( zmm13, 0x40(rcx, r13, 1) MASK_(k(2)))      \
                                                        \
	vmulpd( zmm30, zmm15, zmm15 )                       \
	vmovupd( 0x40(rdx), zmm4 MASK_KZ(2))                \
                                                        \
	vfmadd231pd( zmm4, zmm31, zmm15)                    \
	vmovupd( zmm15, 0x40(rdx) MASK_(k(2)))

/**
 * @brief STORE_COLSTORED_C_4x5_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C4Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_4x5_Bn                    \
	STORE_COLSTORED_C4Bn(0, 6, mem(rcx))            \
	STORE_COLSTORED_C4Bn(1, 8, mem(rcx, rdi, 1))    \
	STORE_COLSTORED_C4Bn(2, 10, mem(rcx, rdi, 2))   \
	STORE_COLSTORED_C4Bn(3, 12, mem(rcx, r13, 1))   \
	STORE_COLSTORED_C4Bn(4, 14, mem(rdx))

/**
 * @brief STORE_COLSTORED_C_2x5_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C2Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_2x5_Bn                    \
	STORE_COLSTORED_C2Bn(0, 6, mem(rcx))            \
	STORE_COLSTORED_C2Bn(1, 8, mem(rcx, rdi, 1))    \
	STORE_COLSTORED_C2Bn(2, 10, mem(rcx, rdi, 2))   \
	STORE_COLSTORED_C2Bn(3, 12, mem(rcx, r13, 1))   \
	STORE_COLSTORED_C2Bn(4, 14, mem(rdx))

/**
 * @brief STORE_COLSTORED_C_1x5_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C1Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_1x5_Bn                    \
	STORE_COLSTORED_C1Bn(0, 6, mem(rcx))            \
	STORE_COLSTORED_C1Bn(1, 8, mem(rcx, rdi, 1))    \
	STORE_COLSTORED_C1Bn(2, 10, mem(rcx, rdi, 2))   \
	STORE_COLSTORED_C1Bn(3, 12, mem(rcx, r13, 1))   \
	STORE_COLSTORED_C1Bn(4, 14, mem(rdx))

#define STORE_COLSTORED_C_8MASKx5_Bn                \
    vmulpd( zmm30, zmm6, zmm6 )                     \
    vmovupd( mem(rcx), zmm0 MASK_KZ(2))             \
                                                    \
    vmulpd( zmm30, zmm8, zmm8 )                     \
    vmovupd( mem(rcx, rdi, 1), zmm1 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm10, zmm10 )                   \
    vmovupd( mem(rcx, rdi, 2), zmm2 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm12, zmm12 )                   \
    vmovupd( mem(rcx, r13, 1), zmm3 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm14, zmm14 )                   \
    vmovupd( mem(rdx), zmm4 MASK_KZ(2))             \
                                                    \
    vfmadd231pd( zmm0, zmm31, zmm6)                 \
    vmovupd( zmm6, mem(rcx) MASK_(k(2)))            \
                                                    \
    vfmadd231pd( zmm1, zmm31, zmm8)                 \
    vmovupd( zmm8, mem(rcx, rdi, 1) MASK_(k(2)))    \
                                                    \
    vfmadd231pd( zmm2, zmm31, zmm10)                \
    vmovupd( zmm10, mem(rcx, rdi, 2) MASK_(k(2)))   \
                                                    \
    vfmadd231pd( zmm3, zmm31, zmm12)                \
    vmovupd( zmm12, mem(rcx, r13, 1) MASK_(k(2)))   \
                                                    \
    vfmadd231pd( zmm4, zmm31, zmm14)                \
    vmovupd( zmm14, mem(rdx) MASK_(k(2)))

/******************************************************/
/**
 * @brief STORE_COLSTORED_C_24x5_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C24B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_24x5_B0                                                               \
	STORE_COLSTORED_C24B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C24B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C24B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C24B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C24B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))

/**
 * STORE_COLSTORED_C_20x5_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C20B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_20x5_B0                                                               \
	STORE_COLSTORED_C20B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C20B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C20B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C20B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C20B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))

/**
 * STORE_COLSTORED_C_18x5_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C18B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_18x5_B0                                                               \
	STORE_COLSTORED_C18B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C18B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C18B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C18B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C18B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))

/**
 * STORE_COLSTORED_C_17x5_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C17B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_17x5_B0                                                               \
	STORE_COLSTORED_C17B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C17B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C17B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C17B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C17B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))

/**
 * STORE_COLSTORED_C_16x5_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C16B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_16x5_B0                                       \
	STORE_COLSTORED_C16B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C16B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C16B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C16B0(14, 15, mem(rdx), 0x40(rdx))

#define STORE_COLSTORED_C_24MASKx5_B0                                                               \
	STORE_COLSTORED_C24MASKB0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C24MASKB0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C24MASKB0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C24MASKB0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))   \
	STORE_COLSTORED_C24MASKB0(14, 15, 24, mem(rdx), 0x40(rdx), 0x80(rdx))

/**
 * STORE_COLSTORED_C_12x5_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C12B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_12x5_B0                                       \
	STORE_COLSTORED_C12B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C12B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C12B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C12B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C12B0(14, 15, mem(rdx), 0x40(rdx))

/**
 * STORE_COLSTORED_C_11x5_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C11B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - 0x50(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - 0x50(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 * - zmm14, 15, 24 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_11x5_B0                                                               \
	STORE_COLSTORED_C11B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x50(rcx))                             \
	STORE_COLSTORED_C11B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x50(rcx, rdi, 1))     \
	STORE_COLSTORED_C11B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x50(rcx, rdi, 2))   \
	STORE_COLSTORED_C11B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x50(rcx, r13, 1))   \
	STORE_COLSTORED_C11B0(14, 15, 24, mem(rdx), 0x40(rdx), 0x50(rdx))

/**
 * STORE_COLSTORED_C_10x5_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C10B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_10x5_B0                                       \
	STORE_COLSTORED_C10B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C10B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C10B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C10B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C10B0(14, 15, mem(rdx), 0x40(rdx))

/**
 * STORE_COLSTORED_C_9x5_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C9B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 * - zmm14, 15 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_9x5_B0                                        \
	STORE_COLSTORED_C9B0(6, 7, mem(rcx), 0x40(rcx))                     \
	STORE_COLSTORED_C9B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))     \
	STORE_COLSTORED_C9B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))   \
	STORE_COLSTORED_C9B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))   \
	STORE_COLSTORED_C9B0(14, 15, mem(rdx), 0x40(rdx))

/**
 * STORE_COLSTORED_C_8x5_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_8x5_B0                \
	STORE_COLSTORED_C8B0(6, mem(rcx))           \
	STORE_COLSTORED_C8B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C8B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C8B0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C8B0(14, mem(rdx))

#define STORE_COLSTORED_C_16MASKx5_B0                                       \
	STORE_COLSTORED_C16MASKB0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16MASKB0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C16MASKB0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C16MASKB0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))  \
	STORE_COLSTORED_C16MASKB0(14, 15, mem(rdx), 0x40(rdx))

/**
 * STORE_COLSTORED_C_4x5_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C4B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_4x5_B0                \
	STORE_COLSTORED_C4B0(6, mem(rcx))           \
	STORE_COLSTORED_C4B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C4B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C4B0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C4B0(14, mem(rdx))

/**
 * STORE_COLSTORED_C_2x5_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C2B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_2x5_B0                \
	STORE_COLSTORED_C2B0(6, mem(rcx))           \
	STORE_COLSTORED_C2B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C2B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C2B0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C2B0(14, mem(rdx))

/**
 * STORE_COLSTORED_C_1x5_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C1B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 * - zmm14 holds A * B for C[0, 4]
 */
#define STORE_COLSTORED_C_1x5_B0                \
	STORE_COLSTORED_C1B0(6, mem(rcx))           \
	STORE_COLSTORED_C1B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C1B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C1B0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C1B0(14, mem(rdx))

#define STORE_COLSTORED_C_8MASKx5_B0                \
	STORE_COLSTORED_C8MASKB0(6, mem(rcx))           \
	STORE_COLSTORED_C8MASKB0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C8MASKB0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C8MASKB0(12, mem(rcx, r13, 1))  \
	STORE_COLSTORED_C8MASKB0(14, mem(rdx))

/**************************END COLSTORED C STORE Bn x5********************************/

/**************************START COLSTORED C STORE Bn x4********************************/
/**
 * @brief STORE_COLSTORED_C_24x4_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C24Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_24x4_Bn                                                               \
	STORE_COLSTORED_C24Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C24Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C24Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C24Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_20x4_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C20Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_20x4_Bn                                                               \
	STORE_COLSTORED_C20Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C20Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C20Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C20Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_18x4_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C18Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_18x4_Bn                                                               \
	STORE_COLSTORED_C18Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C18Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C18Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C18Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_17x4_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C17Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_17x4_Bn                                                               \
	STORE_COLSTORED_C17Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C17Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C17Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C17Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_16x4_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C16Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_16x4_Bn                                       \
	STORE_COLSTORED_C16Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C16Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C16Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))

#define STORE_COLSTORED_C_24MASKx4_Bn                   \
	vmulpd( zmm30, zmm6, zmm6 )                         \
	vfmadd231pd( mem(rcx), zmm31, zmm6)                 \
	vmovupd( zmm6, mem(rcx))                            \
                                                        \
	vmulpd( zmm30, zmm7, zmm7 )                         \
	vfmadd231pd( 0x40(rcx), zmm31, zmm7)                \
	vmovupd( zmm7, 0x40(rcx))                           \
                                                        \
	vmulpd( zmm30, zmm8, zmm8 )                         \
	vfmadd231pd( mem(rcx, rdi, 1), zmm31, zmm8)         \
	vmovupd( zmm8, mem(rcx, rdi, 1))                    \
                                                        \
	vmulpd( zmm30, zmm9, zmm9 )                         \
	vfmadd231pd( 0x40(rcx, rdi, 1), zmm31, zmm9)        \
	vmovupd( zmm9, 0x40(rcx, rdi, 1))                   \
                                                        \
	vmulpd( zmm30, zmm10, zmm10 )                       \
	vfmadd231pd( mem(rcx, rdi, 2), zmm31, zmm10)        \
	vmovupd( zmm10, mem(rcx, rdi, 2))                   \
                                                        \
	vmulpd( zmm30, zmm11, zmm11 )                       \
	vfmadd231pd( 0x40(rcx, rdi, 2), zmm31, zmm11)       \
	vmovupd( zmm11, 0x40(rcx, rdi, 2))                  \
                                                        \
	vmulpd( zmm30, zmm12, zmm12 )                       \
	vfmadd231pd( mem(rcx, r13, 1), zmm31, zmm12)        \
	vmovupd( zmm12, mem(rcx, r13, 1))                   \
                                                        \
	vmulpd( zmm30, zmm13, zmm13 )                       \
	vfmadd231pd( 0x40(rcx, r13, 1), zmm31, zmm13)       \
	vmovupd( zmm13, 0x40(rcx, r13, 1))                  \
                                                        \
	vmulpd( zmm30, zmm28, zmm28 )                       \
	vmovupd( 0x80(rcx), zmm0 MASK_KZ(2))                \
                                                        \
	vfmadd231pd( zmm0, zmm31, zmm28)                    \
	vmovupd( zmm28, 0x80(rcx) MASK_(k(2)))              \
                                                        \
	vmulpd( zmm30, zmm29, zmm29 )                       \
	vmovupd( 0x80(rcx, rdi, 1), zmm1 MASK_KZ(2))        \
                                                        \
	vfmadd231pd( zmm1, zmm31, zmm29)                    \
	vmovupd( zmm29, 0x80(rcx, rdi, 1) MASK_(k(2)))      \
                                                        \
	vmulpd( zmm30, zmm26, zmm26 )                       \
	vmovupd( 0x80(rcx, rdi, 2), zmm2 MASK_KZ(2))        \
                                                        \
	vfmadd231pd( zmm2, zmm31, zmm26)                    \
	vmovupd( zmm26, 0x80(rcx, rdi, 2) MASK_(k(2)))      \
                                                        \
	vmulpd( zmm30, zmm27, zmm27 )                       \
	vmovupd( 0x80(rcx, r13, 1), zmm3 MASK_KZ(2))        \
                                                        \
	vfmadd231pd( zmm3, zmm31, zmm27)                    \
	vmovupd( zmm27, 0x80(rcx, r13, 1) MASK_(k(2)))

/**
 * @brief STORE_COLSTORED_C_12x4_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C12Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_12x4_Bn                                       \
	STORE_COLSTORED_C12Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C12Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C12Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C12Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_11x4_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C11Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - 0x50(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - 0x50(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_11x4_Bn                                                               \
	STORE_COLSTORED_C11Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x50(rcx))                             \
	STORE_COLSTORED_C11Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x50(rcx, rdi, 1))     \
	STORE_COLSTORED_C11Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x50(rcx, rdi, 2))   \
	STORE_COLSTORED_C11Bn(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x50(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_10x4_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C10Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_10x4_Bn                                       \
	STORE_COLSTORED_C10Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C10Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C10Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C10Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_9x4_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C9Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_9x4_Bn                                        \
	STORE_COLSTORED_C9Bn(6, 7, mem(rcx), 0x40(rcx))                     \
	STORE_COLSTORED_C9Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))     \
	STORE_COLSTORED_C9Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))   \
	STORE_COLSTORED_C9Bn(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_8x4_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_8x4_Bn                \
	STORE_COLSTORED_C8Bn(6, mem(rcx))           \
	STORE_COLSTORED_C8Bn(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C8Bn(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C8Bn(12, mem(rcx, r13, 1))

#define STORE_COLSTORED_C_16MASKx4_Bn               \
	vmulpd( zmm30, zmm6, zmm6 )                     \
	vfmadd231pd( mem(rcx), zmm31, zmm6)             \
	vmovupd( zmm6, mem(rcx))                        \
                                                    \
	vmulpd( zmm30, zmm8, zmm8 )                     \
	vfmadd231pd( mem(rcx, rdi, 1), zmm31, zmm8)     \
	vmovupd( zmm8, mem(rcx, rdi, 1))                \
                                                    \
	vmulpd( zmm30, zmm10, zmm10 )                   \
	vfmadd231pd( mem(rcx, rdi, 2), zmm31, zmm10)    \
	vmovupd( zmm10, mem(rcx, rdi, 2))               \
                                                    \
	vmulpd( zmm30, zmm12, zmm12 )                   \
	vfmadd231pd( mem(rcx, r13, 1), zmm31, zmm12)    \
	vmovupd( zmm12, mem(rcx, r13, 1))               \
                                                    \
	vmulpd( zmm30, zmm7, zmm7 )                     \
	vmovupd( 0x40(rcx), zmm0 MASK_KZ(2))            \
                                                    \
	vfmadd231pd( zmm0, zmm31, zmm7)                 \
	vmovupd( zmm7, 0x40(rcx) MASK_(k(2)))           \
                                                    \
	vmulpd( zmm30, zmm9, zmm9 )                     \
	vmovupd( 0x40(rcx, rdi, 1), zmm1 MASK_KZ(2))    \
                                                    \
	vfmadd231pd( zmm1, zmm31, zmm9)                 \
	vmovupd( zmm9, 0x40(rcx, rdi, 1) MASK_(k(2)))   \
                                                    \
	vmulpd( zmm30, zmm11, zmm11 )                   \
	vmovupd( 0x40(rcx, rdi, 2), zmm2 MASK_KZ(2))    \
                                                    \
	vfmadd231pd( zmm2, zmm31, zmm11)                \
	vmovupd( zmm11, 0x40(rcx, rdi, 2) MASK_(k(2)))  \
                                                    \
	vmulpd( zmm30, zmm13, zmm13 )                   \
	vmovupd( 0x40(rcx, r13, 1), zmm3 MASK_KZ(2))    \
                                                    \
	vfmadd231pd( zmm3, zmm31, zmm13)                \
	vmovupd( zmm13, 0x40(rcx, r13, 1) MASK_(k(2)))

/**
 * @brief STORE_COLSTORED_C_4x4_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C4Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_4x4_Bn                    \
	STORE_COLSTORED_C4Bn(0, 6, mem(rcx))            \
	STORE_COLSTORED_C4Bn(1, 8, mem(rcx, rdi, 1))    \
	STORE_COLSTORED_C4Bn(2, 10, mem(rcx, rdi, 2))   \
	STORE_COLSTORED_C4Bn(3, 12, mem(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_2x4_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C2Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_2x4_Bn                    \
	STORE_COLSTORED_C2Bn(0, 6, mem(rcx))            \
	STORE_COLSTORED_C2Bn(1, 8, mem(rcx, rdi, 1))    \
	STORE_COLSTORED_C2Bn(2, 10, mem(rcx, rdi, 2))   \
	STORE_COLSTORED_C2Bn(3, 12, mem(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_1x4_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C1Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_1x4_Bn                    \
	STORE_COLSTORED_C1Bn(0, 6, mem(rcx))            \
	STORE_COLSTORED_C1Bn(1, 8, mem(rcx, rdi, 1))    \
	STORE_COLSTORED_C1Bn(2, 10, mem(rcx, rdi, 2))   \
	STORE_COLSTORED_C1Bn(3, 12, mem(rcx, r13, 1))

#define STORE_COLSTORED_C_8MASKx4_Bn                \
    vmulpd( zmm30, zmm6, zmm6 )                     \
    vmovupd( mem(rcx), zmm0 MASK_KZ(2))             \
                                                    \
    vmulpd( zmm30, zmm8, zmm8 )                     \
    vmovupd( mem(rcx, rdi, 1), zmm1 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm10, zmm10 )                   \
    vmovupd( mem(rcx, rdi, 2), zmm2 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm12, zmm12 )                   \
    vmovupd( mem(rcx, r13, 1), zmm3 MASK_KZ(2))     \
                                                    \
    vfmadd231pd( zmm0, zmm31, zmm6)                 \
    vmovupd( zmm6, mem(rcx) MASK_(k(2)))            \
                                                    \
    vfmadd231pd( zmm1, zmm31, zmm8)                 \
    vmovupd( zmm8, mem(rcx, rdi, 1) MASK_(k(2)))    \
                                                    \
    vfmadd231pd( zmm2, zmm31, zmm10)                \
    vmovupd( zmm10, mem(rcx, rdi, 2) MASK_(k(2)))   \
                                                    \
    vfmadd231pd( zmm3, zmm31, zmm12)                \
    vmovupd( zmm12, mem(rcx, r13, 1) MASK_(k(2)))

/****************************************************/
/**
 * @brief STORE_COLSTORED_C_24x4_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C24B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_24x4_B0                                                               \
	STORE_COLSTORED_C24B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C24B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C24B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C24B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_20x4_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C20B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_20x4_B0                                                               \
	STORE_COLSTORED_C20B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C20B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C20B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C20B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_18x4_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C18B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_18x4_B0                                                               \
	STORE_COLSTORED_C18B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C18B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C18B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C18B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_17x4_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C17B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_17x4_B0                                                               \
	STORE_COLSTORED_C17B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C17B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C17B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C17B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_16x4_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C16B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_16x4_B0                                       \
	STORE_COLSTORED_C16B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C16B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C16B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))

#define STORE_COLSTORED_C_24MASKx4_B0                                                               \
	STORE_COLSTORED_C24MASKB0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C24MASKB0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C24MASKB0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))   \
	STORE_COLSTORED_C24MASKB0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x80(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_12x4_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C12B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_12x4_B0                                       \
	STORE_COLSTORED_C12B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C12B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C12B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C12B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_11x4_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C11B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - 0x50(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - 0x50(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 * - zmm12, 13, 27 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_11x4_B0                                                               \
	STORE_COLSTORED_C11B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x50(rcx))                             \
	STORE_COLSTORED_C11B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x50(rcx, rdi, 1))     \
	STORE_COLSTORED_C11B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x50(rcx, rdi, 2))   \
	STORE_COLSTORED_C11B0(12, 13, 27, mem(rcx, r13, 1), 0x40(rcx, r13, 1), 0x50(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_10x4_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C10B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_10x4_B0                                       \
	STORE_COLSTORED_C10B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C10B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C10B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C10B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_9x4_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C9B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 * - zmm12, 13 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_9x4_B0                                        \
	STORE_COLSTORED_C9B0(6, 7, mem(rcx), 0x40(rcx))                     \
	STORE_COLSTORED_C9B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))     \
	STORE_COLSTORED_C9B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))   \
	STORE_COLSTORED_C9B0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_8x4_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_8x4_B0                \
	STORE_COLSTORED_C8B0(6, mem(rcx))           \
	STORE_COLSTORED_C8B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C8B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C8B0(12, mem(rcx, r13, 1))

#define STORE_COLSTORED_C_16MASKx4_B0                                       \
	STORE_COLSTORED_C16MASKB0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16MASKB0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C16MASKB0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))  \
	STORE_COLSTORED_C16MASKB0(12, 13, mem(rcx, r13, 1), 0x40(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_4x4_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C4B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_4x4_B0                \
	STORE_COLSTORED_C4B0(6, mem(rcx))           \
	STORE_COLSTORED_C4B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C4B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C4B0(12, mem(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_2x4_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C2B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_2x4_B0                \
	STORE_COLSTORED_C2B0(6, mem(rcx))           \
	STORE_COLSTORED_C2B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C2B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C2B0(12, mem(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_1x4_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C1B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_1x4_B0                \
	STORE_COLSTORED_C1B0(6, mem(rcx))           \
	STORE_COLSTORED_C1B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C1B0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C1B0(12, mem(rcx, r13, 1))

/**
 * @brief STORE_COLSTORED_C_8MASKx4_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8MASKB0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] m_left elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] m_left elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 * - zmm12 holds A * B for C[0, 3]
 */
#define STORE_COLSTORED_C_8MASKx4_B0                \
	STORE_COLSTORED_C8MASKB0(6, mem(rcx))           \
	STORE_COLSTORED_C8MASKB0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C8MASKB0(10, mem(rcx, rdi, 2))  \
	STORE_COLSTORED_C8MASKB0(12, mem(rcx, r13, 1))

/**************************END COLSTORED C STORE Bn x4********************************/

/**************************START COLSTORED C STORE Bn x3********************************/
/**
 * @brief STORE_COLSTORED_C_24x3_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C24Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_24x3_Bn                                                               \
	STORE_COLSTORED_C24Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C24Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C24Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_20x3_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C20Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_20x3_Bn                                                               \
	STORE_COLSTORED_C20Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C20Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C20Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_18x3_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C18Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_18x3_Bn                                                               \
	STORE_COLSTORED_C18Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C18Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C18Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_17x3_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C17Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_17x3_Bn                                                               \
	STORE_COLSTORED_C17Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C17Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C17Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_16x3_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C16Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_16x3_Bn                                       \
	STORE_COLSTORED_C16Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C16Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))

#define STORE_COLSTORED_C_24MASKx3_Bn               \
	vmulpd( zmm30, zmm6, zmm6 )                     \
	vfmadd231pd( mem(rcx), zmm31, zmm6)             \
	vmovupd( zmm6, mem(rcx))                        \
                                                    \
	vmulpd( zmm30, zmm7, zmm7 )                     \
	vfmadd231pd( 0x40(rcx), zmm31, zmm7)            \
	vmovupd( zmm7, 0x40(rcx))                       \
                                                    \
	vmulpd( zmm30, zmm8, zmm8 )                     \
	vfmadd231pd( mem(rcx, rdi, 1), zmm31, zmm8)     \
	vmovupd( zmm8, mem(rcx, rdi, 1))                \
                                                    \
	vmulpd( zmm30, zmm9, zmm9 )                     \
	vfmadd231pd( 0x40(rcx, rdi, 1), zmm31, zmm9)    \
	vmovupd( zmm9, 0x40(rcx, rdi, 1))               \
                                                    \
	vmulpd( zmm30, zmm10, zmm10 )                   \
	vfmadd231pd( mem(rcx, rdi, 2), zmm31, zmm10)    \
	vmovupd( zmm10, mem(rcx, rdi, 2))               \
                                                    \
	vmulpd( zmm30, zmm11, zmm11 )                   \
	vfmadd231pd( 0x40(rcx, rdi, 2), zmm31, zmm11)   \
	vmovupd( zmm11, 0x40(rcx, rdi, 2))              \
                                                    \
	vmulpd( zmm30, zmm28, zmm28 )                   \
	vmovupd( 0x80(rcx), zmm0 MASK_KZ(2))            \
                                                    \
	vfmadd231pd( zmm0, zmm31, zmm28)                \
	vmovupd( zmm28, 0x80(rcx) MASK_(k(2)))          \
                                                    \
	vmulpd( zmm30, zmm29, zmm29 )                   \
	vmovupd( 0x80(rcx, rdi, 1), zmm1 MASK_KZ(2))    \
                                                    \
	vfmadd231pd( zmm1, zmm31, zmm29)                \
	vmovupd( zmm29, 0x80(rcx, rdi, 1) MASK_(k(2)))  \
                                                    \
	vmulpd( zmm30, zmm26, zmm26 )                   \
	vmovupd( 0x80(rcx, rdi, 2), zmm2 MASK_KZ(2))    \
                                                    \
	vfmadd231pd( zmm2, zmm31, zmm26)                \
	vmovupd( zmm26, 0x80(rcx, rdi, 2) MASK_(k(2)))

/**
 * @brief STORE_COLSTORED_C_12x3_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C12Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_12x3_Bn                                       \
	STORE_COLSTORED_C12Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C12Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C12Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_11x3_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C11Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - 0x50(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - 0x50(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_11x3_Bn                                                               \
	STORE_COLSTORED_C11Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x50(rcx))                             \
	STORE_COLSTORED_C11Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x50(rcx, rdi, 1))     \
	STORE_COLSTORED_C11Bn(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x50(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_10x3_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C10Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_10x3_Bn                                       \
	STORE_COLSTORED_C10Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C10Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C10Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_9x3_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C9Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_9x3_Bn                                        \
	STORE_COLSTORED_C9Bn(6, 7, mem(rcx), 0x40(rcx))                     \
	STORE_COLSTORED_C9Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))     \
	STORE_COLSTORED_C9Bn(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_8x3_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_8x3_Bn                \
	STORE_COLSTORED_C8Bn(6, mem(rcx))           \
	STORE_COLSTORED_C8Bn(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C8Bn(10, mem(rcx, rdi, 2))

#define STORE_COLSTORED_C_16MASKx3_Bn               \
	vmulpd( zmm30, zmm6, zmm6 )                     \
	vfmadd231pd( mem(rcx), zmm31, zmm6)             \
	vmovupd( zmm6, mem(rcx))                        \
                                                    \
	vmulpd( zmm30, zmm8, zmm8 )                     \
	vfmadd231pd( mem(rcx, rdi, 1), zmm31, zmm8)     \
	vmovupd( zmm8, mem(rcx, rdi, 1))                \
                                                    \
	vmulpd( zmm30, zmm10, zmm10 )                   \
	vfmadd231pd( mem(rcx, rdi, 2), zmm31, zmm10)    \
	vmovupd( zmm10, mem(rcx, rdi, 2))               \
                                                    \
	vmulpd( zmm30, zmm7, zmm7 )                     \
	vmovupd( 0x40(rcx), zmm0 MASK_KZ(2))            \
                                                    \
	vfmadd231pd( zmm0, zmm31, zmm7)                 \
	vmovupd( zmm7, 0x40(rcx) MASK_(k(2)))           \
                                                    \
	vmulpd( zmm30, zmm9, zmm9 )                     \
	vmovupd( 0x40(rcx, rdi, 1), zmm1 MASK_KZ(2))    \
                                                    \
	vfmadd231pd( zmm1, zmm31, zmm9)                 \
	vmovupd( zmm9, 0x40(rcx, rdi, 1) MASK_(k(2)))   \
                                                    \
	vmulpd( zmm30, zmm11, zmm11 )                   \
	vmovupd( 0x40(rcx, rdi, 2), zmm2 MASK_KZ(2))    \
                                                    \
	vfmadd231pd( zmm2, zmm31, zmm11)                \
	vmovupd( zmm11, 0x40(rcx, rdi, 2) MASK_(k(2)))

/**
 * @brief STORE_COLSTORED_C_4x3_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C4Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_4x3_Bn                    \
	STORE_COLSTORED_C4Bn(0, 6, mem(rcx))            \
	STORE_COLSTORED_C4Bn(1, 8, mem(rcx, rdi, 1))    \
	STORE_COLSTORED_C4Bn(2, 10, mem(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_2x3_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C2Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_2x3_Bn                    \
	STORE_COLSTORED_C2Bn(0, 6, mem(rcx))            \
	STORE_COLSTORED_C2Bn(1, 8, mem(rcx, rdi, 1))    \
	STORE_COLSTORED_C2Bn(2, 10, mem(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_1x3_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C1Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_1x3_Bn                    \
	STORE_COLSTORED_C1Bn(0, 6, mem(rcx))            \
	STORE_COLSTORED_C1Bn(1, 8, mem(rcx, rdi, 1))    \
	STORE_COLSTORED_C1Bn(2, 10, mem(rcx, rdi, 2))

#define STORE_COLSTORED_C_8MASKx3_Bn                \
    vmulpd( zmm30, zmm6, zmm6 )                     \
    vmovupd( mem(rcx), zmm0 MASK_KZ(2))             \
                                                    \
    vmulpd( zmm30, zmm8, zmm8 )                     \
    vmovupd( mem(rcx, rdi, 1), zmm1 MASK_KZ(2))     \
                                                    \
    vmulpd( zmm30, zmm10, zmm10 )                   \
    vmovupd( mem(rcx, rdi, 2), zmm2 MASK_KZ(2))     \
                                                    \
    vfmadd231pd( zmm0, zmm31, zmm6)                 \
    vmovupd( zmm6, mem(rcx) MASK_(k(2)))            \
                                                    \
    vfmadd231pd( zmm1, zmm31, zmm8)                 \
    vmovupd( zmm8, mem(rcx, rdi, 1) MASK_(k(2)))    \
                                                    \
    vfmadd231pd( zmm2, zmm31, zmm10)                \
    vmovupd( zmm10, mem(rcx, rdi, 2) MASK_(k(2)))

/**********************************************/
/**
 * @brief STORE_COLSTORED_C_24x3_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C24B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_24x3_B0                                                               \
	STORE_COLSTORED_C24B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C24B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C24B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_20x3_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C20B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_20x3_B0                                                               \
	STORE_COLSTORED_C20B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C20B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C20B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_18x3_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C18B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_18x3_B0                                                               \
	STORE_COLSTORED_C18B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C18B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C18B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_17x3_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C17B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_17x3_B0                                                               \
	STORE_COLSTORED_C17B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C17B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C17B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_16x3_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C16B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_16x3_B0                                       \
	STORE_COLSTORED_C16B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C16B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))

#define STORE_COLSTORED_C_24MASKx3_B0                                                               \
	STORE_COLSTORED_C24MASKB0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C24MASKB0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))     \
	STORE_COLSTORED_C24MASKB0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x80(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_12x3_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C12B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_12x3_B0                                       \
	STORE_COLSTORED_C12B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C12B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C12B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_11x3_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C11B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - 0x50(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - 0x50(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 * - zmm10, 11, 26 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_11x3_B0                                                               \
	STORE_COLSTORED_C11B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x50(rcx))                             \
	STORE_COLSTORED_C11B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x50(rcx, rdi, 1))     \
	STORE_COLSTORED_C11B0(10, 11, 26, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2), 0x50(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_10x3_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C10B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_10x3_B0                                       \
	STORE_COLSTORED_C10B0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C10B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C10B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_9x3_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C9B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_9x3_B0                                        \
	STORE_COLSTORED_C9B0(6, 7, mem(rcx), 0x40(rcx))                     \
	STORE_COLSTORED_C9B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))     \
	STORE_COLSTORED_C9B0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_8x3_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_8x3_B0                \
	STORE_COLSTORED_C8B0(6, mem(rcx))           \
	STORE_COLSTORED_C8B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C8B0(10, mem(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_16MASKx3_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C16MASKB0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next m_left elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next m_left elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 * - zmm10, 11 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_16MASKx3_B0                                       \
	STORE_COLSTORED_C16MASKB0(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16MASKB0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))    \
	STORE_COLSTORED_C16MASKB0(10, 11, mem(rcx, rdi, 2), 0x40(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_4x3_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C4B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_4x3_B0                \
	STORE_COLSTORED_C4B0(6, mem(rcx))           \
	STORE_COLSTORED_C4B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C4B0(10, mem(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_2x3_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C2B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_2x3_B0                \
	STORE_COLSTORED_C2B0(6, mem(rcx))           \
	STORE_COLSTORED_C2B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C2B0(10, mem(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_1x3_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C1B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_1x3_B0                \
	STORE_COLSTORED_C1B0(6, mem(rcx))           \
	STORE_COLSTORED_C1B0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C1B0(10, mem(rcx, rdi, 2))

/**
 * @brief STORE_COLSTORED_C_8MASKx3_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8MASKB0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] m_left elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] m_left elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 * - zmm10 holds A * B for C[0, 2]
 */
#define STORE_COLSTORED_C_8MASKx3_B0                \
	STORE_COLSTORED_C8MASKB0(6, mem(rcx))           \
	STORE_COLSTORED_C8MASKB0(8, mem(rcx, rdi, 1))   \
	STORE_COLSTORED_C8MASKB0(10, mem(rcx, rdi, 2))

/**************************END COLSTORED C STORE Bn x3********************************/

/**************************START COLSTORED C STORE Bn x2********************************/
/**
 * @brief STORE_COLSTORED_C_24x2_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C24Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_24x2_Bn                                                               \
	STORE_COLSTORED_C24Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C24Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_20x2_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C20Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_20x2_Bn                                                                   \
	STORE_COLSTORED_C20Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                                 \
	STORE_COLSTORED_C20Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_18x2_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C18Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_18x2_Bn                                                               \
	STORE_COLSTORED_C18Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                             \
	STORE_COLSTORED_C18Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_17x2_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C17Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_17x2_Bn                                                           \
	STORE_COLSTORED_C17Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))                         \
	STORE_COLSTORED_C17Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_16x2_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C16Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_16x2_Bn                                       \
	STORE_COLSTORED_C16Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C16Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))

#define STORE_COLSTORED_C_24MASKx2_Bn               \
	vmulpd( zmm30, zmm6, zmm6 )                     \
	vfmadd231pd( mem(rcx), zmm31, zmm6)             \
	vmovupd( zmm6, mem(rcx))                        \
                                                    \
	vmulpd( zmm30, zmm7, zmm7 )                     \
	vfmadd231pd( 0x40(rcx), zmm31, zmm7)            \
	vmovupd( zmm7, 0x40(rcx))                       \
                                                    \
	vmulpd( zmm30, zmm8, zmm8 )                     \
	vfmadd231pd( mem(rcx, rdi, 1), zmm31, zmm8)     \
	vmovupd( zmm8, mem(rcx, rdi, 1))                \
                                                    \
	vmulpd( zmm30, zmm9, zmm9 )                     \
	vfmadd231pd( 0x40(rcx, rdi, 1), zmm31, zmm9)    \
	vmovupd( zmm9, 0x40(rcx, rdi, 1))               \
                                                    \
	vmulpd( zmm30, zmm28, zmm28 )                   \
	vmovupd( 0x80(rcx), zmm0 MASK_KZ(2))            \
                                                    \
	vfmadd231pd( zmm0, zmm31, zmm28)                \
	vmovupd( zmm28, 0x80(rcx) MASK_(k(2)))          \
                                                    \
	vmulpd( zmm30, zmm29, zmm29 )                   \
	vmovupd( 0x80(rcx, rdi, 1), zmm1 MASK_KZ(2))    \
                                                    \
	vfmadd231pd( zmm1, zmm31, zmm29)                \
	vmovupd( zmm29, 0x80(rcx, rdi, 1) MASK_(k(2)))

/**
 * @brief STORE_COLSTORED_C_12x2_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C12Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_12x2_Bn                                       \
	STORE_COLSTORED_C12Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C12Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_11x2_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C11Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - 0x50(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - 0x50(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_11x2_Bn                                                               \
	STORE_COLSTORED_C11Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x50(rcx))                             \
	STORE_COLSTORED_C11Bn(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x50(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_10x2_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C10Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_10x2_Bn                                       \
	STORE_COLSTORED_C10Bn(6, 7, mem(rcx), 0x40(rcx))                    \
	STORE_COLSTORED_C10Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_9x2_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C9Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_9x2_Bn                                    \
	STORE_COLSTORED_C9Bn(6, 7, mem(rcx), 0x40(rcx))                 \
	STORE_COLSTORED_C9Bn(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_8x2_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_8x2_Bn                \
	STORE_COLSTORED_C8Bn(6, mem(rcx))           \
	STORE_COLSTORED_C8Bn(8, mem(rcx, rdi, 1))

#define STORE_COLSTORED_C_16MASKx2_Bn               \
	vmulpd( zmm30, zmm6, zmm6 )                     \
	vfmadd231pd( mem(rcx), zmm31, zmm6)             \
	vmovupd( zmm6, mem(rcx))                        \
                                                    \
	vmulpd( zmm30, zmm8, zmm8 )                     \
	vfmadd231pd( mem(rcx, rdi, 1), zmm31, zmm8)     \
	vmovupd( zmm8, mem(rcx, rdi, 1))                \
                                                    \
	vmulpd( zmm30, zmm7, zmm7 )                     \
	vmovupd( 0x40(rcx), zmm0 MASK_KZ(2))            \
                                                    \
	vfmadd231pd( zmm0, zmm31, zmm7)                 \
	vmovupd( zmm7, 0x40(rcx) MASK_(k(2)))           \
                                                    \
	vmulpd( zmm30, zmm9, zmm9 )                     \
	vmovupd( 0x40(rcx, rdi, 1), zmm1 MASK_KZ(2))    \
                                                    \
	vfmadd231pd( zmm1, zmm31, zmm9)                 \
	vmovupd( zmm9, 0x40(rcx, rdi, 1) MASK_(k(2)))

/**
 * @brief STORE_COLSTORED_C_4x2_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C4Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_4x2_Bn                \
	STORE_COLSTORED_C4Bn(0, 6, mem(rcx))        \
	STORE_COLSTORED_C4Bn(1, 8, mem(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_2x2_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C2Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_2x2_Bn                \
	STORE_COLSTORED_C2Bn(0, 6, mem(rcx))        \
	STORE_COLSTORED_C2Bn(1, 8, mem(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_1x2_Bn
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C1Bn macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_1x2_Bn                \
	STORE_COLSTORED_C1Bn(0, 6, mem(rcx))        \
	STORE_COLSTORED_C1Bn(1, 8, mem(rcx, rdi, 1))

#define STORE_COLSTORED_C_8MASKx2_Bn                \
    vmulpd( zmm30, zmm6, zmm6 )                     \
    vmovupd( mem(rcx), zmm0 MASK_KZ(2))             \
                                                    \
    vmulpd( zmm30, zmm8, zmm8 )                     \
    vmovupd( mem(rcx, rdi, 1), zmm1 MASK_KZ(2))     \
                                                    \
    vfmadd231pd( zmm0, zmm31, zmm6)                 \
    vmovupd( zmm6, mem(rcx) MASK_(k(2)))            \
                                                    \
    vfmadd231pd( zmm1, zmm31, zmm8)                 \
    vmovupd( zmm8, mem(rcx, rdi, 1) MASK_(k(2)))

/******************************************************/
/**
 * @brief STORE_COLSTORED_C_24x2_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C24B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_24x2_B0\
	STORE_COLSTORED_C24B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))\
	STORE_COLSTORED_C24B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_20x2_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C20B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_20x2_B0\
	STORE_COLSTORED_C20B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))\
	STORE_COLSTORED_C20B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_18x2_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C18B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_18x2_B0\
	STORE_COLSTORED_C18B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))\
	STORE_COLSTORED_C18B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_17x2_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C17B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - 0x80(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - 0x80(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_17x2_B0\
	STORE_COLSTORED_C17B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))\
	STORE_COLSTORED_C17B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_16x2_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C16B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_16x2_B0\
	STORE_COLSTORED_C16B0(6, 7, mem(rcx), 0x40(rcx))\
	STORE_COLSTORED_C16B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))

#define STORE_COLSTORED_C_24MASKx2_B0\
	STORE_COLSTORED_C24MASKB0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))\
	STORE_COLSTORED_C24MASKB0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x80(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_12x2_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C12B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_12x2_B0\
	STORE_COLSTORED_C12B0(6, 7, mem(rcx), 0x40(rcx))\
	STORE_COLSTORED_C12B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_11x2_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C11B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - 0x50(rcx) = C[16, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - 0x50(rdx) = 4th Column C[16, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7, 28 holds A * B for C[0, 0]
 * - zmm8,   9, 29 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_11x2_B0\
	STORE_COLSTORED_C11B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x50(rcx))\
	STORE_COLSTORED_C11B0(8, 9, 29, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1), 0x50(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_10x2_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C10B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_10x2_B0\
	STORE_COLSTORED_C10B0(6, 7, mem(rcx), 0x40(rcx))\
	STORE_COLSTORED_C10B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_9x2_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C9B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_9x2_B0\
	STORE_COLSTORED_C9B0(6, 7, mem(rcx), 0x40(rcx))\
	STORE_COLSTORED_C9B0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_8x2_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_8x2_B0\
	STORE_COLSTORED_C8B0(6, mem(rcx))\
	STORE_COLSTORED_C8B0(8, mem(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_16MASKx2_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C16MASKB0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 8 elements from current column
 * - 0x40(rcx) = C[8, 0] next m_left elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 8 elements from current column
 * - 0x40(rdx) = 4th Column C[8, 0] next m_left elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6,   7 holds A * B for C[0, 0]
 * - zmm8,   9 holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_16MASKx2_B0\
	STORE_COLSTORED_C16MASKB0(6, 7, mem(rcx), 0x40(rcx))\
	STORE_COLSTORED_C16MASKB0(8, 9, mem(rcx, rdi, 1), 0x40(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_4x2_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C4B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 4 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 4 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_4x2_B0\
	STORE_COLSTORED_C4B0(6, mem(rcx))\
	STORE_COLSTORED_C4B0(8, mem(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_2x2_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C2B0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 2 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 2 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_2x2_B0\
	STORE_COLSTORED_C2B0(6, mem(rcx))\
	STORE_COLSTORED_C2B0(8, mem(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_1x2_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8MASKB0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] 1 elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] 1 elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_1x2_B0\
	STORE_COLSTORED_C1B0(6, mem(rcx))\
	STORE_COLSTORED_C1B0(8, mem(rcx, rdi, 1))

/**
 * @brief STORE_COLSTORED_C_8MASKx2_B0
 *
 * This macro stores column-stored data into memory locations.
 * It uses the STORE_COLSTORED_C8MASKB0 macro to handle the storage
 * for different sets of registers and memory addresses.
 *
 * Parameters:
 * - rcx = memory address of C matrix.
 * - mem(rcx)  = C[0, 0] m_left elements from current column
 * - rdx = 4th column of C matrix.
 * - mem(rdx)  = 4th Column C[0, 0] m_left elements from current column
 * - rdi = column stride of C matrix in bytes.
 * - r13 = 3rd column of C matrix
 *
 * - zmm6, holds A * B for C[0, 0]
 * - zmm8, holds A * B for C[0, 1]
 */
#define STORE_COLSTORED_C_8MASKx2_B0\
	STORE_COLSTORED_C8MASKB0(6, mem(rcx))\
	STORE_COLSTORED_C8MASKB0(8, mem(rcx, rdi, 1))

/**************************END COLSTORED C STORE Bn x2********************************/

/**************************START COLSTORED C STORE Bn x1********************************/
#define STORE_COLSTORED_C_24x1_Bn\
	STORE_COLSTORED_C24Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))

#define STORE_COLSTORED_C_20x1_Bn\
	STORE_COLSTORED_C20Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))

#define STORE_COLSTORED_C_18x1_Bn\
	STORE_COLSTORED_C18Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))

#define STORE_COLSTORED_C_17x1_Bn\
	STORE_COLSTORED_C17Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))

#define STORE_COLSTORED_C_16x1_Bn\
	STORE_COLSTORED_C16Bn(6, 7, mem(rcx), 0x40(rcx))

#define STORE_COLSTORED_C_24MASKx1_Bn\
	STORE_COLSTORED_C24MASKBn(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))

#define STORE_COLSTORED_C_12x1_Bn\
	STORE_COLSTORED_C12Bn(6, 7, mem(rcx), 0x40(rcx))

#define STORE_COLSTORED_C_11x1_Bn\
	STORE_COLSTORED_C11Bn(6, 7, 28, mem(rcx), 0x40(rcx), 0x50(rcx))

#define STORE_COLSTORED_C_10x1_Bn\
	STORE_COLSTORED_C10Bn(6, 7, mem(rcx), 0x40(rcx))

#define STORE_COLSTORED_C_9x1_Bn\
	STORE_COLSTORED_C9Bn(6, 7, mem(rcx), 0x40(rcx))

#define STORE_COLSTORED_C_8x1_Bn\
	STORE_COLSTORED_C8Bn(6, mem(rcx))

#define STORE_COLSTORED_C_16MASKx1_Bn\
	STORE_COLSTORED_C16MASKBn(6, 7, mem(rcx), 0x40(rcx))

#define STORE_COLSTORED_C_4x1_Bn\
	STORE_COLSTORED_C4Bn(0, 6, mem(rcx))

#define STORE_COLSTORED_C_2x1_Bn\
	STORE_COLSTORED_C2Bn(0, 6, mem(rcx))

#define STORE_COLSTORED_C_1x1_Bn\
	STORE_COLSTORED_C1Bn(0, 6, mem(rcx))

#define STORE_COLSTORED_C_8MASKx1_Bn\
	STORE_COLSTORED_C8MASKBn(6, mem(rcx))

/*********************************************/
#define STORE_COLSTORED_C_24x1_B0\
	STORE_COLSTORED_C24B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))

#define STORE_COLSTORED_C_20x1_B0\
	STORE_COLSTORED_C20B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))

#define STORE_COLSTORED_C_18x1_B0\
	STORE_COLSTORED_C18B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))

#define STORE_COLSTORED_C_17x1_B0\
	STORE_COLSTORED_C17B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))

#define STORE_COLSTORED_C_16x1_B0\
	STORE_COLSTORED_C16B0(6, 7, mem(rcx), 0x40(rcx))

#define STORE_COLSTORED_C_24MASKx1_B0\
	STORE_COLSTORED_C24MASKB0(6, 7, 28, mem(rcx), 0x40(rcx), 0x80(rcx))

#define STORE_COLSTORED_C_12x1_B0\
	STORE_COLSTORED_C12B0(6, 7, mem(rcx), 0x40(rcx))

#define STORE_COLSTORED_C_11x1_B0\
	STORE_COLSTORED_C11B0(6, 7, 28, mem(rcx), 0x40(rcx), 0x50(rcx))

#define STORE_COLSTORED_C_10x1_B0\
	STORE_COLSTORED_C10B0(6, 7, mem(rcx), 0x40(rcx))

#define STORE_COLSTORED_C_9x1_B0\
	STORE_COLSTORED_C9B0(6, 7, mem(rcx), 0x40(rcx))

#define STORE_COLSTORED_C_8x1_B0\
	STORE_COLSTORED_C8B0(6, mem(rcx))

#define STORE_COLSTORED_C_16MASKx1_B0\
	STORE_COLSTORED_C16MASKB0(6, 7, mem(rcx), 0x40(rcx))

#define STORE_COLSTORED_C_4x1_B0\
	STORE_COLSTORED_C4B0(6, mem(rcx))

#define STORE_COLSTORED_C_2x1_B0\
	STORE_COLSTORED_C2B0(6, mem(rcx))

#define STORE_COLSTORED_C_1x1_B0\
	STORE_COLSTORED_C1B0(6, mem(rcx))

#define STORE_COLSTORED_C_8MASKx1_B0\
	STORE_COLSTORED_C8MASKB0(6, mem(rcx))


/**************************START ROWSTORED C STORE Bn x8********************************/
#define STORE_ROWSTORED_C_24x8_Bn\
	lea(mem(rsi,  rsi,  2), r12)    /* r12 = 3*rs_c */              \
	lea(mem(r12, rsi,  2), r13)     /* r13 = 5*rs_c */              \
	lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */              \
	lea(mem(   , rsi, 8), r14)                                      \
	UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                        \
	SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                          \
                                                                    \
	UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                      \
	SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
	SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
	SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                        \
                                                                    \
	vbroadcastsd(mem(rbx), zmm31)                                   \
	UPDATE_C                                                        \
	                                /* First 8x8 tile updated */    \
                                                                    \
	UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)                        \
	SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
	UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)                      \
	SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
	SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
	SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
                                                                    \
	UPDATE_C                                                        \
	                                /* Second 8x8 tile updated */   \
                                                                    \
	UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)                      \
	SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
	UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)                      \
	SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
	SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
	SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
                                                                    \
	                                /* Third 8x8 tile updated */    \
	UPDATE_C

#define STORE_ROWSTORED_C_20x8_Bn\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */          \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */          \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */          \
    lea(mem(   , rsi, 8), r14)                                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                      \
                                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                      \
                                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                    \
                                                                \
    mov(var(beta), rbx)             /* load address of beta */  \
    vbroadcastsd(mem(rbx), zmm31)   /* broadcast beta */        \
    UPDATE_C_8                                                  \
                                    /*First 8x8 tile updated*/  \
                                                                \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)                    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                        \
                                                                \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)                  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                      \
                                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                      \
                                                                \
    UPDATE_C_8                                                  \
                                    /*Second 8x8 tile updated*/ \
                                                                \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)                  \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                        \
                                                                \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)                  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                      \
                                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                      \
    UPDATE_C_4

#define STORE_ROWSTORED_C_18x8_Bn\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */              \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */              \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */              \
    lea(mem(   , rsi, 8), r14)                                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                          \
                                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                        \
                                                                    \
    mov(var(beta), rbx)             /* load address of beta */      \
    vbroadcastsd(mem(rbx), zmm31)   /* broadcast beta */            \
    UPDATE_C_8                                                      \
                                    /* First 8x8 tile updated */    \
                                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
                                                                    \
    UPDATE_C_8                                                      \
                                    /* Second 8x8 tile updated */   \
                                                                    \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
    UPDATE_C_2

#define STORE_ROWSTORED_C_17x8_Bn\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */          \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */          \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */          \
    lea(mem(   , rsi, 8), r14)                                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                      \
                                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                      \
                                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                    \
                                                                \
    mov(var(beta), rbx)             /* load address of beta */  \
    vbroadcastsd(mem(rbx), zmm31)   /* broadcast beta */        \
    UPDATE_C_8                                                  \
                                    /*First 8x8 tile updated*/  \
                                                                \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)                    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                        \
                                                                \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)                  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                      \
                                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                      \
                                                                \
    UPDATE_C_8                                                  \
                                    /*Second 8x8 tile updated*/ \
                                                                \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)                  \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                        \
                                                                \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)                  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                      \
                                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                      \
    UPDATE_C_1

#define STORE_ROWSTORED_C_16x8_Bn\
    lea(mem(rsi,  rsi,  2), r12)                        \
    lea(mem(r12, rsi,  2), r13)                         \
    lea(mem(r12, rsi,  4), rdx)                         \
    lea(mem(   , rsi, 8), r14)                          \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)            \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)              \
                                                        \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)          \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)              \
                                                        \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)            \
                                                        \
    mov(var(beta), rbx) /* load address of beta */      \
    vbroadcastsd(mem(rbx), zmm31) /* broadcast beta */  \
    UPDATE_C_8                                          \
                                                        \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)            \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                \
                                                        \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)          \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)              \
                                                        \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)              \
    UPDATE_C_8


#define STORE_ROWSTORED_C_24MASKx8_Bn\
    lea(mem(rsi,  rsi,  2), r12)                        \
    lea(mem(r12, rsi,  2), r13)                         \
    lea(mem(r12, rsi,  4), rdx)                         \
    lea(mem(   , rsi, 8), r14)                          \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)            \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)              \
                                                        \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)          \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)              \
                                                        \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)            \
                                                        \
    mov(var(beta), rbx)                                 \
    vbroadcastsd(mem(rbx), zmm31)                       \
    UPDATE_C_8                                          \
                                                        \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)            \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                \
                                                        \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)          \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)              \
                                                        \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)              \
                                                        \
    UPDATE_C_8                                          \
                                                        \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)          \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                \
                                                        \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)          \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)              \
                                                        \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)              \
                                                        \
    mov(var(m_left), rdi)                               \
    sub(imm(16), rdi)                                   \
    cmp(imm(7), rdi)                                    \
    JZ(.UPDATE24MASK7)                                  \
    cmp(imm(6), rdi)                                    \
    JZ(.UPDATE24MASK6)                                  \
    cmp(imm(5), rdi)                                    \
    JZ(.UPDATE24MASK5)                                  \
    cmp(imm(4), rdi)                                    \
    JZ(.UPDATE24MASK4)                                  \
    cmp(imm(3), rdi)                                    \
    JZ(.UPDATE24MASK3)                                  \
    cmp(imm(2), rdi)                                    \
    JZ(.UPDATE24MASK2)                                  \
    cmp(imm(1), rdi)                                    \
    JZ(.UPDATE24MASK1)                                  \
    cmp(imm(0), rdi)                                    \
    JZ(.UPDATE24MASK0)                                  \
    LABEL(.UPDATE24MASK7)                               \
    UPDATE_C_7                                          \
    jmp(.CONCLUDE)                                      \
    LABEL(.UPDATE24MASK6)                               \
    UPDATE_C_6                                          \
    jmp(.CONCLUDE)                                      \
    LABEL(.UPDATE24MASK5)                               \
    UPDATE_C_5                                          \
    jmp(.CONCLUDE)                                      \
    LABEL(.UPDATE24MASK4)                               \
    UPDATE_C_4                                          \
    jmp(.CONCLUDE)                                      \
    LABEL(.UPDATE24MASK3)                               \
    UPDATE_C_3                                          \
    jmp(.CONCLUDE)                                      \
    LABEL(.UPDATE24MASK2)                               \
    UPDATE_C_2                                          \
    jmp(.CONCLUDE)                                      \
    LABEL(.UPDATE24MASK1)                               \
    UPDATE_C_1                                          \
    jmp(.CONCLUDE)                                      \
    LABEL(.UPDATE24MASK0)

#define STORE_ROWSTORED_C_12x8_Bn\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */              \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */              \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */              \
    lea(mem(   , rsi, 8), r14)                                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                          \
                                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                        \
                                                                    \
    mov(var(beta), rbx)                                             \
    vbroadcastsd(mem(rbx), zmm31)                                   \
    UPDATE_C_8                                                      \
                                    /* First 8x8 tile updated */    \
                                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
                                                                    \
                                    /*Second 8x8 tile updated*/     \
    UPDATE_C_4                                                      \

#define STORE_ROWSTORED_C_11x8_Bn\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */              \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */              \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */              \
    lea(mem(   , rsi, 8), r14)                                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                          \
                                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                        \
                                                                    \
    mov(var(beta), rbx)                                             \
    vbroadcastsd(mem(rbx), zmm31)                                   \
    UPDATE_C_8                                                      \
                                    /* First 8x8 tile updated */    \
                                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
                                                                    \
                                    /* Second 8x8 tile updated */   \
    UPDATE_C_2                                                      \
                                                                    \
    lea(mem(   , rsi, 2), r14)                                      \
    add(r14, rcx)                                                   \
                                                                    \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
    UPDATE_C_1                                                      \

#define STORE_ROWSTORED_C_10x8_Bn\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */              \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */              \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */              \
    lea(mem(   , rsi, 8), r14)                                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                          \
                                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                        \
                                                                    \
    mov(var(beta), rbx)                                             \
    vbroadcastsd(mem(rbx), zmm31)                                   \
    UPDATE_C_8                                                      \
                                    /* First 8x8 tile updated */    \
                                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
                                                                    \
                                    /* Second 8x8 tile updated */   \
    UPDATE_C_2

#define STORE_ROWSTORED_C_9x8_Bn\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */              \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */              \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */              \
    lea(mem(   , rsi, 8), r14)                                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                          \
                                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                        \
                                                                    \
    mov(var(beta), rbx)                                             \
    vbroadcastsd(mem(rbx), zmm31)                                   \
    UPDATE_C_8                                                      \
                                    /* First 8x8 tile updated */    \
                                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
                                                                    \
                                    /* Second 8x8 tile updated */   \
    UPDATE_C_1

#define STORE_ROWSTORED_C_8x8_Bn\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */          \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */          \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */          \
    lea(mem(   , rsi, 8), r14)                                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                      \
                                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                      \
                                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                    \
                                                                \
    mov(var(beta), rbx)             /* load address of beta */  \
    vbroadcastsd(mem(rbx), zmm31)   /* broadcast beta */        \
    UPDATE_C

#define STORE_ROWSTORED_C_16MASKx8_Bn\
    lea(mem(rsi,  rsi,  2), r12)                        \
    lea(mem(r12, rsi,  2), r13)                         \
    lea(mem(r12, rsi,  4), rdx)                         \
    lea(mem(   , rsi, 8), r14)                          \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)            \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)              \
                                                        \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)          \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)              \
                                                        \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)            \
                                                        \
    mov(var(beta), rbx)                                 \
    vbroadcastsd(mem(rbx), zmm31)                       \
    UPDATE_C_8                                          \
                                                        \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)            \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                \
                                                        \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)          \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)              \
                                                        \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)              \
                                                        \
    mov(var(m_left), rdi)                               \
    sub(imm(8), rdi)                                    \
    cmp(imm(7), rdi)                                    \
    JZ(.UPDATE16MASK7)                                  \
    cmp(imm(6), rdi)                                    \
    JZ(.UPDATE16MASK6)                                  \
    cmp(imm(5), rdi)                                    \
    JZ(.UPDATE16MASK5)                                  \
    cmp(imm(4), rdi)                                    \
    JZ(.UPDATE16MASK4)                                  \
    cmp(imm(3), rdi)                                    \
    JZ(.UPDATE16MASK3)                                  \
    cmp(imm(2), rdi)                                    \
    JZ(.UPDATE16MASK2)                                  \
    cmp(imm(1), rdi)                                    \
    JZ(.UPDATE16MASK1)                                  \
    cmp(imm(0), rdi)                                    \
    JZ(.UPDATE16MASK0)                                  \
    LABEL(.UPDATE16MASK7)                               \
    UPDATE_C_7                                          \
    jmp(.CONCLUDE)                                      \
    LABEL(.UPDATE16MASK6)                               \
    UPDATE_C_6                                          \
    jmp(.CONCLUDE)                                      \
    LABEL(.UPDATE16MASK5)                               \
    UPDATE_C_5                                          \
    jmp(.CONCLUDE)                                      \
    LABEL(.UPDATE16MASK4)                               \
    UPDATE_C_4                                          \
    jmp(.CONCLUDE)                                      \
    LABEL(.UPDATE16MASK3)                               \
    UPDATE_C_3                                          \
    jmp(.CONCLUDE)                                      \
    LABEL(.UPDATE16MASK2)                               \
    UPDATE_C_2                                          \
    jmp(.CONCLUDE)                                      \
    LABEL(.UPDATE16MASK1)                               \
    UPDATE_C_1                                          \
    jmp(.CONCLUDE)                                      \
    LABEL(.UPDATE16MASK0)                               \
    jmp(.CONCLUDE)

#define STORE_ROWSTORED_C_4x8_Bn\
    lea(mem(rsi,  rsi,  2), r12)    /*rdx = 3*rs_c*/    \
    lea(mem(r12, rsi,  2), r13)     /*rdx = 5*rs_c*/    \
    lea(mem(r12, rsi,  4), rdx)     /*rdx = 7*rs_c*/    \
    lea(mem(   , rsi, 8), r14)                          \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)            \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)              \
                                                        \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)          \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)              \
                                                        \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)            \
                                                        \
    mov(var(beta), rbx)                                 \
    vbroadcastsd(mem(rbx), zmm31)                       \
    UPDATE_C_4

#define STORE_ROWSTORED_C_2x8_Bn\
    lea(mem(rsi,  rsi,  2), r12)    /*rdx = 3*rs_c*/    \
    lea(mem(r12, rsi,  2), r13)     /*rdx = 5*rs_c*/    \
    lea(mem(r12, rsi,  4), rdx)     /*rdx = 7*rs_c*/    \
    lea(mem(   , rsi, 8), r14)                          \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)            \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)              \
                                                        \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)          \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)              \
                                                        \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)            \
                                                        \
    mov(var(beta), rbx)                                 \
    vbroadcastsd(mem(rbx), zmm31)                       \
    UPDATE_C_2

#define STORE_ROWSTORED_C_1x8_Bn\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    mov(var(beta), rbx)                             \
    vbroadcastsd(mem(rbx), zmm31)                   \
    UPDATE_C_1

#define STORE_ROWSTORED_C_8MASKx8_Bn\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    mov(var(beta), rbx)                             \
    vbroadcastsd(mem(rbx), zmm31)                   \
                                                    \
    mov(var(m_left), rdi)                           \
    cmp(imm(7), rdi)                                \
    JZ(.UPDATE7)                                    \
    cmp(imm(6), rdi)                                \
    JZ(.UPDATE6)                                    \
    cmp(imm(5), rdi)                                \
    JZ(.UPDATE5)                                    \
    cmp(imm(4), rdi)                                \
    JZ(.UPDATE4)                                    \
    cmp(imm(3), rdi)                                \
    JZ(.UPDATE3)                                    \
    cmp(imm(2), rdi)                                \
    JZ(.UPDATE2)                                    \
    cmp(imm(1), rdi)                                \
    JZ(.UPDATE1)                                    \
    cmp(imm(0), rdi)                                \
    JZ(.UPDATE0)                                    \
    LABEL(.UPDATE7)                                 \
    UPDATE_C_7                                      \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE6)                                 \
    UPDATE_C_6                                      \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE5)                                 \
    UPDATE_C_5                                      \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE4)                                 \
    UPDATE_C_4                                      \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE3)                                 \
    UPDATE_C_3                                      \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE2)                                 \
    UPDATE_C_2                                      \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE1)                                 \
    UPDATE_C_1                                      \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE0)


/*************************************************************/
#define STORE_ROWSTORED_C_24x8_B0\
	lea(mem(rsi,  rsi,  2), r12)    /* r12 = 3*rs_c */              \
	lea(mem(r12, rsi,  2), r13)     /* r13 = 5*rs_c */              \
	lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */              \
	lea(mem(   , rsi, 8), r14)                                      \
	UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                        \
	SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                          \
                                                                    \
	UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                      \
	SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
	SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
	SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                        \
                                                                    \
	UPDATE_C_BZ                                                     \
	                                /* First 8x8 tile updated */    \
                                                                    \
	UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)                        \
	SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
	UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)                      \
	SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
	SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
	SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
                                                                    \
	UPDATE_C_BZ                                                     \
	                                /* Second 8x8 tile updated */   \
                                                                    \
	UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)                      \
	SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
	UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)                      \
	SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
	SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
	SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
                                                                    \
	                                /* Third 8x8 tile updated */    \
	UPDATE_C_BZ

#define STORE_ROWSTORED_C_20x8_B0\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */              \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */              \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */              \
    lea(mem(   , rsi, 8), r14)                                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                          \
                                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                        \
                                                                    \
    mov(var(beta), rbx)             /* load address of beta */      \
    vbroadcastsd(mem(rbx), zmm31)   /* broadcast beta */            \
    UPDATE_C_8_BZ                                                   \
                                    /* First 8x8 tile updated */    \
                                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
                                                                    \
    UPDATE_C_8_BZ                                                   \
                                    /* Second 8x8 tile updated */   \
                                                                    \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
    UPDATE_C_4_BZ

#define STORE_ROWSTORED_C_18x8_B0\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */              \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */              \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */              \
    lea(mem(   , rsi, 8), r14)                                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                          \
                                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                        \
                                                                    \
    UPDATE_C_8_BZ                                                   \
                                    /* First 8x8 tile updated */    \
                                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
                                                                    \
    UPDATE_C_8_BZ                                                   \
                                    /* Second 8x8 tile updated */   \
                                                                    \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
    UPDATE_C_2_BZ

#define STORE_ROWSTORED_C_17x8_B0\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */              \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */              \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */              \
    lea(mem(   , rsi, 8), r14)                                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                          \
                                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                        \
                                                                    \
    UPDATE_C_8_BZ                                                   \
                                    /* First 8x8 tile updated */    \
                                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
                                                                    \
    UPDATE_C_8_BZ                                                   \
                                    /* Second 8x8 tile updated */   \
                                                                    \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
    UPDATE_C_1_BZ

#define STORE_ROWSTORED_C_16x8_B0\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    UPDATE_C_8_BZ                                   \
                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)            \
                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)          \
    UPDATE_C_8_BZ

#define STORE_ROWSTORED_C_24MASKx8_B0\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    UPDATE_C_8_BZ                               \
                                                \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
                                                \
    UPDATE_C_8_BZ                               \
                                                \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)  \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
                                                \
    mov(var(m_left), rdi)                       \
    sub(imm(16), rdi)                           \
    cmp(imm(7), rdi)                            \
    JZ(.UPDATE24MASK7B0)                        \
    cmp(imm(6), rdi)                            \
    JZ(.UPDATE24MASK6B0)                        \
    cmp(imm(5), rdi)                            \
    JZ(.UPDATE24MASK5B0)                        \
    cmp(imm(4), rdi)                            \
    JZ(.UPDATE24MASK4B0)                        \
    cmp(imm(3), rdi)                            \
    JZ(.UPDATE24MASK3B0)                        \
    cmp(imm(2), rdi)                            \
    JZ(.UPDATE24MASK2B0)                        \
    cmp(imm(1), rdi)                            \
    JZ(.UPDATE24MASK1B0)                        \
    cmp(imm(0), rdi)                            \
    JZ(.UPDATE24MASK0B0)                        \
    LABEL(.UPDATE24MASK7B0)                     \
    UPDATE_C_7_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE24MASK6B0)                     \
    UPDATE_C_6_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE24MASK5B0)                     \
    UPDATE_C_5_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE24MASK4B0)                     \
    UPDATE_C_4_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE24MASK3B0)                     \
    UPDATE_C_3_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE24MASK2B0)                     \
    UPDATE_C_2_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE24MASK1B0)                     \
    UPDATE_C_1_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE24MASK0B0)

#define STORE_ROWSTORED_C_12x8_B0\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */              \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */              \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */              \
    lea(mem(   , rsi, 8), r14)                                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                          \
                                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                        \
                                                                    \
    UPDATE_C_8_BZ                                                   \
                                    /* First 8x8 tile updated */    \
                                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
                                                                    \
                                    /* Second 8x8 tile updated */   \
    UPDATE_C_4_BZ                                                   \

#define STORE_ROWSTORED_C_11x8_B0\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */              \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */              \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */              \
    lea(mem(   , rsi, 8), r14)                                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                          \
                                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                        \
                                                                    \
    UPDATE_C_8_BZ                                                   \
                                    /* First 8x8 tile updated */    \
                                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
                                                                    \
                                    /* Second 8x8 tile updated */   \
    UPDATE_C_2_BZ                                                   \
                                                                    \
    lea(mem(   , rsi, 2), r14)                                      \
    add(r14, rcx)                                                   \
                                                                    \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
    UPDATE_C_1_BZ                                                   \

#define STORE_ROWSTORED_C_10x8_B0\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */              \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */              \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */              \
    lea(mem(   , rsi, 8), r14)                                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                          \
                                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                        \
                                                                    \
    UPDATE_C_8_BZ                                                   \
                                    /* First 8x8 tile updated */    \
                                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
                                                                    \
                                    /* Second 8x8 tile updated */   \
    UPDATE_C_2_BZ

#define STORE_ROWSTORED_C_9x8_B0\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */              \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */              \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */              \
    lea(mem(   , rsi, 8), r14)                                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                          \
                                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                        \
                                                                    \
    UPDATE_C_8_BZ                                                   \
                                    /* First 8x8 tile updated */    \
                                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)                            \
                                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)                          \
                                                                    \
                                    /* Second 8x8 tile updated */   \
    UPDATE_C_1_BZ

#define STORE_ROWSTORED_C_8x8_B0\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */              \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */              \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */              \
    lea(mem(   , rsi, 8), r14)                                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)                        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)                          \
                                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)                      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)                          \
                                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)                        \
                                                                    \
    UPDATE_C_BZ

#define STORE_ROWSTORED_C_16MASKx8_B0\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    UPDATE_C_8_BZ                               \
                                                \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
                                                \
    mov(var(m_left), rdi)                       \
    sub(imm(8), rdi)                            \
    cmp(imm(7), rdi)                            \
    JZ(.UPDATE16MASK7B0)                        \
    cmp(imm(6), rdi)                            \
    JZ(.UPDATE16MASK6B0)                        \
    cmp(imm(5), rdi)                            \
    JZ(.UPDATE16MASK5B0)                        \
    cmp(imm(4), rdi)                            \
    JZ(.UPDATE16MASK4B0)                        \
    cmp(imm(3), rdi)                            \
    JZ(.UPDATE16MASK3B0)                        \
    cmp(imm(2), rdi)                            \
    JZ(.UPDATE16MASK2B0)                        \
    cmp(imm(1), rdi)                            \
    JZ(.UPDATE16MASK1B0)                        \
    cmp(imm(0), rdi)                            \
    JZ(.UPDATE16MASK0B0)                        \
    LABEL(.UPDATE16MASK7B0)                     \
    UPDATE_C_7_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK6B0)                     \
    UPDATE_C_6_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK5B0)                     \
    UPDATE_C_5_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK4B0)                     \
    UPDATE_C_4_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK3B0)                     \
    UPDATE_C_3_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK2B0)                     \
    UPDATE_C_2_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK1B0)                     \
    UPDATE_C_1_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK0B0)                     \
    jmp(.CONCLUDE)

#define STORE_ROWSTORED_C_4x8_B0\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */  \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */  \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */  \
    lea(mem(   , rsi, 8), r14)                          \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)            \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)              \
                                                        \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)          \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)              \
                                                        \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)            \
                                                        \
    UPDATE_C_4_BZ

#define STORE_ROWSTORED_C_2x8_B0\
    lea(mem(rsi,  rsi,  2), r12)    /* rdx = 3*rs_c */  \
    lea(mem(r12, rsi,  2), r13)     /* rdx = 5*rs_c */  \
    lea(mem(r12, rsi,  4), rdx)     /* rdx = 7*rs_c */  \
    lea(mem(   , rsi, 8), r14)                          \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)            \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)              \
                                                        \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)          \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)              \
                                                        \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)                \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)            \
                                                        \
    UPDATE_C_2_BZ

#define STORE_ROWSTORED_C_1x8_B0\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    UPDATE_C_1_BZ

#define STORE_ROWSTORED_C_8MASKx8_B0\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    mov(var(m_left), rdi)                       \
    cmp(imm(7), rdi)                            \
    JZ(.UPDATE7B0)                              \
    cmp(imm(6), rdi)                            \
    JZ(.UPDATE6B0)                              \
    cmp(imm(5), rdi)                            \
    JZ(.UPDATE5B0)                              \
    cmp(imm(4), rdi)                            \
    JZ(.UPDATE4B0)                              \
    cmp(imm(3), rdi)                            \
    JZ(.UPDATE3B0)                              \
    cmp(imm(2), rdi)                            \
    JZ(.UPDATE2B0)                              \
    cmp(imm(1), rdi)                            \
    JZ(.UPDATE1B0)                              \
    cmp(imm(0), rdi)                            \
    JZ(.UPDATE0B0)                              \
    LABEL(.UPDATE7B0)                           \
    UPDATE_C_7_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE6B0)                           \
    UPDATE_C_6_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE5B0)                           \
    UPDATE_C_5_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE4B0)                           \
    UPDATE_C_4_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE3B0)                           \
    UPDATE_C_3_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE2B0)                           \
    UPDATE_C_2_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE1B0)                           \
    UPDATE_C_1_BZ                               \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE0B0)

/**************************END ROWSTORED C STORE Bn x8********************************/

/**************************START ROWSTORED C STORE Bn xN********************************/
#define STORE_ROWSTORED_C_24xN_Bn\
	lea(mem(rsi,  rsi,  2), r12)                \
	lea(mem(r12, rsi,  2), r13)                 \
	lea(mem(r12, rsi,  4), rdx)                 \
	lea(mem(   , rsi, 8), r14)                  \
	UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
	SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
	UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
	SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
	SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
	SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
	mov(var(beta), rbx)                         \
	vbroadcastsd(mem(rbx), zmm31)               \
	UPDATE_MASKED_C                             \
                                                \
	UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)    \
	SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
	UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)  \
	SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
	SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
	SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
                                                \
	UPDATE_MASKED_C                             \
                                                \
	UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)  \
	SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
	UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)  \
	SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
	SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
	SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
                                                \
	UPDATE_MASKED_C

#define STORE_ROWSTORED_C_20xN_Bn\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    mov(var(beta), rbx)                         \
    vbroadcastsd(mem(rbx), zmm31)               \
    UPDATE_MASKED_C_8                           \
    /*First 8x7 tile updated*/                  \
                                                \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
                                                \
    UPDATE_MASKED_C_8                           \
    /*Second 8x7 tile updated*/                 \
                                                \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)  \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
    UPDATE_MASKED_C_4

#define STORE_ROWSTORED_C_18xN_Bn\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    mov(var(beta), rbx)                         \
    vbroadcastsd(mem(rbx), zmm31)               \
    UPDATE_MASKED_C_8                           \
    /*First 8x7 tile updated*/                  \
                                                \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
                                                \
    UPDATE_MASKED_C_8                           \
    /*Second 8x7 tile updated*/                 \
                                                \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)  \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
    UPDATE_MASKED_C_2

#define STORE_ROWSTORED_C_17xN_Bn\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    mov(var(beta), rbx)                             \
    vbroadcastsd(mem(rbx), zmm31)                   \
    UPDATE_MASKED_C_8                               \
    /*First 8x7 tile updated*/                      \
                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)            \
                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)          \
                                                    \
    UPDATE_MASKED_C_8                               \
    /*Second 8x7 tile updated*/                     \
                                                    \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)      \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)            \
                                                    \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)          \
    UPDATE_MASKED_C_1

#define STORE_ROWSTORED_C_16xN_Bn\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    mov(var(beta), rbx)                             \
    vbroadcastsd(mem(rbx), zmm31)                   \
    UPDATE_MASKED_C_8                               \
    /*First 8x7 tile updated*/                      \
                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)            \
                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)          \
                                                    \
    UPDATE_MASKED_C_8

#define STORE_ROWSTORED_C_24MASKxN_Bn\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    mov(var(beta), rbx)                             \
    vbroadcastsd(mem(rbx), zmm31)                   \
    UPDATE_MASKED_C_8                               \
                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)            \
                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)          \
                                                    \
    UPDATE_MASKED_C_8                               \
                                                    \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)      \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)            \
                                                    \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)          \
                                                    \
    mov(var(m_left), rdi)                           \
    sub(imm(16), rdi)                               \
    cmp(imm(7), rdi)                                \
    JZ(.UPDATE24MASK7)                              \
    cmp(imm(6), rdi)                                \
    JZ(.UPDATE24MASK6)                              \
    cmp(imm(5), rdi)                                \
    JZ(.UPDATE24MASK5)                              \
    cmp(imm(4), rdi)                                \
    JZ(.UPDATE24MASK4)                              \
    cmp(imm(3), rdi)                                \
    JZ(.UPDATE24MASK3)                              \
    cmp(imm(2), rdi)                                \
    JZ(.UPDATE24MASK2)                              \
    cmp(imm(1), rdi)                                \
    JZ(.UPDATE24MASK1)                              \
    cmp(imm(0), rdi)                                \
    JZ(.UPDATE24MASK0)                              \
    LABEL(.UPDATE24MASK7)                           \
    UPDATE_MASKED_C_7                               \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE24MASK6)                           \
    UPDATE_MASKED_C_6                               \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE24MASK5)                           \
    UPDATE_MASKED_C_5                               \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE24MASK4)                           \
    UPDATE_MASKED_C_4                               \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE24MASK3)                           \
    UPDATE_MASKED_C_3                               \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE24MASK2)                           \
    UPDATE_MASKED_C_2                               \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE24MASK1)                           \
    UPDATE_MASKED_C_1                               \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE24MASK0)

#define STORE_ROWSTORED_C_12xN_Bn\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    mov(var(beta), rbx)                             \
    vbroadcastsd(mem(rbx), zmm31)                   \
    UPDATE_MASKED_C_8                               \
    /*First 8x7 tile updated*/                      \
                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)            \
                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)          \
                                                    \
    UPDATE_MASKED_C_4

#define STORE_ROWSTORED_C_11xN_Bn\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    mov(var(beta), rbx)                             \
    vbroadcastsd(mem(rbx), zmm31)                   \
    UPDATE_MASKED_C_8                               \
    /*First 8x7 tile updated*/                      \
                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)            \
                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)          \
                                                    \
    UPDATE_MASKED_C_2                               \
    lea(mem(   , rsi, 2), r14)                      \
    add(r14, rcx)                                   \
    /*Second 8x7 tile updated*/                     \
                                                    \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)      \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)            \
                                                    \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)          \
    UPDATE_MASKED_C_1

#define STORE_ROWSTORED_C_10xN_Bn\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    mov(var(beta), rbx)                             \
    vbroadcastsd(mem(rbx), zmm31)                   \
    UPDATE_MASKED_C_8                               \
    /*First 8x7 tile updated*/                      \
                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)            \
                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)          \
                                                    \
    UPDATE_MASKED_C_2

#define STORE_ROWSTORED_C_9xN_Bn\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    mov(var(beta), rbx)                             \
    vbroadcastsd(mem(rbx), zmm31)                   \
    UPDATE_MASKED_C_8                               \
    /*First 8x7 tile updated*/                      \
                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)            \
                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)          \
                                                    \
    UPDATE_MASKED_C_1

#define STORE_ROWSTORED_C_8xN_Bn\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    mov(var(beta), rbx)                             \
    vbroadcastsd(mem(rbx), zmm31)                   \
    UPDATE_MASKED_C_8


#define STORE_ROWSTORED_C_16MASKxN_Bn\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    mov(var(beta), rbx)                         \
    vbroadcastsd(mem(rbx), zmm31)               \
    UPDATE_MASKED_C_8                           \
                                                \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
                                                \
    mov(var(m_left), rdi)                       \
    sub(imm(8), rdi)                            \
    cmp(imm(7), rdi)                            \
    JZ(.UPDATE16MASK7)                          \
    cmp(imm(6), rdi)                            \
    JZ(.UPDATE16MASK6)                          \
    cmp(imm(5), rdi)                            \
    JZ(.UPDATE16MASK5)                          \
    cmp(imm(4), rdi)                            \
    JZ(.UPDATE16MASK4)                          \
    cmp(imm(3), rdi)                            \
    JZ(.UPDATE16MASK3)                          \
    cmp(imm(2), rdi)                            \
    JZ(.UPDATE16MASK2)                          \
    cmp(imm(1), rdi)                            \
    JZ(.UPDATE16MASK1)                          \
    cmp(imm(0), rdi)                            \
    JZ(.UPDATE16MASK0)                          \
                                                \
    LABEL(.UPDATE16MASK7)                       \
    UPDATE_MASKED_C_7                           \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK6)                       \
    UPDATE_MASKED_C_6                           \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK5)                       \
    UPDATE_MASKED_C_5                           \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK4)                       \
    UPDATE_MASKED_C_4                           \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK3)                       \
    UPDATE_MASKED_C_3                           \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK2)                       \
    UPDATE_MASKED_C_2                           \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK1)                       \
    UPDATE_MASKED_C_1                           \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK0)                       \
    jmp(.CONCLUDE)

#define STORE_ROWSTORED_C_4xN_Bn\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    mov(var(beta), rbx)                             \
    vbroadcastsd(mem(rbx), zmm31)                   \
    UPDATE_MASKED_C_4

#define STORE_ROWSTORED_C_2xN_Bn\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    mov(var(beta), rbx)                         \
    vbroadcastsd(mem(rbx), zmm31)               \
    UPDATE_MASKED_C_2

#define STORE_ROWSTORED_C_1xN_Bn\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    mov(var(beta), rbx)                         \
    vbroadcastsd(mem(rbx), zmm31)               \
    UPDATE_MASKED_C_1

#define STORE_ROWSTORED_C_8MASKxN_Bn\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    mov(var(beta), rbx)                         \
    vbroadcastsd(mem(rbx), zmm31)               \
                                                \
    mov(var(m_left), rdi)                       \
    cmp(imm(7), rdi)                            \
    JZ(.UPDATE7)                                \
    cmp(imm(6), rdi)                            \
    JZ(.UPDATE6)                                \
    cmp(imm(5), rdi)                            \
    JZ(.UPDATE5)                                \
    cmp(imm(4), rdi)                            \
    JZ(.UPDATE4)                                \
    cmp(imm(3), rdi)                            \
    JZ(.UPDATE3)                                \
    cmp(imm(2), rdi)                            \
    JZ(.UPDATE2)                                \
    cmp(imm(1), rdi)                            \
    JZ(.UPDATE1)                                \
    cmp(imm(0), rdi)                            \
    JZ(.UPDATE0)                                \
    LABEL(.UPDATE7)                             \
    UPDATE_MASKED_C_7                           \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE6)                             \
    UPDATE_MASKED_C_6                           \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE5)                             \
    UPDATE_MASKED_C_5                           \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE4)                             \
    UPDATE_MASKED_C_4                           \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE3)                             \
    UPDATE_MASKED_C_3                           \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE2)                             \
    UPDATE_MASKED_C_2                           \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE1)                             \
    UPDATE_MASKED_C_1                           \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE0)

/************************************************/

#define STORE_ROWSTORED_C_24xN_B0\
	lea(mem(rsi,  rsi,  2), r12)                \
	lea(mem(r12, rsi,  2), r13)                 \
	lea(mem(r12, rsi,  4), rdx)                 \
	lea(mem(   , rsi, 8), r14)                  \
	UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
	SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
	UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
	SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
	SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
	SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
	UPDATE_MASKED_C_BZ                          \
                                                \
	UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)    \
	SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
	UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)  \
	SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
	SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
	SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
                                                \
	UPDATE_MASKED_C_BZ                          \
                                                \
	UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)  \
	SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
	UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)  \
	SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
	SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
	SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
                                                \
	UPDATE_MASKED_C_BZ

#define STORE_ROWSTORED_C_20xN_B0\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    UPDATE_MASKED_C_8_BZ                        \
    /*First 8x7 tile updated*/                  \
                                                \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
                                                \
    UPDATE_MASKED_C_8_BZ                        \
    /*Second 8x7 tile updated*/                 \
                                                \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)  \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
    UPDATE_MASKED_C_4_BZ

#define STORE_ROWSTORED_C_18xN_B0\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    UPDATE_MASKED_C_8_BZ                        \
    /*First 8x7 tile updated*/                  \
                                                \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
                                                \
    UPDATE_MASKED_C_8_BZ                        \
    /*Second 8x7 tile updated*/                 \
                                                \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)  \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
    UPDATE_MASKED_C_2_BZ

#define STORE_ROWSTORED_C_17xN_B0\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    UPDATE_MASKED_C_8_BZ                            \
    /*First 8x7 tile updated*/                      \
                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)            \
                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)          \
                                                    \
    UPDATE_MASKED_C_8_BZ                            \
    /*Second 8x7 tile updated*/                     \
                                                    \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)      \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)            \
                                                    \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)          \
    UPDATE_MASKED_C_1_BZ

#define STORE_ROWSTORED_C_16xN_B0\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    UPDATE_MASKED_C_8_BZ                            \
    /*First 8x7 tile updated*/                      \
                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)            \
                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)          \
                                                    \
    UPDATE_MASKED_C_8_BZ

#define STORE_ROWSTORED_C_24MASKxN_B0\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    UPDATE_MASKED_C_8_BZ                            \
                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)            \
                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)          \
                                                    \
    UPDATE_MASKED_C_8_BZ                            \
                                                    \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)      \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)            \
                                                    \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)          \
                                                    \
    mov(var(m_left), rdi)                           \
    sub(imm(16), rdi)                               \
    cmp(imm(7), rdi)                                \
    JZ(.UPDATE24MASK7B0)                            \
    cmp(imm(6), rdi)                                \
    JZ(.UPDATE24MASK6B0)                            \
    cmp(imm(5), rdi)                                \
    JZ(.UPDATE24MASK5B0)                            \
    cmp(imm(4), rdi)                                \
    JZ(.UPDATE24MASK4B0)                            \
    cmp(imm(3), rdi)                                \
    JZ(.UPDATE24MASK3B0)                            \
    cmp(imm(2), rdi)                                \
    JZ(.UPDATE24MASK2B0)                            \
    cmp(imm(1), rdi)                                \
    JZ(.UPDATE24MASK1B0)                            \
    cmp(imm(0), rdi)                                \
    JZ(.UPDATE24MASK0B0)                            \
    LABEL(.UPDATE24MASK7B0)                         \
    UPDATE_MASKED_C_7_BZ                            \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE24MASK6B0)                         \
    UPDATE_MASKED_C_6_BZ                            \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE24MASK5B0)                         \
    UPDATE_MASKED_C_5_BZ                            \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE24MASK4B0)                         \
    UPDATE_MASKED_C_4_BZ                            \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE24MASK3B0)                         \
    UPDATE_MASKED_C_3_BZ                            \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE24MASK2B0)                         \
    UPDATE_MASKED_C_2_BZ                            \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE24MASK1B0)                         \
    UPDATE_MASKED_C_1_BZ                            \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE24MASK0B0)

#define STORE_ROWSTORED_C_12xN_B0\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    UPDATE_MASKED_C_8_BZ                            \
    /*First 8x7 tile updated*/                      \
                                                    \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)            \
                                                    \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)          \
                                                    \
    UPDATE_MASKED_C_4_BZ

#define STORE_ROWSTORED_C_11xN_B0\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    UPDATE_MASKED_C_8_BZ                        \
    /*First 8x7 tile updated*/                  \
                                                \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
                                                \
    UPDATE_MASKED_C_2_BZ                        \
    lea(mem(   , rsi, 2), r14)                  \
    add(r14, rcx)                               \
    /*Second 8x7 tile updated*/                 \
                                                \
    UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)  \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
    UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
    UPDATE_MASKED_C_1_BZ

#define STORE_ROWSTORED_C_10xN_B0\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    UPDATE_MASKED_C_8_BZ                        \
    /*First 8x7 tile updated*/                  \
                                                \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
                                                \
    UPDATE_MASKED_C_2_BZ

#define STORE_ROWSTORED_C_9xN_B0\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    UPDATE_MASKED_C_8_BZ                        \
    /*First 8x7 tile updated*/                  \
                                                \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
                                                \
    UPDATE_MASKED_C_1_BZ

#define STORE_ROWSTORED_C_8xN_B0\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    UPDATE_MASKED_C_8_BZ

#define STORE_ROWSTORED_C_16MASKxN_B0\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    UPDATE_MASKED_C_8_BZ                        \
                                                \
    UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)        \
                                                \
    UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)      \
                                                \
    mov(var(m_left), rdi)                       \
    sub(imm(8), rdi)                            \
    cmp(imm(7), rdi)                            \
    JZ(.UPDATE16MASK7B0)                        \
    cmp(imm(6), rdi)                            \
    JZ(.UPDATE16MASK6B0)                        \
    cmp(imm(5), rdi)                            \
    JZ(.UPDATE16MASK5B0)                        \
    cmp(imm(4), rdi)                            \
    JZ(.UPDATE16MASK4B0)                        \
    cmp(imm(3), rdi)                            \
    JZ(.UPDATE16MASK3B0)                        \
    cmp(imm(2), rdi)                            \
    JZ(.UPDATE16MASK2B0)                        \
    cmp(imm(1), rdi)                            \
    JZ(.UPDATE16MASK1B0)                        \
    cmp(imm(0), rdi)                            \
    JZ(.UPDATE16MASK0B0)                        \
                                                \
    LABEL(.UPDATE16MASK7B0)                     \
    UPDATE_MASKED_C_7_BZ                        \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK6B0)                     \
    UPDATE_MASKED_C_6_BZ                        \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK5B0)                     \
    UPDATE_MASKED_C_5_BZ                        \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK4B0)                     \
    UPDATE_MASKED_C_4_BZ                        \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK3B0)                     \
    UPDATE_MASKED_C_3_BZ                        \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK2B0)                     \
    UPDATE_MASKED_C_2_BZ                        \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK1B0)                     \
    UPDATE_MASKED_C_1_BZ                        \
    jmp(.CONCLUDE)                              \
    LABEL(.UPDATE16MASK0B0)                     \
    jmp(.CONCLUDE)

#define STORE_ROWSTORED_C_4xN_B0\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    UPDATE_MASKED_C_4_BZ

#define STORE_ROWSTORED_C_2xN_B0\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    UPDATE_MASKED_C_2_BZ

#define STORE_ROWSTORED_C_1xN_B0\
    lea(mem(rsi,  rsi,  2), r12)                \
    lea(mem(r12, rsi,  2), r13)                 \
    lea(mem(r12, rsi,  4), rdx)                 \
    lea(mem(   , rsi, 8), r14)                  \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)    \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)      \
                                                \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)  \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)      \
                                                \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)        \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)    \
                                                \
    UPDATE_MASKED_C_1_BZ

#define STORE_ROWSTORED_C_8MASKxN_B0\
    lea(mem(rsi,  rsi,  2), r12)                    \
    lea(mem(r12, rsi,  2), r13)                     \
    lea(mem(r12, rsi,  4), rdx)                     \
    lea(mem(   , rsi, 8), r14)                      \
    UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)        \
    SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)          \
                                                    \
    UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)      \
    SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)          \
                                                    \
    SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)            \
    SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)        \
                                                    \
    mov(var(m_left), rdi)                           \
    cmp(imm(7), rdi)                                \
    JZ(.UPDATE7B0)                                  \
    cmp(imm(6), rdi)                                \
    JZ(.UPDATE6B0)                                  \
    cmp(imm(5), rdi)                                \
    JZ(.UPDATE5B0)                                  \
    cmp(imm(4), rdi)                                \
    JZ(.UPDATE4B0)                                  \
    cmp(imm(3), rdi)                                \
    JZ(.UPDATE3B0)                                  \
    cmp(imm(2), rdi)                                \
    JZ(.UPDATE2B0)                                  \
    cmp(imm(1), rdi)                                \
    JZ(.UPDATE1B0)                                  \
    cmp(imm(0), rdi)                                \
    JZ(.UPDATE0B0)                                  \
    LABEL(.UPDATE7B0)                               \
    UPDATE_MASKED_C_7_BZ                            \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE6B0)                               \
    UPDATE_MASKED_C_6_BZ                            \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE5B0)                               \
    UPDATE_MASKED_C_5_BZ                            \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE4B0)                               \
    UPDATE_MASKED_C_4_BZ                            \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE3B0)                               \
    UPDATE_MASKED_C_3_BZ                            \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE2B0)                               \
    UPDATE_MASKED_C_2_BZ                            \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE1B0)                               \
    UPDATE_MASKED_C_1_BZ                            \
    jmp(.CONCLUDE)                                  \
    LABEL(.UPDATE0B0)

/**************************END ROWSTORED C STORE Bn xN********************************/
#define K_LOOP_PREFETCH_A   \
    prefetch( 1,mem(r14) )  \
    prefetch( 1,0x40(r14) ) \
    prefetch( 1,0x80(r14) ) \
    add( r10,r14 )

/*********************Start of 24x8 kernel family K-LOOP macros*******************/

/**
 * K_LOOP_24x8
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9((b[0, 1])) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm28, store result in zmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm29, store result in zmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm26, store result in zmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm27, store result in zmm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm24, store result in zmm24
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm25, store result in zmm25
 *
 * Broadcast the scalar value at memory location r12 + r13(b[0, 7]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm19, store result in zmm19
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm22, store result in zmm22
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm20, store result in zmm20
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm21, store result in zmm21
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm23, store result in zmm23
 * Increment the second pointer of B by rs_b
 * Increment B by rs_b
 */
#define K_LOOP_24x8(s1, s2, s3)                 \
    vbroadcastsd( mem(rbx), zmm30 )             \
    vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
                                                \
    vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
    vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
    vfmadd231pd( zmm(s3), zmm30, zmm28 )        \
                                                \
    vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
                                                \
    vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
    vfmadd231pd( zmm(s2), zmm31, zmm9 )         \
    vfmadd231pd( zmm(s3), zmm31, zmm29 )        \
                                                \
    vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
                                                \
    vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
    vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
    vfmadd231pd( zmm(s3), zmm30, zmm26 )        \
                                                \
    vbroadcastsd( mem(r12), zmm30 )             \
                                                \
    vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
    vfmadd231pd( zmm(s2), zmm31, zmm13 )        \
    vfmadd231pd( zmm(s3), zmm31, zmm27 )        \
                                                \
    vbroadcastsd( mem(r12, r9, 1), zmm31 )      \
                                                \
    vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
    vfmadd231pd( zmm(s2), zmm30, zmm15 )        \
    vfmadd231pd( zmm(s3), zmm30, zmm24 )        \
                                                \
    vbroadcastsd( mem(r12, r9, 2), zmm30 )      \
                                                \
    vfmadd231pd( zmm(s1), zmm31, zmm16 )        \
    vfmadd231pd( zmm(s2), zmm31, zmm17 )        \
    vfmadd231pd( zmm(s3), zmm31, zmm25 )        \
                                                \
    vbroadcastsd( mem(r12, r13, 1), zmm31 )     \
                                                \
    vfmadd231pd( zmm(s1), zmm30, zmm18 )        \
    vfmadd231pd( zmm(s2), zmm30, zmm19 )        \
    vfmadd231pd( zmm(s3), zmm30, zmm22 )        \
                                                \
    vfmadd231pd( zmm(s1), zmm31, zmm20 )        \
    vfmadd231pd( zmm(s2), zmm31, zmm21 )        \
    vfmadd231pd( zmm(s3), zmm31, zmm23 )        \
    add( r8, r12 )                              \
    add( r8, rbx )

/**
 * K_LOOP_20x8(s1, s2, s3)
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: ymm(s3)(a[16:19]) * ymm30 + ymm28, store result in ymm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: ymm(s3)(a[16:19]) * ymm31 + ymm29, store result in ymm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: ymm(s3)(a[16:19]) * ymm30 + ymm26, store result in ymm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: ymm(s3)(a[16:19]) * ymm31 + ymm27, store result in ymm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: ymm(s3)(a[16:19]) * ymm30 + ymm24, store result in ymm24
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 * Fused multiply-add: ymm(s3)(a[16:19]) * ymm31 + ymm25, store result in ymm25
 *
 * Broadcast the scalar value at memory location r12 + r13(b[0, 7]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm19, store result in zmm19
 * Fused multiply-add: ymm(s3)(a[16:19]) * ymm30 + ymm22, store result in ymm22
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm20, store result in zmm20
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm21, store result in zmm21
 * Fused multiply-add: ymm(s3)(a[16:19]) * ymm31 + ymm23, store result in ymm23
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_20x8(s1, s2, s3)                 \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
                                                \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
	vfmadd231pd( ymm(s3), ymm30, ymm28 )        \
                                                \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
                                                \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )         \
	vfmadd231pd( ymm(s3), ymm31, ymm29 )        \
                                                \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
                                                \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
	vfmadd231pd( ymm(s3), ymm30, ymm26 )        \
                                                \
	vbroadcastsd( mem(r12), zmm30 )             \
                                                \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )        \
	vfmadd231pd( ymm(s3), ymm31, ymm27 )        \
                                                \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )      \
                                                \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )        \
	vfmadd231pd( ymm(s3), ymm30, ymm24 )        \
                                                \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )      \
                                                \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )        \
	vfmadd231pd( ymm(s3), ymm31, ymm25 )        \
                                                \
	vbroadcastsd( mem(r12, r13, 1), zmm31 )     \
                                                \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )        \
	vfmadd231pd( ymm(s3), ymm30, ymm22 )        \
                                                \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm21 )        \
	vfmadd231pd( ymm(s3), ymm31, ymm23 )        \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_18x8
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm29, store result in xmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm30 + xmm26, store result in xmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm27, store result in xmm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm24, store result in xmm24
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm25, store result in xmm25
 *
 * Broadcast the scalar value at memory location r12 + r13(b[0, 7]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm19, store result in zmm19
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm30 + xmm22, store result in xmm22
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm20, store result in zmm20
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm21, store result in zmm21
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm23, store result in xmm23
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_18x8(s1, s2, s3)             \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( xmm(s3), xmm30, xmm28 )    \
                                            \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( xmm(s3), xmm31, xmm29 )    \
                                            \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm26 )    \
                                            \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm27 )    \
                                            \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm24 )    \
                                            \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm25 )    \
	vbroadcastsd( mem(r12, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm22 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm21 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm23 )    \
	add( r8,rbx )                           \
	add( r8,r12 )

/**
 * K_LOOP_17x8
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm29, store result in xmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm30 + xmm26, store result in xmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm27, store result in xmm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm24, store result in xmm24
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm25, store result in xmm25
 *
 * Broadcast the scalar value at memory location r12 + r13(b[0, 7]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm19, store result in zmm19
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm30 + xmm22, store result in xmm22
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm20, store result in zmm20
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm21, store result in zmm21
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm23, store result in xmm23
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_17x8(s1, s2, s3)             \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( xmm(s3), xmm30, xmm28 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( xmm(s3), xmm31, xmm29 )    \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm26 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm27 )    \
	vbroadcastsd( mem(r12, r9,1), zmm31 )   \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm24 )    \
	vbroadcastsd( mem(r12, r9,2), zmm30 )   \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm25 )    \
	vbroadcastsd( mem(r12, r13,1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm22 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm21 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm23 )    \
	add( r8,rbx )                           \
	add( r8,r12 )

/**
 * K_LOOP_16x8
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 *
 * Broadcast the scalar value at memory location r12 + r13(b[0, 7]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm19, store result in zmm19
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm20, store result in zmm20
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm21, store result in zmm21
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_16x8(s1, s2)                 \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )    \
	vbroadcastsd( mem(r12, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm21 )    \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_24MASKx8
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm28, store result in zmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm29, store result in zmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm26, store result in zmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: zmm(s3)(a[16:32]) * zmm31 + zmm27, store result in zmm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm24, store result in zmm24
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm25, store result in zmm25
 *
 * Broadcast the scalar value at memory location r12 + r13(b[0, 7]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm19, store result in zmm19
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm22, store result in zmm22
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm20, store result in zmm20
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm21, store result in zmm21
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm23, store result in zmm23
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_24MASKx8(s1, s2, s3)         \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( zmm(s3), zmm30, zmm28 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( zmm(s3), zmm31, zmm29 )    \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( zmm(s3), zmm30, zmm26 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	vfmadd231pd( zmm(s3), zmm31, zmm27 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vfmadd231pd( zmm(s3), zmm30, zmm24 )    \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )    \
	vfmadd231pd( zmm(s3), zmm31, zmm25 )    \
	vbroadcastsd( mem(r12, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )    \
	vfmadd231pd( zmm(s3), zmm30, zmm22 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm21 )    \
	vfmadd231pd( zmm(s3), zmm31, zmm23 )    \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_12x8
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm30 + ymm7, store result in ymm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm31 + ymm9, store result in ymm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm30 + ymm11, store result in ymm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm31 + ymm13, store result in ymm13
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: ymm(s2) (a[8:11])* ymm30 + ymm15, store result in ymm15
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm31 + ymm17, store result in ymm17
 *
 * Broadcast the scalar value at memory location r12 + r13(b[0, 7]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm30 + ymm19, store result in ymm19
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm20, store result in zmm20
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm31 + ymm21, store result in ymm21
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_12x8(s1, s2)                 \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( ymm(s2), ymm30, ymm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( ymm(s2), ymm31, ymm9 )     \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( ymm(s2), ymm30, ymm11 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( ymm(s2), ymm31, ymm13 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( ymm(s2), ymm30, ymm15 )    \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( ymm(s2), ymm31, ymm17 )    \
	vbroadcastsd( mem(r12, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( ymm(s2), ymm30, ymm19 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )    \
	vfmadd231pd( ymm(s2), ymm31, ymm21 )    \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_11x8
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm1 * xmm30 + xmm7, store result in xmm7
 * Fused multiply-add: xmm2 * xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm0 * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm1 * xmm31 + xmm9, store result in xmm9
 * Fused multiply-add: xmm2 * xmm31 + xmm29, store result in xmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm1 * xmm30 + xmm11, store result in xmm11
 * Fused multiply-add: xmm2 * xmm30 + xmm26, store result in xmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm0 * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: xmm1 * xmm31 + xmm13, store result in xmm13
 * Fused multiply-add: xmm2 * xmm31 + xmm27, store result in xmm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: xmm1 * xmm30 + xmm15, store result in xmm15
 * Fused multiply-add: xmm2 * xmm30 + xmm24, store result in xmm24
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm0 * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: xmm1 * xmm31 + xmm17, store result in xmm17
 * Fused multiply-add: xmm2 * xmm31 + xmm25, store result in xmm25
 *
 * Broadcast the scalar value at memory location r12 + r13(b[0, 7]) to all elements of zmm31
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: xmm1 * xmm30 + xmm19, store result in xmm19
 * Fused multiply-add: xmm2 * xmm30 + xmm22, store result in xmm22
 *
 * Fused multiply-add: zmm0 * zmm31 + zmm20, store result in zmm20
 * Fused multiply-add: xmm1 * xmm31 + xmm21, store result in xmm21
 * Fused multiply-add: xmm2 * xmm31 + xmm23, store result in xmm23
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_11x8(s1, s2, s3)             \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm0, zmm30, zmm6 )        \
	vfmadd231pd( xmm1, xmm30, xmm7 )        \
	vfmadd231pd( xmm2, xmm30, xmm28 )       \
                                            \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm0, zmm31, zmm8 )        \
	vfmadd231pd( xmm1, xmm31, xmm9 )        \
	vfmadd231pd( xmm2, xmm31, xmm29 )       \
                                            \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm0, zmm30, zmm10 )       \
	vfmadd231pd( xmm1, xmm30, xmm11 )       \
	vfmadd231pd( xmm2, xmm30, xmm26 )       \
                                            \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm0, zmm31, zmm12 )       \
	vfmadd231pd( xmm1, xmm31, xmm13 )       \
	vfmadd231pd( xmm2, xmm31, xmm27 )       \
                                            \
	vbroadcastsd( mem(r12, r9, 1),zmm31 )   \
	vfmadd231pd( zmm0, zmm30, zmm14 )       \
	vfmadd231pd( xmm1, xmm30, xmm15 )       \
	vfmadd231pd( xmm2, xmm30, xmm24 )       \
                                            \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )  \
	vfmadd231pd( zmm0, zmm31, zmm16 )       \
	vfmadd231pd( xmm1, xmm31, xmm17 )       \
	vfmadd231pd( xmm2, xmm31, xmm25 )       \
                                            \
	vbroadcastsd( mem(r12, r13, 1), zmm31 ) \
	vfmadd231pd( zmm0, zmm30, zmm18 )       \
	vfmadd231pd( xmm1, xmm30, xmm19 )       \
	vfmadd231pd( xmm2, xmm30, xmm22 )       \
	vfmadd231pd( zmm0, zmm31, zmm20 )       \
	vfmadd231pd( xmm1, xmm31, xmm21 )       \
	vfmadd231pd( xmm2, xmm31, xmm23 )       \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_10x8
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm(s2) * xmm30 + xmm7, store result in xmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm(s2) * xmm31 + xmm9, store result in xmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm(s2) * xmm30 + xmm11, store result in xmm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: xmm(s2) * xmm31 + xmm13, store result in xmm13
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: xmm(s2) * xmm30 + xmm15, store result in xmm15
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: xmm(s2) * xmm31 + xmm17, store result in xmm17
 *
 * Broadcast the scalar value at memory location r12 + r13(b[0, 7]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: xmm(s2) * xmm30 + xmm19, store result in xmm19
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm20, store result in zmm20
 * Fused multiply-add: xmm(s2) * xmm31 + xmm21, store result in xmm21
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_10x8(s1, s2)                 \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( xmm(s2), xmm30, xmm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( xmm(s2), xmm31, xmm9 )     \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm11 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm13 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm15 )    \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm17 )    \
	vbroadcastsd( mem(r12, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm19 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm21 )    \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_9x8
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm(s2) * xmm30 + xmm7, store result in xmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm(s2) * xmm31 + xmm9, store result in xmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm(s2) * xmm30 + xmm11, store result in xmm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: xmm(s2) * xmm31 + xmm13, store result in xmm13
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: xmm(s2) * xmm30 + xmm15, store result in xmm15
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: xmm(s2) * xmm31 + xmm17, store result in xmm17
 *
 * Broadcast the scalar value at memory location r12 + r13(b[0, 7]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: xmm(s2) * xmm30 + xmm19, store result in xmm19
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm20, store result in zmm20
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm21, store result in zmm21
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_9x8(s1, s2)                  \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( xmm(s2), xmm30, xmm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( xmm(s2), xmm31, xmm9 )     \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm11 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm13 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm15 )    \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm17 )    \
	vbroadcastsd( mem(r12, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm19 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm21 )    \
	add( r8,rbx )                           \
	add( r8,r12 )

/**
 * K_LOOP_8x8_SET1
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx) + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r9) + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + 2*r9) + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r13) + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s1) * mem_1to8(r12) + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s1) * mem_1to8(r12 + r9) + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s1) * mem_1to8(r12 + 2*r9) + zmm18, store result in zmm18
 * Fused multiply-add: zmm(s1) * mem_1to8(r12 + r13) + zmm20, store result in zmm20
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_8x8_SET1(s1)                                 \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm6 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm8 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm10 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm12 )    \
	vfmadd231pd( mem_1to8(r12), zmm(s1), zmm14 )            \
	vfmadd231pd( mem_1to8(r12, r9, 1), zmm(s1), zmm16 )     \
	vfmadd231pd( mem_1to8(r12, r9, 2), zmm(s1), zmm18 )     \
	vfmadd231pd( mem_1to8(r12, r13, 1), zmm(s1), zmm20 )    \
	add( r8, rbx )                                          \
	add( r8, r12 )

#define K_LOOP_8x8_SET2(s1)                                 \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm7 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm9 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm11 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm13 )    \
	vfmadd231pd( mem_1to8(r12),zmm(s1), zmm15 )             \
	vfmadd231pd( mem_1to8(r12, r9, 1), zmm(s1), zmm17 )     \
	vfmadd231pd( mem_1to8(r12, r9, 2), zmm(s1), zmm19 )     \
	vfmadd231pd( mem_1to8(r12, r13, 1), zmm(s1), zmm21 )    \
	add( r8, rbx )                                          \
	add( r8, r12 )

/**
 * K_LOOP_16MASKx8
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 *
 * Broadcast the scalar value at memory location r12 + r13(b[0, 7]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm19, store result in zmm19
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm20, store result in zmm20
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm21, store result in zmm21
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_16MASKx8(s1, s2)             \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )    \
	vbroadcastsd( mem(r12, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm21 )    \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_4x8_SET1
 * Broadcast the scalar value at memory location rbx to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm6, store result in ymm6
 * Broadcast the scalar value at memory location rbx + r9 to all elements of ymm31
 * Fused multiply-add: ymm(s1) * ymm31 + ymm8, store result in ymm8
 * Broadcast the scalar value at memory location rbx + 2*r9 to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm10, store result in ymm10
 * Broadcast the scalar value at memory location rbx + r13 to all elements of ymm31
 * Fused multiply-add: ymm(s1) * ymm31 + ymm12, store result in ymm12
 * Broadcast the scalar value at memory location r12 to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm14, store result in ymm14
 * Broadcast the scalar value at memory location r12 + r9 to all elements of ymm31
 * Fused multiply-add: ymm(s1) * ymm31 + ymm16, store result in ymm16
 * Broadcast the scalar value at memory location r12 + 2*r9 to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm18, store result in ymm18
 * Broadcast the scalar value at memory location r12 + r13 to all elements of ymm31
 * Fused multiply-add: ymm(s1) * ymm31 + ymm20, store result in ymm20
 * Increment the second pointer of B by rs_b
 * Increment B by rs_b
 */
#define K_LOOP_4x8_SET1(s1)                 \
	vbroadcastsd( mem(rbx), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm6 )     \
	vbroadcastsd( mem(rbx, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm8 )     \
	vbroadcastsd( mem(rbx, r9, 2), ymm30 )  \
	vfmadd231pd( ymm(s1), ymm30, ymm10 )    \
	vbroadcastsd( mem(rbx, r13, 1), ymm31 ) \
	vfmadd231pd( ymm(s1), ymm31, ymm12 )    \
	vbroadcastsd( mem(r12), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm14 )    \
	vbroadcastsd( mem(r12, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm16 )    \
	vbroadcastsd( mem(r12, r9, 2), ymm30 )  \
	vfmadd231pd( ymm(s1), ymm30, ymm18 )    \
	vbroadcastsd( mem(r12, r13, 1), ymm31 ) \
	vfmadd231pd( ymm(s1), ymm31, ymm20 )    \
	add( r8, r12 )                          \
	add( r8, rbx )

#define K_LOOP_4x8_SET2(s1)                 \
	vbroadcastsd( mem(rbx), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm7 )     \
	vbroadcastsd( mem(rbx, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm9 )     \
	vbroadcastsd( mem(rbx, r9, 2), ymm30 )  \
	vfmadd231pd( ymm(s1), ymm30, ymm11 )    \
	vbroadcastsd( mem(rbx, r13, 1), ymm31 ) \
	vfmadd231pd( ymm(s1), ymm31, ymm13 )    \
	vbroadcastsd( mem(r12), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm15 )    \
	vbroadcastsd( mem(r12, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm17 )    \
	vbroadcastsd( mem(r12, r9, 2), ymm30 )  \
	vfmadd231pd( ymm(s1), ymm30, ymm19 )    \
	vbroadcastsd( mem(r12, r13, 1), ymm31 ) \
	vfmadd231pd( ymm(s1), ymm31, ymm21 )    \
	add( r8, r12 )                          \
	add( r8, rbx )

/**
 * K_LOOP_2x8_SET1
 * Move and duplicate the scalar value at memory location rbx to all elements of xmm30
 * Move and duplicate the scalar value at memory location rbx + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm6, store result in xmm6
 * Move and duplicate the scalar value at memory location rbx + 2*r9 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm8, store result in xmm8
 * Move and duplicate the scalar value at memory location rbx + r13 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm10, store result in xmm10
 * Move and duplicate the scalar value at memory location r12 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm12, store result in xmm12
 * Move and duplicate the scalar value at memory location r12 + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm14, store result in xmm14
 * Move and duplicate the scalar value at memory location r12 + 2*r9 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm16, store result in xmm16
 * Broadcast the scalar value at memory location r12 + r13(b[0, 7]) to all elements of zmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm18, store result in xmm18
 * Fused multiply-add: xmm(s1) * xmm31 + xmm20, store result in xmm20
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_2x8_SET1(s1)                 \
	vmovddup ( mem(rbx), xmm30 )            \
	vmovddup ( mem(rbx, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm6 )     \
	vmovddup ( mem(rbx, r9, 2), xmm30 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm8 )     \
	vmovddup ( mem(rbx, r13, 1), xmm31 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm10 )    \
	vmovddup ( mem(r12), xmm30 )            \
	vfmadd231pd( xmm(s1), xmm31, xmm12 )    \
	vmovddup ( mem(r12, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm14 )    \
	vmovddup ( mem(r12, r9, 2), xmm30 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm16 )    \
	vbroadcastsd( mem(r12, r13, 1), zmm31 ) \
	vfmadd231pd( xmm(s1), xmm30, xmm18 )    \
	vfmadd231pd( xmm(s1), xmm31, xmm20 )    \
	add( r8, rbx )                          \
	add( r8, r12 )

#define K_LOOP_2x8_SET2(s1)                 \
	vmovddup ( mem(rbx), xmm30 )            \
	vmovddup ( mem(rbx, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm7 )     \
	vmovddup ( mem(rbx, r9, 2), xmm30 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm9 )     \
	vmovddup ( mem(rbx, r13, 1), xmm31 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm11 )    \
	vmovddup ( mem(r12), xmm30 )            \
	vfmadd231pd( xmm(s1), xmm31, xmm13 )    \
	vmovddup ( mem(r12, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm15 )    \
	vmovddup ( mem(r12, r9, 2), xmm30 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm17 )    \
	vbroadcastsd( mem(r12, r13, 1), zmm31 ) \
	vfmadd231pd( xmm(s1), xmm30, xmm19 )    \
	vfmadd231pd( xmm(s1), xmm31, xmm21 )    \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_1x8_SET1
 * Move and duplicate the scalar value at memory location rbx to all elements of xmm30
 * Move and duplicate the scalar value at memory location rbx + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm6, store result in xmm6
 * Move and duplicate the scalar value at memory location rbx + 2*r9 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm8, store result in xmm8
 * Move and duplicate the scalar value at memory location rbx + r13 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm10, store result in xmm10
 * Move and duplicate the scalar value at memory location r12 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm12, store result in xmm12
 * Move and duplicate the scalar value at memory location r12 + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm14, store result in xmm14
 * Move and duplicate the scalar value at memory location r12 + 2*r9 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm16, store result in xmm16
 * Move and duplicate the scalar value at memory location r12 + r13 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm18, store result in xmm18
 * Fused multiply-add: xmm(s1) * xmm31 + xmm20, store result in xmm20
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_1x8_SET1(s1)                 \
	vmovddup( mem(rbx), xmm30 )             \
	vmovddup( mem(rbx, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm6 )     \
	vmovddup( mem(rbx, r9, 2), xmm30 )      \
	vfmadd231pd( xmm(s1), xmm31, xmm8 )     \
	vmovddup( mem(rbx, r13, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm10 )    \
	vmovddup( mem(r12), xmm30 )             \
	vfmadd231pd( xmm(s1), xmm31, xmm12 )    \
	vmovddup( mem(r12, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm14 )    \
	vmovddup( mem(r12, r9, 2), xmm30 )      \
	vfmadd231pd( xmm(s1), xmm31, xmm16 )    \
	vmovddup( mem(r12, r13, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm18 )    \
	vfmadd231pd( xmm(s1), xmm31, xmm20 )    \
	add( r8, rbx )                          \
	add( r8, r12 )

#define K_LOOP_1x8_SET2(s1)                 \
	vmovddup( mem(rbx), xmm30 )             \
	vmovddup( mem(rbx, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm7 )     \
	vmovddup( mem(rbx, r9, 2), xmm30 )      \
	vfmadd231pd( xmm(s1), xmm31, xmm9 )     \
	vmovddup( mem(rbx, r13, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm11 )    \
	vmovddup( mem(r12), xmm30 )             \
	vfmadd231pd( xmm(s1), xmm31, xmm13 )    \
	vmovddup( mem(r12, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm15 )    \
	vmovddup( mem(r12, r9, 2), xmm30 )      \
	vfmadd231pd( xmm(s1), xmm31, xmm17 )    \
	vmovddup( mem(r12, r13, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm19 )    \
	vfmadd231pd( xmm(s1), xmm31, xmm21 )    \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_8MASKx8_SET1
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx) + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r9) + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + 2*r9) + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r13) + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s1) * mem_1to8(r12) + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s1) * mem_1to8(r12 + r9) + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s1) * mem_1to8(r12 + 2*r9) + zmm18, store result in zmm18
 * Fused multiply-add: zmm(s1) * mem_1to8(r12 + r13) + zmm20, store result in zmm20
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_8MASKx8_SET1(s1)                             \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm6 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm8 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm10 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm12 )    \
	vfmadd231pd( mem_1to8(r12), zmm(s1), zmm14 )            \
	vfmadd231pd( mem_1to8(r12, r9, 1), zmm(s1), zmm16 )     \
	vfmadd231pd( mem_1to8(r12, r9, 2), zmm(s1), zmm18 )     \
	vfmadd231pd( mem_1to8(r12, r13, 1), zmm(s1), zmm20 )    \
	add( r8, rbx )\
	add( r8, r12 )

#define K_LOOP_8MASKx8_SET2(s1)                             \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm7 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm9 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm11 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm13 )    \
	vfmadd231pd( mem_1to8(r12),zmm(s1), zmm15 )             \
	vfmadd231pd( mem_1to8(r12, r9, 1), zmm(s1), zmm17 )     \
	vfmadd231pd( mem_1to8(r12, r9, 2), zmm(s1), zmm19 )     \
	vfmadd231pd( mem_1to8(r12, r13, 1), zmm(s1), zmm21 )    \
	add( r8, rbx )                                          \
	add( r8, r12 )
/*********************End of 24x8 kernel family K-LOOP macros*******************/

/*********************Start of 24x7 kernel family K-LOOP macros*******************/
/**
 * K_LOOP_24x7
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm28, store result in zmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm29, store result in zmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm26, store result in zmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: zmm(s3)(a[16:32]) * zmm31 + zmm27, store result in zmm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm24, store result in zmm24
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm25, store result in zmm25
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm19, store result in zmm19
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm22, store result in zmm22
 * Increment the second pointer of B by rs_b
 * Increment B by rs_b
 */
#define K_LOOP_24x7(s1, s2, s3)                 \
    vbroadcastsd( mem(rbx), zmm30 )             \
    vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
    vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
    vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
    vfmadd231pd( zmm(s3), zmm30, zmm28 )        \
    vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
    vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
    vfmadd231pd( zmm(s2), zmm31, zmm9 )         \
    vfmadd231pd( zmm(s3), zmm31, zmm29 )        \
    vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
    vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
    vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
    vfmadd231pd( zmm(s3), zmm30, zmm26 )        \
    vbroadcastsd( mem(r12), zmm30 )             \
    vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
    vfmadd231pd( zmm(s2), zmm31, zmm13 )        \
    vfmadd231pd( zmm(s3), zmm31, zmm27 )        \
    vbroadcastsd( mem(r12, r9, 1), zmm31 )      \
    vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
    vfmadd231pd( zmm(s2), zmm30, zmm15 )        \
    vfmadd231pd( zmm(s3), zmm30, zmm24 )        \
    vbroadcastsd( mem(r12, r9, 2), zmm30 )      \
    vfmadd231pd( zmm(s1), zmm31, zmm16 )        \
    vfmadd231pd( zmm(s2), zmm31, zmm17 )        \
    vfmadd231pd( zmm(s3), zmm31, zmm25 )        \
    vfmadd231pd( zmm(s1), zmm30, zmm18 )        \
    vfmadd231pd( zmm(s2), zmm30, zmm19 )        \
    vfmadd231pd( zmm(s3), zmm30, zmm22 )        \
    add( r8, r12 )                              \
    add( r8, rbx )

/**
 * K_LOOP_20x7
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: ymm(s3) * ymm30 + ymm28, store result in ymm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: ymm(s3) * ymm31 + ymm29, store result in ymm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: ymm(s3) * ymm30 + ymm26, store result in ymm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: ymm(s3) * ymm31 + ymm27, store result in ymm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: ymm(s3) * ymm30 + ymm24, store result in ymm24
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 * Fused multiply-add: ymm(s3) * ymm31 + ymm25, store result in ymm25
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm19, store result in zmm19
 * Fused multiply-add: ymm(s3) * ymm30 + ymm22, store result in ymm22
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_20x7(s1, s2, s3)                 \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
	vfmadd231pd( ymm(s3), ymm30, ymm28 )        \
                                                \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )         \
	vfmadd231pd( ymm(s3), ymm31, ymm29 )        \
                                                \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
	vfmadd231pd( ymm(s3), ymm30, ymm26 )        \
                                                \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )        \
	vfmadd231pd( ymm(s3), ymm31, ymm27 )        \
                                                \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )        \
	vfmadd231pd( ymm(s3), ymm30, ymm24 )        \
                                                \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )        \
	vfmadd231pd( ymm(s3), ymm31, ymm25 )        \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )        \
	vfmadd231pd( ymm(s3), ymm30, ymm22 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_18x7
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm29, store result in xmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm30 + xmm26, store result in xmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm27, store result in xmm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm24, store result in xmm24
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm25, store result in xmm25
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm19, store result in zmm19
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm30 + xmm22, store result in xmm22
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_18x7(s1, s2, s3)             \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( xmm(s3), xmm30, xmm28 )    \
                                            \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( xmm(s3), xmm31, xmm29 )    \
                                            \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm26 )    \
                                            \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm27 )    \
                                            \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm24 )    \
                                            \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm25 )    \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm22 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_17x7
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm29, store result in xmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm30 + xmm26, store result in xmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm27, store result in xmm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm24, store result in xmm24
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm25, store result in xmm25
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm19, store result in zmm19
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm30 + xmm22, store result in xmm22
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_17x7(s1, s2, s3)             \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( xmm(s3), xmm30, xmm28 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( xmm(s3), xmm31, xmm29 )    \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm26 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm27 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm24 )    \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm25 )    \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm22 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_16x7
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm19, store result in zmm19
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_16x7(s1, s2)                 \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )    \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_24MASKx8
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm28, store result in zmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm29, store result in zmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm26, store result in zmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: zmm(s3)(a[16:32]) * zmm31 + zmm27, store result in zmm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm24, store result in zmm24
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm25, store result in zmm25
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm19, store result in zmm19
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm22, store result in zmm22
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_24MASKx7(s1, s2, s3)         \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( zmm(s3), zmm30, zmm28 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( zmm(s3), zmm31, zmm29 )    \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( zmm(s3), zmm30, zmm26 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	vfmadd231pd( zmm(s3), zmm31, zmm27 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vfmadd231pd( zmm(s3), zmm30, zmm24 )    \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )    \
	vfmadd231pd( zmm(s3), zmm31, zmm25 )    \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )    \
	vfmadd231pd( zmm(s3), zmm30, zmm22 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_12x7
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm30 + ymm7, store result in ymm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm31 + ymm9, store result in ymm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm30 + ymm11, store result in ymm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm31 + ymm13, store result in ymm13
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: ymm(s2) (a[8:11])* ymm30 + ymm15, store result in ymm15
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm31 + ymm17, store result in ymm17
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm30 + ymm19, store result in ymm19
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_12x7(s1, s2)                 \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( ymm(s2), ymm30, ymm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( ymm(s2), ymm31, ymm9 )     \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( ymm(s2), ymm30, ymm11 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( ymm(s2), ymm31, ymm13 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( ymm(s2), ymm30, ymm15 )    \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( ymm(s2), ymm31, ymm17 )    \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( ymm(s2), ymm30, ymm19 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_11x8
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm1 * xmm30 + xmm7, store result in xmm7
 * Fused multiply-add: xmm2 * xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm0 * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm1 * xmm31 + xmm9, store result in xmm9
 * Fused multiply-add: xmm2 * xmm31 + xmm29, store result in xmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm1 * xmm30 + xmm11, store result in xmm11
 * Fused multiply-add: xmm2 * xmm30 + xmm26, store result in xmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm0 * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: xmm1 * xmm31 + xmm13, store result in xmm13
 * Fused multiply-add: xmm2 * xmm31 + xmm27, store result in xmm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: xmm1 * xmm30 + xmm15, store result in xmm15
 * Fused multiply-add: xmm2 * xmm30 + xmm24, store result in xmm24
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm0 * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: xmm1 * xmm31 + xmm17, store result in xmm17
 * Fused multiply-add: xmm2 * xmm31 + xmm25, store result in xmm25
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: xmm1 * xmm30 + xmm19, store result in xmm19
 * Fused multiply-add: xmm2 * xmm30 + xmm22, store result in xmm22
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_11x7(s1, s2, s3)             \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm0, zmm30, zmm6 )        \
	vfmadd231pd( xmm1, xmm30, xmm7 )        \
	vfmadd231pd( xmm2, xmm30, xmm28 )       \
                                            \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm0, zmm31, zmm8 )        \
	vfmadd231pd( xmm1, xmm31, xmm9 )        \
	vfmadd231pd( xmm2, xmm31, xmm29 )       \
                                            \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm0, zmm30, zmm10 )       \
	vfmadd231pd( xmm1, xmm30, xmm11 )       \
	vfmadd231pd( xmm2, xmm30, xmm26 )       \
                                            \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm0, zmm31,zmm12 )        \
	vfmadd231pd( xmm1, xmm31,xmm13 )        \
	vfmadd231pd( xmm2, xmm31,xmm27 )        \
                                            \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm0, zmm30, zmm14 )       \
	vfmadd231pd( xmm1, xmm30, xmm15 )       \
	vfmadd231pd( xmm2, xmm30, xmm24 )       \
                                            \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )  \
	vfmadd231pd( zmm0, zmm31, zmm16 )       \
	vfmadd231pd( xmm1, xmm31, xmm17 )       \
	vfmadd231pd( xmm2, xmm31, xmm25 )       \
                                            \
	vfmadd231pd( zmm0, zmm30, zmm18 )       \
	vfmadd231pd( xmm1, xmm30, xmm19 )       \
	vfmadd231pd( xmm2, xmm30, xmm22 )       \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_10x7
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm(s2) * xmm30 + xmm7, store result in xmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm(s2) * xmm31 + xmm9, store result in xmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm(s2) * xmm30 + xmm11, store result in xmm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: xmm(s2) * xmm31 + xmm13, store result in xmm13
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: xmm(s2) * xmm30 + xmm15, store result in xmm15
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: xmm(s2) * xmm31 + xmm17, store result in xmm17
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: xmm(s2) * xmm30 + xmm19, store result in xmm19
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_10x7(s1, s2)                 \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( xmm(s2), xmm30, xmm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( xmm(s2), xmm31, xmm9 )     \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm11 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm13 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm15 )    \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm17 )    \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm19 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_9x7
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm(s2) * xmm30 + xmm7, store result in xmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm(s2) * xmm31 + xmm9, store result in xmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm(s2) * xmm30 + xmm11, store result in xmm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: xmm(s2) * xmm31 + xmm13, store result in xmm13
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: xmm(s2) * xmm30 + xmm15, store result in xmm15
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: xmm(s2) * xmm31 + xmm17, store result in xmm17
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: xmm(s2) * xmm30 + xmm19, store result in xmm19
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_9x7(s1, s2)                  \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( xmm(s2), xmm30, xmm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( xmm(s2), xmm31, xmm9 )     \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm11 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm13 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm15 )    \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm17 )    \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm19 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_8x7_SET1
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx) + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r9) + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + 2*r9) + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r13) + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s1) * mem_1to8(r12) + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s1) * mem_1to8(r12 + r9) + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s1) * mem_1to8(r12 + 2*r9) + zmm18, store result in zmm18
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_8x7_SET1(s1)                                 \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm6 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm8 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm10 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm12 )    \
	vfmadd231pd( mem_1to8(r12), zmm(s1), zmm14 )            \
	vfmadd231pd( mem_1to8(r12, r9, 1), zmm(s1), zmm16 )     \
	vfmadd231pd( mem_1to8(r12, r9, 2), zmm(s1), zmm18 )     \
	/*second pointer of b += rs_b*/                         \
	add( r8, rbx )                                          \
	add( r8, r12 )

#define K_LOOP_8x7_SET2(s1)                                 \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm7 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm9 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm11 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm13 )    \
	vfmadd231pd( mem_1to8(r12), zmm(s1), zmm15 )            \
	vfmadd231pd( mem_1to8(r12, r9, 1), zmm(s1), zmm17 )     \
	vfmadd231pd( mem_1to8(r12, r9, 2), zmm(s1), zmm19 )     \
	/*second pointer of b += rs_b*/                         \
	add( r8, rbx )                                          \
	add( r8, r12 )

/**
 * K_LOOP_16MASKx7
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 *
 * Broadcast the scalar value at memory location r12 + 2*r9(b[0, 6]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm18, store result in zmm18
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm19, store result in zmm19
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_16MASKx7(s1, s2)             \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vbroadcastsd( mem(r12, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )    \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_4x7_SET1
 * Broadcast the scalar value at memory location rbx to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm6, store result in ymm6
 * Broadcast the scalar value at memory location rbx + r9 to all elements of ymm31
 * Fused multiply-add: ymm(s1) * ymm31 + ymm8, store result in ymm8
 * Broadcast the scalar value at memory location rbx + 2*r9 to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm10, store result in ymm10
 * Broadcast the scalar value at memory location rbx + r13 to all elements of ymm31
 * Fused multiply-add: ymm(s1) * ymm31 + ymm12, store result in ymm12
 * Broadcast the scalar value at memory location r12 to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm14, store result in ymm14
 * Broadcast the scalar value at memory location r12 + r9 to all elements of ymm31
 * Fused multiply-add: ymm(s1) * ymm31 + ymm16, store result in ymm16
 * Broadcast the scalar value at memory location r12 + 2*r9 to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm18, store result in ymm18
 * Increment the second pointer of B by rs_b
 * Increment B by rs_b
 */
#define K_LOOP_4x7_SET1(s1)                 \
	vbroadcastsd( mem(rbx), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm6 )     \
	vbroadcastsd( mem(rbx, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm8 )     \
	vbroadcastsd( mem(rbx, r9, 2), ymm30 )  \
	vfmadd231pd( ymm(s1), ymm30, ymm10 )    \
	vbroadcastsd( mem(rbx, r13, 1), ymm31 ) \
	vfmadd231pd( ymm(s1), ymm31, ymm12 )    \
	vbroadcastsd( mem(r12), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm14 )    \
	vbroadcastsd( mem(r12, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm16 )    \
	vbroadcastsd( mem(r12, r9, 2), ymm30 )  \
	vfmadd231pd( ymm(s1), ymm30, ymm18 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, r12 )                          \
	add( r8, rbx )

#define K_LOOP_4x7_SET2(s1)                 \
	vbroadcastsd( mem(rbx), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm7 )     \
	vbroadcastsd( mem(rbx, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm9 )     \
	vbroadcastsd( mem(rbx, r9, 2), ymm30 )  \
	vfmadd231pd( ymm(s1), ymm30, ymm11 )    \
	vbroadcastsd( mem(rbx, r13, 1), ymm31 ) \
	vfmadd231pd( ymm(s1), ymm31, ymm13 )    \
	vbroadcastsd( mem(r12), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm15 )    \
	vbroadcastsd( mem(r12, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm17 )    \
	vbroadcastsd( mem(r12, r9, 2), ymm30 )  \
	vfmadd231pd( ymm(s1), ymm30, ymm19 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, r12 )                          \
	add( r8, rbx )

/**
 * K_LOOP_2x7_SET1
 * Move and duplicate the scalar value at memory location rbx to all elements of xmm30
 * Move and duplicate the scalar value at memory location rbx + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm6, store result in xmm6
 * Move and duplicate the scalar value at memory location rbx + 2*r9 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm8, store result in xmm8
 * Move and duplicate the scalar value at memory location rbx + r13 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm10, store result in xmm10
 * Move and duplicate the scalar value at memory location r12 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm12, store result in xmm12
 * Move and duplicate the scalar value at memory location r12 + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm14, store result in xmm14
 * Move and duplicate the scalar value at memory location r12 + 2*r9 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm16, store result in xmm16
 * Fused multiply-add: xmm(s1) * xmm30 + xmm18, store result in xmm18
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_2x7_SET1(s1)                 \
	vmovddup ( mem(rbx), xmm30 )            \
	vmovddup ( mem(rbx, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm6 )     \
	vmovddup ( mem(rbx, r9, 2), xmm30 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm8 )     \
	vmovddup ( mem(rbx, r13, 1), xmm31 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm10 )    \
	vmovddup ( mem(r12), xmm30 )            \
	vfmadd231pd( xmm(s1), xmm31, xmm12 )    \
	vmovddup ( mem(r12, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm14 )    \
	vmovddup ( mem(r12, r9, 2), xmm30 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm16 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm18 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

#define K_LOOP_2x7_SET2(s1)                 \
	vmovddup ( mem(rbx), xmm30 )            \
	vmovddup ( mem(rbx, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm7 )     \
	vmovddup ( mem(rbx, r9, 2), xmm30 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm9 )     \
	vmovddup ( mem(rbx, r13, 1), xmm31 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm11 )    \
	vmovddup ( mem(r12), xmm30 )            \
	vfmadd231pd( xmm(s1), xmm31, xmm13 )    \
	vmovddup ( mem(r12, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm15 )    \
	vmovddup ( mem(r12, r9, 2), xmm30 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm17 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm19 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_1x7_SET1
 * Move and duplicate the scalar value at memory location rbx to all elements of xmm30
 * Move and duplicate the scalar value at memory location rbx + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm6, store result in xmm6
 * Move and duplicate the scalar value at memory location rbx + 2*r9 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm8, store result in xmm8
 * Move and duplicate the scalar value at memory location rbx + r13 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm10, store result in xmm10
 * Move and duplicate the scalar value at memory location r12 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm12, store result in xmm12
 * Move and duplicate the scalar value at memory location r12 + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm14, store result in xmm14
 * Move and duplicate the scalar value at memory location r12 + 2*r9 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm16, store result in xmm16
 * Fused multiply-add: xmm(s1) * xmm30 + xmm18, store result in xmm18
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_1x7_SET1(s1)                 \
	vmovddup( mem(rbx), xmm30 )             \
	vmovddup( mem(rbx, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm6 )     \
	vmovddup( mem(rbx, r9, 2), xmm30 )      \
	vfmadd231pd( xmm(s1), xmm31, xmm8 )     \
	vmovddup( mem(rbx, r13, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm10 )    \
	vmovddup( mem(r12), xmm30 )             \
	vfmadd231pd( xmm(s1), xmm31, xmm12 )    \
	vmovddup( mem(r12, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm14 )    \
	vmovddup( mem(r12, r9, 2), xmm30 )      \
	vfmadd231pd( xmm(s1), xmm31, xmm16 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm18 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

#define K_LOOP_1x7_SET2(s1)                 \
	vmovddup( mem(rbx), xmm30 )             \
	vmovddup( mem(rbx, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm7 )     \
	vmovddup( mem(rbx, r9, 2), xmm30 )      \
	vfmadd231pd( xmm(s1), xmm31, xmm9 )     \
	vmovddup( mem(rbx, r13, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm11 )    \
	vmovddup( mem(r12), xmm30 )             \
	vfmadd231pd( xmm(s1), xmm31, xmm13 )    \
	vmovddup( mem(r12, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm15 )    \
	vmovddup( mem(r12, r9, 2), xmm30 )      \
	vfmadd231pd( xmm(s1), xmm31, xmm17 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm19 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_8MASKx7_SET1
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx) + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r9) + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + 2*r9) + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r13) + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s1) * mem_1to8(r12) + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s1) * mem_1to8(r12 + r9) + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s1) * mem_1to8(r12 + 2*r9) + zmm18, store result in zmm18
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_8MASKx7_SET1(s1)                             \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm6 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm8 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm10 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm12 )    \
	vfmadd231pd( mem_1to8(r12), zmm(s1), zmm14 )            \
	vfmadd231pd( mem_1to8(r12, r9, 1), zmm(s1), zmm16 )     \
	vfmadd231pd( mem_1to8(r12, r9, 2), zmm(s1), zmm18 )     \
	/*second pointer of b += rs_b*/                         \
	add( r8, rbx )                                          \
	add( r8, r12 )

#define K_LOOP_8MASKx7_SET2(s1)                             \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm7 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm9 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm11 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm13 )    \
	vfmadd231pd( mem_1to8(r12), zmm(s1), zmm15 )            \
	vfmadd231pd( mem_1to8(r12, r9, 1), zmm(s1), zmm17 )     \
	vfmadd231pd( mem_1to8(r12, r9, 2), zmm(s1), zmm19 )     \
	/*second pointer of b += rs_b*/                         \
	add( r8, rbx )                                          \
	add( r8, r12 )
/*********************End of 24x7 kernel family K-LOOP macros*******************/

/*********************Start of 24x6 kernel family K-LOOP macros*******************/

/**
 * K_LOOP_24x6
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm28, store result in zmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm29, store result in zmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm26, store result in zmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: zmm(s3)(a[16:32]) * zmm31 + zmm27, store result in zmm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm24, store result in zmm24
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm25, store result in zmm25
 * Increment the second pointer of B by rs_b
 * Increment B by rs_b
 */
#define K_LOOP_24x6(s1, s2, s3)                 \
    vbroadcastsd( mem(rbx), zmm30 )             \
    vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
    vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
    vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
    vfmadd231pd( zmm(s3), zmm30, zmm28 )        \
    vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
    vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
    vfmadd231pd( zmm(s2), zmm31, zmm9 )         \
    vfmadd231pd( zmm(s3), zmm31, zmm29 )        \
    vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
    vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
    vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
    vfmadd231pd( zmm(s3), zmm30, zmm26 )        \
    vbroadcastsd( mem(r12), zmm30 )             \
    vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
    vfmadd231pd( zmm(s2), zmm31, zmm13 )        \
    vfmadd231pd( zmm(s3), zmm31, zmm27 )        \
    vbroadcastsd( mem(r12, r9, 1), zmm31 )      \
    vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
    vfmadd231pd( zmm(s2), zmm30, zmm15 )        \
    vfmadd231pd( zmm(s3), zmm30, zmm24 )        \
    vfmadd231pd( zmm(s1), zmm31, zmm16 )        \
    vfmadd231pd( zmm(s2), zmm31, zmm17 )        \
    vfmadd231pd( zmm(s3), zmm31, zmm25 )        \
	/*second pointer of b += rs_b*/             \
    add( r8, r12 )                              \
    add( r8, rbx )/* b += rs_b*/

/**
 * K_LOOP_20x6
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: ymm(s3) * ymm30 + ymm28, store result in ymm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: ymm(s3) * ymm31 + ymm29, store result in ymm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: ymm(s3) * ymm30 + ymm26, store result in ymm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: ymm(s3) * ymm31 + ymm27, store result in ymm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: ymm(s3) * ymm30 + ymm24, store result in ymm24
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 * Fused multiply-add: ymm(s3) * ymm31 + ymm25, store result in ymm25
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_20x6(s1, s2, s3)                 \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
	vfmadd231pd( ymm(s3), ymm30, ymm28 )        \
                                                \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )         \
	vfmadd231pd( ymm(s3), ymm31, ymm29 )        \
                                                \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
	vfmadd231pd( ymm(s3), ymm30, ymm26 )        \
                                                \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )        \
	vfmadd231pd( ymm(s3), ymm31, ymm27 )        \
                                                \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )        \
	vfmadd231pd( ymm(s3), ymm30, ymm24 )        \
                                                \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )        \
	vfmadd231pd( ymm(s3), ymm31, ymm25 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_18x6
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm29, store result in xmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm30 + xmm26, store result in xmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm27, store result in xmm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm24, store result in xmm24
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm25, store result in xmm25
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_18x6(s1, s2, s3)                 \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
	vfmadd231pd( xmm(s3), xmm30, xmm28 )        \
                                                \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31,zmm8 )          \
	vfmadd231pd( zmm(s2), zmm31,  zmm9 )        \
	vfmadd231pd( xmm(s3), xmm31, xmm29 )        \
                                                \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
	vfmadd231pd( xmm(s3), xmm30, xmm26 )        \
                                                \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )        \
	vfmadd231pd( xmm(s3), xmm31, xmm27 )        \
                                                \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )        \
	vfmadd231pd( xmm(s3), xmm30, xmm24 )        \
                                                \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )        \
	vfmadd231pd( xmm(s3), xmm31, xmm25 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_17x6
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm29, store result in xmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm30 + xmm26, store result in xmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm27, store result in xmm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm24, store result in xmm24
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm25, store result in xmm25
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_17x6(s1, s2, s3)             \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( xmm(s3), xmm30, xmm28 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( xmm(s3), xmm31, xmm29 )    \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm26 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm27 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm24 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm25 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_16x6_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_16x6_SET1(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

#define K_LOOP_16x6_SET2(s1, s2)                \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )        \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm21 )        \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm22 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm23 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm24 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm25 )        \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm26 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm27 )        \
	vfmadd231pd( zmm(s1), zmm31, zmm28 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm29 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_24MASKx6
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm28, store result in zmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm29, store result in zmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm26, store result in zmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: zmm(s3)(a[16:32]) * zmm31 + zmm27, store result in zmm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm24, store result in zmm24
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm25, store result in zmm25
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_24MASKx6(s1, s2, s3)         \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( zmm(s3), zmm30, zmm28 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( zmm(s3), zmm31, zmm29 )    \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( zmm(s3), zmm30, zmm26 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	vfmadd231pd( zmm(s3), zmm31, zmm27 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vfmadd231pd( zmm(s3), zmm30, zmm24 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )    \
	vfmadd231pd( zmm(s3), zmm31, zmm25 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_12x6_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm30 + ymm7, store result in ymm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm31 + ymm9, store result in ymm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm30 + ymm11, store result in ymm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm31 + ymm13, store result in ymm13
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: ymm(s2) (a[8:11])* ymm30 + ymm15, store result in ymm15
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm31 + ymm17, store result in ymm17
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_12x6_SET1(s1, s2)                \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( ymm(s2), ymm30, ymm7 )         \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( ymm(s2), ymm31, ymm9 )         \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( ymm(s2), ymm30, ymm11 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
	vfmadd231pd( ymm(s2), ymm31, ymm13 )        \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
	vfmadd231pd( ymm(s2), ymm30, ymm15 )        \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )        \
	vfmadd231pd( ymm(s2), ymm31, ymm17 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

#define K_LOOP_12x6_SET2(s1, s2)                \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )        \
	vfmadd231pd( ymm(s2), ymm30, ymm19 )        \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )        \
	vfmadd231pd( ymm(s2), ymm31, ymm21 )        \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm22 )        \
	vfmadd231pd( ymm(s2), ymm30, ymm23 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm24 )        \
	vfmadd231pd( ymm(s2), ymm31, ymm25 )        \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm26 )        \
	vfmadd231pd( ymm(s2), ymm30, ymm27 )        \
	vfmadd231pd( zmm(s1), zmm31, zmm28 )        \
	vfmadd231pd( ymm(s2), ymm31, ymm29 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_11x6
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm1 * xmm30 + xmm7, store result in xmm7
 * Fused multiply-add: xmm2 * xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm0 * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm1 * xmm31 + xmm9, store result in xmm9
 * Fused multiply-add: xmm2 * xmm31 + xmm29, store result in xmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm1 * xmm30 + xmm11, store result in xmm11
 * Fused multiply-add: xmm2 * xmm30 + xmm26, store result in xmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm0 * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: xmm1 * xmm31 + xmm13, store result in xmm13
 * Fused multiply-add: xmm2 * xmm31 + xmm27, store result in xmm27
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: xmm1 * xmm30 + xmm15, store result in xmm15
 * Fused multiply-add: xmm2 * xmm30 + xmm24, store result in xmm24
 *
 * Fused multiply-add: zmm0 * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: xmm1 * xmm31 + xmm17, store result in xmm17
 * Fused multiply-add: xmm2 * xmm31 + xmm25, store result in xmm25
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_11x6(s1, s2, s3)                 \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm0, zmm30, zmm6 )            \
	vfmadd231pd( xmm1, xmm30, xmm7 )            \
	vfmadd231pd( xmm2, xmm30, xmm28 )           \
                                                \
	vbroadcastsd( mem(rbx, r9, 2),zmm30 )       \
	vfmadd231pd( zmm0, zmm31, zmm8 )            \
	vfmadd231pd( xmm1, xmm31, xmm9 )            \
	vfmadd231pd( xmm2, xmm31, xmm29 )           \
                                                \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm0, zmm30, zmm10 )           \
	vfmadd231pd( xmm1, xmm30, xmm11 )           \
	vfmadd231pd( xmm2, xmm30, xmm26 )           \
                                                \
	vbroadcastsd( mem(r12),  zmm30 )            \
	vfmadd231pd( zmm0, zmm31, zmm12 )           \
	vfmadd231pd( xmm1, xmm31, xmm13 )           \
	vfmadd231pd( xmm2, xmm31, xmm27 )           \
                                                \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )      \
	vfmadd231pd( zmm0, zmm30, zmm14 )           \
	vfmadd231pd( xmm1, xmm30, xmm15 )           \
	vfmadd231pd( xmm2, xmm30, xmm24 )           \
                                                \
	vfmadd231pd( zmm0, zmm31, zmm16 )           \
	vfmadd231pd( xmm1, xmm31, xmm17 )           \
	vfmadd231pd( xmm2, xmm31, xmm25 )           \
                                                \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_10x6_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm(s2) * xmm30 + xmm7, store result in xmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm(s2) * xmm31 + xmm9, store result in xmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm(s2) * xmm30 + xmm11, store result in xmm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: xmm(s2) * xmm31 + xmm13, store result in xmm13
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: xmm(s2) * xmm30 + xmm15, store result in xmm15
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: xmm(s2) * xmm31 + xmm17, store result in xmm17
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_10x6_SET1(s1, s2)                \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( xmm(s2), xmm30, xmm7 )         \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( xmm(s2), xmm31, xmm9 )         \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( xmm(s2), xmm30, xmm11 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
	vfmadd231pd( xmm(s2), xmm31, xmm13 )        \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
	vfmadd231pd( xmm(s2), xmm30, xmm15 )        \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )        \
	vfmadd231pd( xmm(s2), xmm31, xmm17 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

#define K_LOOP_10x6_SET2(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm19 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm21 )    \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm22 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm23 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm24 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm25 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm26 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm27 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm28 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm29 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_9x6_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm(s2) * xmm30 + xmm7, store result in xmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm(s2) * xmm31 + xmm9, store result in xmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm(s2) * xmm30 + xmm11, store result in xmm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: xmm(s2) * xmm31 + xmm13, store result in xmm13
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: xmm(s2) * xmm30 + xmm15, store result in xmm15
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: xmm(s2) * xmm31 + xmm17, store result in xmm17
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_9x6_SET1(s1, s2)             \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( xmm(s2), xmm30, xmm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( xmm(s2), xmm31, xmm9 )     \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm11 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm13 )    \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm15 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm17 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

#define K_LOOP_9x6_SET2(s1, s2)                 \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )        \
	vfmadd231pd( xmm(s2), xmm30, xmm19 )        \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )        \
	vfmadd231pd( xmm(s2), xmm31, xmm21 )        \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm22 )        \
	vfmadd231pd( xmm(s2), xmm30, xmm23 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm24 )        \
	vfmadd231pd( xmm(s2), xmm31, xmm25 )        \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm26 )        \
	vfmadd231pd( xmm(s2), xmm30, xmm27 )        \
	vfmadd231pd( zmm(s1), zmm31, zmm28 )        \
	vfmadd231pd( xmm(s2), xmm31, xmm29 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_8x6_SET1
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx) + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r9) + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + 2*r9) + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r13) + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s1) * mem_1to8(r12) + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s1) * mem_1to8(r12 + r9) + zmm16, store result in zmm16
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_8x6_SET1(s1)                                 \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm6 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm8 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm10 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm12 )    \
	vfmadd231pd( mem_1to8(r12), zmm(s1), zmm14 )            \
	vfmadd231pd( mem_1to8(r12, r9, 1), zmm(s1), zmm16 )     \
	/*second pointer of b += rs_b*/                         \
	add( r8, rbx )                                          \
	add( r8, r12 )

#define K_LOOP_8x6_SET2(s1)                                 \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm7 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm9 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm11 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm13 )    \
	vfmadd231pd( mem_1to8(r12), zmm(s1), zmm15 )            \
	vfmadd231pd( mem_1to8(r12, r9, 1), zmm(s1), zmm17 )     \
	/*second pointer of b += rs_b*/                         \
	add( r8, rbx )                                          \
	add( r8, r12 )

/**
 * K_LOOP_16MASKx6_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 *
 * Broadcast the scalar value at memory location r12 + r9(b[0, 5]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm16, store result in zmm16
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm17, store result in zmm17
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_16MASKx6_SET1(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )         \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )        \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )        \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

#define K_LOOP_16MASKx6_SET2(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )        \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm21 )        \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm22 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm23 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm24 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm25 )        \
	vbroadcastsd( mem(r12, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm26 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm27 )        \
	vfmadd231pd( zmm(s1), zmm31, zmm28 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm29 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_4x6_SET1
 * Broadcast the scalar value at memory location rbx to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm6, store result in ymm6
 * Broadcast the scalar value at memory location rbx + r9 to all elements of ymm31
 * Fused multiply-add: ymm(s1) * ymm31 + ymm8, store result in ymm8
 * Broadcast the scalar value at memory location rbx + 2*r9 to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm10, store result in ymm10
 * Broadcast the scalar value at memory location rbx + r13 to all elements of ymm31
 * Fused multiply-add: ymm(s1) * ymm31 + ymm12, store result in ymm12
 * Broadcast the scalar value at memory location r12 to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm14, store result in ymm14
 * Broadcast the scalar value at memory location r12 + r9 to all elements of ymm31
 * Fused multiply-add: ymm(s1) * ymm31 + ymm16, store result in ymm16
 * Increment the second pointer of B by rs_b
 * Increment B by rs_b
 */
#define K_LOOP_4x6_SET1(s1)                 \
	vbroadcastsd( mem(rbx), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm6 )     \
	vbroadcastsd( mem(rbx, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm8 )     \
	vbroadcastsd( mem(rbx, r9, 2), ymm30 )  \
	vfmadd231pd( ymm(s1), ymm30, ymm10 )    \
	vbroadcastsd( mem(rbx, r13, 1), ymm31 ) \
	vfmadd231pd( ymm(s1), ymm31, ymm12 )    \
	vbroadcastsd( mem(r12), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm14 )    \
	vbroadcastsd( mem(r12, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm16 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, r12 )                          \
	add( r8, rbx )

#define K_LOOP_4x6_SET2(s1)                 \
	vbroadcastsd( mem(rbx), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm7 )     \
	vbroadcastsd( mem(rbx, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm9 )     \
	vbroadcastsd( mem(rbx, r9, 2), ymm30 )  \
	vfmadd231pd( ymm(s1), ymm30, ymm11 )    \
	vbroadcastsd( mem(rbx, r13, 1), ymm31 ) \
	vfmadd231pd( ymm(s1), ymm31, ymm13 )    \
	vbroadcastsd( mem(r12), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm15 )    \
	vbroadcastsd( mem(r12, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm17 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, r12 )                          \
	add( r8, rbx )

/**
 * K_LOOP_2x6_SET1
 * Move and duplicate the scalar value at memory location rbx to all elements of xmm30
 * Move and duplicate the scalar value at memory location rbx + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm6, store result in xmm6
 * Move and duplicate the scalar value at memory location rbx + 2*r9 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm8, store result in xmm8
 * Move and duplicate the scalar value at memory location rbx + r13 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm10, store result in xmm10
 * Move and duplicate the scalar value at memory location r12 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm12, store result in xmm12
 * Move and duplicate the scalar value at memory location r12 + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm14, store result in xmm14
 * Fused multiply-add: xmm(s1) * xmm31 + xmm16, store result in xmm16
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_2x6_SET1(s1)                 \
	vmovddup ( mem(rbx), xmm30 )            \
	vmovddup ( mem(rbx, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm6 )     \
	vmovddup ( mem(rbx, r9, 2), xmm30 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm8 )     \
	vmovddup ( mem(rbx, r13, 1), xmm31 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm10 )    \
	vmovddup ( mem(r12), xmm30 )            \
	vfmadd231pd( xmm(s1), xmm31, xmm12 )    \
	vmovddup ( mem(r12, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm14 )    \
	vfmadd231pd( xmm(s1), xmm31, xmm16 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

#define K_LOOP_2x6_SET2(s1)                 \
	vmovddup ( mem(rbx), xmm30 )            \
	vmovddup ( mem(rbx, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm7 )     \
	vmovddup ( mem(rbx, r9, 2), xmm30 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm9 )     \
	vmovddup ( mem(rbx, r13, 1), xmm31 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm11 )    \
	vmovddup ( mem(r12), xmm30 )            \
	vfmadd231pd( xmm(s1), xmm31, xmm13 )    \
	vmovddup ( mem(r12, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm15 )    \
	vfmadd231pd( xmm(s1), xmm31, xmm17 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_1x6_SET1
 * Move and duplicate the scalar value at memory location rbx to all elements of xmm30
 * Move and duplicate the scalar value at memory location rbx + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm6, store result in xmm6
 * Move and duplicate the scalar value at memory location rbx + 2*r9 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm8, store result in xmm8
 * Move and duplicate the scalar value at memory location rbx + r13 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm10, store result in xmm10
 * Move and duplicate the scalar value at memory location r12 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm12, store result in xmm12
 * Move and duplicate the scalar value at memory location r12 + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm14, store result in xmm14
 * Fused multiply-add: xmm(s1) * xmm31 + xmm16, store result in xmm16
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_1x6_SET1(s1)                 \
	vmovddup( mem(rbx), xmm30 )             \
	vmovddup( mem(rbx, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm6 )     \
	vmovddup( mem(rbx, r9, 2), xmm30 )      \
	vfmadd231pd( xmm(s1), xmm31, xmm8 )     \
	vmovddup( mem(rbx, r13, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm10 )    \
	vmovddup( mem(r12), xmm30 )             \
	vfmadd231pd( xmm(s1), xmm31, xmm12 )    \
	vmovddup( mem(r12, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm14 )    \
	vfmadd231pd( xmm(s1), xmm31, xmm16 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

#define K_LOOP_1x6_SET2(s1)                 \
	vmovddup( mem(rbx), xmm30 )             \
	vmovddup( mem(rbx, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm7 )     \
	vmovddup( mem(rbx, r9, 2), xmm30 )      \
	vfmadd231pd( xmm(s1), xmm31, xmm9 )     \
	vmovddup( mem(rbx, r13, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm11 )    \
	vmovddup( mem(r12), xmm30 )             \
	vfmadd231pd( xmm(s1), xmm31, xmm13 )    \
	vmovddup( mem(r12, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm15 )    \
	vfmadd231pd( xmm(s1), xmm31, xmm17 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_8MASKx6_SET1
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx) + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r9) + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + 2*r9) + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r13) + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s1) * mem_1to8(r12) + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s1) * mem_1to8(r12 + r9) + zmm16, store result in zmm16
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_8MASKx6_SET1(s1)                             \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm6 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm8 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm10 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm12 )    \
	vfmadd231pd( mem_1to8(r12), zmm(s1), zmm14 )            \
	vfmadd231pd( mem_1to8(r12, r9, 1), zmm(s1), zmm16 )     \
	/*second pointer of b += rs_b*/                         \
	add( r8, rbx )                                          \
	add( r8, r12 )

#define K_LOOP_8MASKx6_SET2(s1)                             \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm7 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm9 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm11 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm13 )    \
	vfmadd231pd( mem_1to8(r12), zmm(s1), zmm15 )            \
	vfmadd231pd( mem_1to8(r12, r9, 1), zmm(s1), zmm17 )     \
	/*second pointer of b += rs_b*/                         \
	add( r8, rbx )                                          \
	add( r8, r12 )
/*********************End of 24x6 kernel family K-LOOP macros*********************/

/*********************Start of 24x5 kernel family K-LOOP macros*******************/
/**
 * K_LOOP_24x5
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm28, store result in zmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm29, store result in zmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm26, store result in zmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: zmm(s3)(a[16:32]) * zmm31 + zmm27, store result in zmm27
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm24, store result in zmm24

 * Increment the second pointer of B by rs_b
 * Increment B by rs_b
 */
#define K_LOOP_24x5(s1, s2, s3)             \
    vbroadcastsd( mem(rbx), zmm30 )         \
    vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
    vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
    vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
    vfmadd231pd( zmm(s3), zmm30, zmm28 )    \
    vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
    vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
    vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
    vfmadd231pd( zmm(s3), zmm31, zmm29 )    \
    vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
    vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
    vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
    vfmadd231pd( zmm(s3), zmm30, zmm26 )    \
    vbroadcastsd( mem(r12), zmm30 )         \
    vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
    vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
    vfmadd231pd( zmm(s3), zmm31, zmm27 )    \
    vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
    vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
    vfmadd231pd( zmm(s3), zmm30, zmm24 )    \
	/*second pointer of b += rs_b*/         \
    add( r8, r12 )                          \
    add( r8, rbx )

/**
 * K_LOOP_20x5
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: ymm(s3) * ymm30 + ymm28, store result in ymm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: ymm(s3) * ymm31 + ymm29, store result in ymm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: ymm(s3) * ymm30 + ymm26, store result in ymm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: ymm(s3) * ymm31 + ymm27, store result in ymm27
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: ymm(s3) * ymm30 + ymm24, store result in ymm24
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_20x5(s1, s2, s3)                 \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
	vfmadd231pd( ymm(s3), ymm30, ymm28 )        \
                                                \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )         \
	vfmadd231pd( ymm(s3), ymm31, ymm29 )        \
                                                \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
	vfmadd231pd( ymm(s3), ymm30, ymm26 )        \
                                                \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )        \
	vfmadd231pd( ymm(s3), ymm31, ymm27 )        \
                                                \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )        \
	vfmadd231pd( ymm(s3), ymm30, ymm24 )        \
                                                \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_18x5
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm29, store result in xmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm30 + xmm26, store result in xmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm27, store result in xmm27
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm24, store result in xmm24
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_18x5(s1, s2, s3)             \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( xmm(s3), xmm30, xmm28 )    \
                                            \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( xmm(s3), xmm31, xmm29 )    \
                                            \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm26 )    \
                                            \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm27 )    \
                                            \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm24 )    \
                                            \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_17x5
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm29, store result in xmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm30 + xmm26, store result in xmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm27, store result in xmm27
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm24, store result in xmm24
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_17x5(s1, s2, s3)                 \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
	vfmadd231pd( xmm(s3), xmm30, xmm28 )        \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )         \
	vfmadd231pd( xmm(s3), xmm31, xmm29 )        \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
	vfmadd231pd( xmm(s3), xmm30, xmm26 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )        \
	vfmadd231pd( xmm(s3), xmm31, xmm27 )        \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )        \
	vfmadd231pd( xmm(s3), xmm30, xmm24 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_16x5_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_16x5_SET1(s1, s2)                \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )         \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )        \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

#define K_LOOP_16x5_SET2(s1, s2)                \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm16 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm17 )        \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm18 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm19 )        \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm20 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm21 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm22 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm23 )        \
	vfmadd231pd( zmm(s1), zmm30, zmm24 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm25 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_24MASKx5
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm28, store result in zmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm29, store result in zmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm26, store result in zmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: zmm(s3)(a[16:32]) * zmm31 + zmm27, store result in zmm27
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm24, store result in zmm24
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_24MASKx5(s1, s2, s3)             \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
	vfmadd231pd( zmm(s3), zmm30, zmm28 )        \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )         \
	vfmadd231pd( zmm(s3), zmm31, zmm29 )        \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
	vfmadd231pd( zmm(s3), zmm30, zmm26 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )        \
	vfmadd231pd( zmm(s3), zmm31, zmm27 )        \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )        \
	vfmadd231pd( zmm(s3), zmm30, zmm24 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_12x5
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm30 + ymm7, store result in ymm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm31 + ymm9, store result in ymm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm30 + ymm11, store result in ymm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm31 + ymm13, store result in ymm13
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: ymm(s2) (a[8:11])* ymm30 + ymm15, store result in ymm15
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_12x5_SET1(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( ymm(s2), ymm30, ymm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( ymm(s2), ymm31, ymm9 )     \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( ymm(s2), ymm30, ymm11 )    \
	vbroadcastsd( mem(r12), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( ymm(s2), ymm31, ymm13 )    \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( ymm(s2), ymm30, ymm15 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

#define K_LOOP_12x5_SET2(s1, s2)                \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm16 )        \
	vfmadd231pd( ymm(s2), ymm30, ymm17 )        \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm18 )        \
	vfmadd231pd( ymm(s2), ymm31, ymm19 )        \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm20 )        \
	vfmadd231pd( ymm(s2), ymm30, ymm21 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm22 )        \
	vfmadd231pd( ymm(s2), ymm31, ymm23 )        \
	vfmadd231pd( zmm(s1), zmm30, zmm24 )        \
	vfmadd231pd( ymm(s2), ymm30, ymm25 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_11x5
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm1 * xmm30 + xmm7, store result in xmm7
 * Fused multiply-add: xmm2 * xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm0 * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm1 * xmm31 + xmm9, store result in xmm9
 * Fused multiply-add: xmm2 * xmm31 + xmm29, store result in xmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm1 * xmm30 + xmm11, store result in xmm11
 * Fused multiply-add: xmm2 * xmm30 + xmm26, store result in xmm26
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm0 * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: xmm1 * xmm31 + xmm13, store result in xmm13
 * Fused multiply-add: xmm2 * xmm31 + xmm27, store result in xmm27
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: xmm1 * xmm30 + xmm15, store result in xmm15
 * Fused multiply-add: xmm2 * xmm30 + xmm24, store result in xmm24
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_11x5(s1, s2, s3)                 \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm0, zmm30, zmm6 )            \
	vfmadd231pd( xmm1, xmm30, xmm7 )            \
	vfmadd231pd( xmm2, xmm30, xmm28 )           \
                                                \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm0, zmm31, zmm8 )            \
	vfmadd231pd( xmm1, xmm31, xmm9 )            \
	vfmadd231pd( xmm2, xmm31, xmm29 )           \
                                                \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm0, zmm30, zmm10 )           \
	vfmadd231pd( xmm1, xmm30, xmm11 )           \
	vfmadd231pd( xmm2, xmm30, xmm26 )           \
                                                \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm0, zmm31, zmm12 )           \
	vfmadd231pd( xmm1, xmm31, xmm13 )           \
	vfmadd231pd( xmm2, xmm31, xmm27 )           \
                                                \
	vfmadd231pd( zmm0, zmm30, zmm14 )           \
	vfmadd231pd( xmm1, xmm30, xmm15 )           \
	vfmadd231pd( xmm2, xmm30, xmm24 )           \
                                                \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_10x5_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm(s2) * xmm30 + xmm7, store result in xmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm(s2) * xmm31 + xmm9, store result in xmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm(s2) * xmm30 + xmm11, store result in xmm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: xmm(s2) * xmm31 + xmm13, store result in xmm13
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: xmm(s2) * xmm30 + xmm15, store result in xmm15
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_10x5_SET1(s1, s2)                \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( xmm(s2), xmm30, xmm7 )         \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( xmm(s2), xmm31, xmm9 )         \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( xmm(s2), xmm30, xmm11 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
	vfmadd231pd( xmm(s2), xmm31, xmm13 )        \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
	vfmadd231pd( xmm(s2), xmm30, xmm15 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

#define K_LOOP_10x5_SET2(s1, s2)                \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm16 )        \
	vfmadd231pd( xmm(s2), xmm30, xmm17 )        \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm18 )        \
	vfmadd231pd( xmm(s2), xmm31, xmm19 )        \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm20 )        \
	vfmadd231pd( xmm(s2), xmm30, xmm21 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm22 )        \
	vfmadd231pd( xmm(s2), xmm31, xmm23 )        \
	vfmadd231pd( zmm(s1), zmm30, zmm24 )        \
	vfmadd231pd( xmm(s2), xmm30, xmm25 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_9x5_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm(s2) * xmm30 + xmm7, store result in xmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm(s2) * xmm31 + xmm9, store result in xmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm(s2) * xmm30 + xmm11, store result in xmm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: xmm(s2) * xmm31 + xmm13, store result in xmm13
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: xmm(s2) * xmm30 + xmm15, store result in xmm15
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_9x5_SET1(s1, s2)                 \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( xmm(s2), xmm30, xmm7 )         \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( xmm(s2), xmm31, xmm9 )         \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( xmm(s2), xmm30, xmm11 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
	vfmadd231pd( xmm(s2), xmm31, xmm13 )        \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
	vfmadd231pd( xmm(s2), xmm30, xmm15 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

#define K_LOOP_9x5_SET2(s1, s2)                 \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm16 )        \
	vfmadd231pd( xmm(s2), xmm30, xmm17 )        \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm18 )        \
	vfmadd231pd( xmm(s2), xmm31, xmm19 )        \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm20 )        \
	vfmadd231pd( xmm(s2), xmm30, xmm21 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm22 )        \
	vfmadd231pd( xmm(s2), xmm31, xmm23 )        \
	vfmadd231pd( zmm(s1), zmm30, zmm24 )        \
	vfmadd231pd( xmm(s2), xmm30, xmm25 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_8x5_SET1
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx) + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r9) + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + 2*r9) + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r13) + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s1) * mem_1to8(r12) + zmm14, store result in zmm14
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_8x5_SET1(s1)                                 \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm6 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm8 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm10 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm12 )    \
	vfmadd231pd( mem_1to8(r12), zmm(s1), zmm14 )            \
	/*second pointer of b += rs_b*/                         \
	add( r8, rbx )                                          \
	add( r8, r12 )

#define K_LOOP_8x5_SET2(s1)                                 \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm7 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm9 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm11 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm13 )    \
	vfmadd231pd( mem_1to8(r12), zmm(s1), zmm15 )            \
	/*second pointer of b += rs_b*/                         \
	add( r8, rbx )                                          \
	add( r8, r12 )

/**
 * K_LOOP_16MASKx5_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 *
 * Broadcast the scalar value at memory location r12(b[0, 4]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm14, store result in zmm14
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm15, store result in zmm15
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_16MASKx5_SET1(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )         \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )        \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

#define K_LOOP_16MASKx5_SET2(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm16 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm17 )        \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm18 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm19 )        \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm20 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm21 )        \
	vbroadcastsd( mem(r12), zmm30 )             \
	vfmadd231pd( zmm(s1), zmm31, zmm22 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm23 )        \
	vfmadd231pd( zmm(s1), zmm30, zmm24 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm25 )        \
	/*second pointer of b += rs_b*/             \
	add( r8, rbx )                              \
	add( r8, r12 )

/**
 * K_LOOP_4x5_SET1
 * Broadcast the scalar value at memory location rbx to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm6, store result in ymm6
 * Broadcast the scalar value at memory location rbx + r9 to all elements of ymm31
 * Fused multiply-add: ymm(s1) * ymm31 + ymm8, store result in ymm8
 * Broadcast the scalar value at memory location rbx + 2*r9 to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm10, store result in ymm10
 * Broadcast the scalar value at memory location rbx + r13 to all elements of ymm31
 * Fused multiply-add: ymm(s1) * ymm31 + ymm12, store result in ymm12
 * Broadcast the scalar value at memory location r12 to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm14, store result in ymm14
 * Increment the second pointer of B by rs_b
 * Increment B by rs_b
 */
#define K_LOOP_4x5_SET1(s1)                 \
	vbroadcastsd( mem(rbx), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm6 )     \
	vbroadcastsd( mem(rbx, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm8 )     \
	vbroadcastsd( mem(rbx, r9, 2), ymm30 )  \
	vfmadd231pd( ymm(s1), ymm30, ymm10 )    \
	vbroadcastsd( mem(rbx, r13, 1), ymm31 ) \
	vfmadd231pd( ymm(s1), ymm31, ymm12 )    \
	vbroadcastsd( mem(r12), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm14 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, r12 )                          \
	add( r8, rbx )

#define K_LOOP_4x5_SET2(s1)                 \
	vbroadcastsd( mem(rbx), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm7 )     \
	vbroadcastsd( mem(rbx, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm9 )     \
	vbroadcastsd( mem(rbx, r9, 2), ymm30 )  \
	vfmadd231pd( ymm(s1), ymm30, ymm11 )    \
	vbroadcastsd( mem(rbx, r13, 1), ymm31 ) \
	vfmadd231pd( ymm(s1), ymm31, ymm13 )    \
	vbroadcastsd( mem(r12), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm15 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, r12 )                          \
	add( r8, rbx )

/**
 * K_LOOP_2x5_SET1
 * Move and duplicate the scalar value at memory location rbx to all elements of xmm30
 * Move and duplicate the scalar value at memory location rbx + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm6, store result in xmm6
 * Move and duplicate the scalar value at memory location rbx + 2*r9 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm8, store result in xmm8
 * Move and duplicate the scalar value at memory location rbx + r13 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm10, store result in xmm10
 * Move and duplicate the scalar value at memory location r12 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm12, store result in xmm12
 * Fused multiply-add: xmm(s1) * xmm30 + xmm14, store result in xmm14
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_2x5_SET1(s1)                 \
	vmovddup ( mem(rbx), xmm30 )            \
	vmovddup ( mem(rbx, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm6 )     \
	vmovddup ( mem(rbx, r9, 2), xmm30 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm8 )     \
	vmovddup ( mem(rbx, r13, 1), xmm31 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm10 )    \
	vmovddup ( mem(r12), xmm30 )            \
	vfmadd231pd( xmm(s1), xmm31, xmm12 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm14 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

#define K_LOOP_2x5_SET2(s1)                 \
	vmovddup ( mem(rbx), xmm30 )            \
	vmovddup ( mem(rbx, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm7 )     \
	vmovddup ( mem(rbx, r9, 2), xmm30 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm9 )     \
	vmovddup ( mem(rbx, r13, 1), xmm31 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm11 )    \
	vmovddup ( mem(r12), xmm30 )            \
	vfmadd231pd( xmm(s1), xmm31, xmm13 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm15 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_1x5_SET1
 * Move and duplicate the scalar value at memory location rbx to all elements of xmm30
 * Move and duplicate the scalar value at memory location rbx + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm6, store result in xmm6
 * Move and duplicate the scalar value at memory location rbx + 2*r9 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm8, store result in xmm8
 * Move and duplicate the scalar value at memory location rbx + r13 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm10, store result in xmm10
 * Move and duplicate the scalar value at memory location r12 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm12, store result in xmm12
 * Fused multiply-add: xmm(s1) * xmm30 + xmm14, store result in xmm14
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_1x5_SET1(s1)                 \
	vmovddup( mem(rbx), xmm30 )             \
	vmovddup( mem(rbx, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm6 )     \
	vmovddup( mem(rbx, r9, 2),xmm30 )       \
	vfmadd231pd( xmm(s1), xmm31, xmm8 )     \
	vmovddup( mem(rbx, r13, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm10 )    \
	vmovddup( mem(r12), xmm30 )             \
	vfmadd231pd( xmm(s1), xmm31, xmm12 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm14 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

#define K_LOOP_1x5_SET2(s1)                 \
	vmovddup( mem(rbx), xmm30 )             \
	vmovddup( mem(rbx, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm7 )     \
	vmovddup( mem(rbx, r9, 2), xmm30 )      \
	vfmadd231pd( xmm(s1), xmm31, xmm9 )     \
	vmovddup( mem(rbx, r13, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm11 )    \
	vmovddup( mem(r12), xmm30 )             \
	vfmadd231pd( xmm(s1), xmm31, xmm13 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm15 )    \
	/*second pointer of b += rs_b*/         \
	add( r8, rbx )                          \
	add( r8, r12 )

/**
 * K_LOOP_8MASKx5_SET1
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx) + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r9) + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + 2*r9) + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r13) + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s1) * mem_1to8(r12) + zmm14, store result in zmm14
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_8MASKx5_SET1(s1)                             \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm6 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm8 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm10 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm12 )    \
	vfmadd231pd( mem_1to8(r12), zmm(s1), zmm14 )            \
	/*second pointer of b += rs_b*/                         \
	add( r8, rbx )                                          \
	add( r8, r12 )

#define K_LOOP_8MASKx5_SET2(s1)                             \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm7 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm9 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm11 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm13 )    \
	vfmadd231pd( mem_1to8(r12), zmm(s1), zmm15 )            \
	/*second pointer of b += rs_b*/                         \
	add( r8, rbx )                                          \
	add( r8, r12 )
/*********************End of 24x5 kernel family K-LOOP macros*********************/

/*********************Start of 24x4 kernel family K-LOOP macros*******************/
/**
 * K_LOOP_24x4_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm28, store result in zmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm29, store result in zmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm26, store result in zmm26
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: zmm(s3)(a[16:32]) * zmm31 + zmm27, store result in zmm27

 * Increment the second pointer of B by rs_b
 * Increment B by rs_b
 */
#define K_LOOP_24x4_SET1(s1, s2, s3)            \
    vbroadcastsd( mem(rbx), zmm30 )             \
    vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
    vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
    vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
    vfmadd231pd( zmm(s3), zmm30, zmm28 )        \
    vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
    vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
    vfmadd231pd( zmm(s2), zmm31, zmm9 )         \
    vfmadd231pd( zmm(s3), zmm31, zmm29 )        \
    vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
    vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
    vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
    vfmadd231pd( zmm(s3), zmm30, zmm26 )        \
    vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
    vfmadd231pd( zmm(s2), zmm31, zmm13 )        \
    vfmadd231pd( zmm(s3), zmm31, zmm27 )        \
    add( r8, rbx )

#define K_LOOP_24x4_SET2(s1, s2, s3)            \
    vbroadcastsd( mem(rbx), zmm30 )             \
    vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
    vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
    vfmadd231pd( zmm(s2), zmm30, zmm15 )        \
    vfmadd231pd( zmm(s3), zmm30, zmm16 )        \
    vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
    vfmadd231pd( zmm(s1), zmm31, zmm17 )        \
    vfmadd231pd( zmm(s2), zmm31, zmm18 )        \
    vfmadd231pd( zmm(s3), zmm31, zmm19 )        \
    vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
    vfmadd231pd( zmm(s1), zmm30, zmm20 )        \
    vfmadd231pd( zmm(s2), zmm30, zmm21 )        \
    vfmadd231pd( zmm(s3), zmm30, zmm22 )        \
    vfmadd231pd( zmm(s1), zmm31, zmm23 )        \
    vfmadd231pd( zmm(s2), zmm31, zmm24 )        \
    vfmadd231pd( zmm(s3), zmm31, zmm25 )        \
    add( r8,rbx )

/**
 * K_LOOP_20x4_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: ymm(s3) * ymm30 + ymm28, store result in ymm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: ymm(s3) * ymm31 + ymm29, store result in ymm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: ymm(s3) * ymm30 + ymm26, store result in ymm26
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: ymm(s3) * ymm31 + ymm27, store result in ymm27
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_20x4_SET1(s1, s2, s3)            \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
	vfmadd231pd( ymm(s3), ymm30, ymm28 )        \
                                                \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )         \
	vfmadd231pd( ymm(s3), ymm31, ymm29 )        \
                                                \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
	vfmadd231pd( ymm(s3), ymm30, ymm26 )        \
                                                \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )        \
	vfmadd231pd( ymm(s3), ymm31, ymm27 )        \
                                                \
	add( r8, rbx )

#define K_LOOP_20x4_SET2(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vfmadd231pd( ymm(s3), ymm30, ymm16 )    \
                                            \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm17 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm18 )    \
	vfmadd231pd( ymm(s3), ymm31, ymm19 )    \
                                            \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm20 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm21 )    \
	vfmadd231pd( ymm(s3), ymm30, ymm22 )    \
                                            \
	vfmadd231pd( zmm(s1), zmm31, zmm23 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm24 )    \
	vfmadd231pd( ymm(s3), ymm31, ymm25 )    \
                                            \
	add( r8, rbx )

/**
 * K_LOOP_18x4_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm29, store result in xmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm30 + xmm26, store result in xmm26
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm27, store result in xmm27
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_18x4_SET1(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( xmm(s3), xmm30, xmm28 )    \
                                            \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( xmm(s3), xmm31, xmm29 )    \
                                            \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm26 )    \
                                            \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm27 )    \
                                            \
	add( r8, rbx )

#define K_LOOP_18x4_SET2(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm16 )    \
                                            \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm17 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm18 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm19 )    \
                                            \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm20 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm21 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm22 )    \
                                            \
	vfmadd231pd( zmm(s1), zmm31, zmm23 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm24 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm25 )    \
                                            \
	add( r8, rbx )

/**
 * K_LOOP_17x4_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm29, store result in xmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm30 + xmm26, store result in xmm26
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm27, store result in xmm27
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_17x4_SET1(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( xmm(s3), xmm30, xmm28 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( xmm(s3), xmm31, xmm29 )    \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm26 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm27 )    \
	add( r8, rbx )

#define K_LOOP_17x4_SET2(s1, s2, s3)            \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )        \
	vfmadd231pd( xmm(s3), xmm30, xmm16 )        \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm17 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm18 )        \
	vfmadd231pd( xmm(s3), xmm31, xmm19 )        \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm20 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm21 )        \
	vfmadd231pd( xmm(s3), xmm30, xmm22 )        \
	vfmadd231pd( zmm(s1), zmm31, zmm23 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm24 )        \
	vfmadd231pd( xmm(s3), xmm31, xmm25 )        \
	add( r8, rbx )

/**
 * K_LOOP_16x4_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_16x4_SET1(s1, s2)                \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )         \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )        \
	add( r8, rbx )

#define K_LOOP_16x4_SET2(s1, s2)                \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )        \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )        \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )        \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm21 )        \
	add( r8, rbx )

/**
 * K_LOOP_24MASKx4
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm28, store result in zmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm29, store result in zmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm26, store result in zmm26
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Fused multiply-add: zmm(s3)(a[16:32]) * zmm31 + zmm27, store result in zmm27
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */

#define K_LOOP_24MASKx4_SET1(s1, s2, s3)    \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( zmm(s3), zmm30, zmm28 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( zmm(s3), zmm31, zmm29 )    \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( zmm(s3), zmm30, zmm26 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	vfmadd231pd( zmm(s3), zmm31, zmm27 )    \
	add( r8, rbx )

#define K_LOOP_24MASKx4_SET2(s1, s2, s3)    \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vfmadd231pd( zmm(s3), zmm30, zmm16 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm17 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm18 )    \
	vfmadd231pd( zmm(s3), zmm31, zmm19 )    \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm20 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm21 )    \
	vfmadd231pd( zmm(s3), zmm30, zmm22 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm23 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm24 )    \
	vfmadd231pd( zmm(s3), zmm31, zmm25 )    \
	add( r8, rbx )

/**
 * K_LOOP_12x4
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm30 + ymm7, store result in ymm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm31 + ymm9, store result in ymm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm30 + ymm11, store result in ymm11
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm31 + ymm13, store result in ymm13
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_12x4_SET1(s1, s2)                \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( ymm(s2), ymm30, ymm7 )         \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( ymm(s2), ymm31, ymm9 )         \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( ymm(s2), ymm30, ymm11 )        \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )        \
	vfmadd231pd( ymm(s2), ymm31, ymm13 )        \
	add( r8, rbx )

#define K_LOOP_12x4_SET2(s1, s2)                \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )        \
	vfmadd231pd( ymm(s2), ymm30, ymm15 )        \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )        \
	vfmadd231pd( ymm(s2), ymm31, ymm17 )        \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )        \
	vfmadd231pd( ymm(s2), ymm30, ymm19 )        \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )        \
	vfmadd231pd( ymm(s2), ymm31, ymm21 )        \
	add( r8, rbx )

/**
 * K_LOOP_11x4_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm1 * xmm30 + xmm7, store result in xmm7
 * Fused multiply-add: xmm2 * xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm0 * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm1 * xmm31 + xmm9, store result in xmm9
 * Fused multiply-add: xmm2 * xmm31 + xmm29, store result in xmm29
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm1 * xmm30 + xmm11, store result in xmm11
 * Fused multiply-add: xmm2 * xmm30 + xmm26, store result in xmm26
 *
 * Fused multiply-add: zmm0 * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: xmm1 * xmm31 + xmm13, store result in xmm13
 * Fused multiply-add: xmm2 * xmm31 + xmm27, store result in xmm27
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_11x4_SET1(s1, s2, s3)            \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm0, zmm30, zmm6 )            \
	vfmadd231pd( xmm1, xmm30, xmm7 )            \
	vfmadd231pd( xmm2, xmm30, xmm28 )           \
                                                \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm0, zmm31, zmm8 )            \
	vfmadd231pd( xmm1, xmm31, xmm9 )            \
	vfmadd231pd( xmm2, xmm31, xmm29 )           \
                                                \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 )     \
	vfmadd231pd( zmm0, zmm30, zmm10 )           \
	vfmadd231pd( xmm1, xmm30, xmm11 )           \
	vfmadd231pd( xmm2, xmm30, xmm26 )           \
                                                \
	vfmadd231pd( zmm0, zmm31, zmm12 )           \
	vfmadd231pd( xmm1, xmm31, xmm13 )           \
	vfmadd231pd( xmm2, xmm31, xmm27 )           \
                                                \
	add( r8, rbx )

#define K_LOOP_11x4_SET2(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm0, zmm30, zmm14 )       \
	vfmadd231pd( xmm1, xmm30, xmm15 )       \
	vfmadd231pd( xmm2, xmm30, xmm16 )       \
                                            \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm0, zmm31, zmm17 )       \
	vfmadd231pd( xmm1, xmm31, xmm18 )       \
	vfmadd231pd( xmm2, xmm31, xmm19 )       \
                                            \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm0, zmm30, zmm20 )       \
	vfmadd231pd( xmm1, xmm30, xmm21 )       \
	vfmadd231pd( xmm2, xmm30, xmm22 )       \
                                            \
	vfmadd231pd( zmm0, zmm31, zmm23 )       \
	vfmadd231pd( xmm1, xmm31, xmm24 )       \
	vfmadd231pd( xmm2, xmm31, xmm25 )       \
                                            \
	add( r8, rbx )

/**
 * K_LOOP_10x4_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm(s2) * xmm30 + xmm7, store result in xmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm(s2) * xmm31 + xmm9, store result in xmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm(s2) * xmm30 + xmm11, store result in xmm11
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: xmm(s2) * xmm31 + xmm13, store result in xmm13
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_10x4_SET1(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( xmm(s2), xmm30, xmm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( xmm(s2), xmm31, xmm9 )     \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm11 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm13 )    \
	add( r8, rbx )

#define K_LOOP_10x4_SET2(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm15 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm17 )    \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm19 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm21 )    \
	add( r8, rbx )

/**
 * K_LOOP_9x4_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm(s2) * xmm30 + xmm7, store result in xmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm(s2) * xmm31 + xmm9, store result in xmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm(s2) * xmm30 + xmm11, store result in xmm11
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: xmm(s2) * xmm31 + xmm13, store result in xmm13
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_9x4_SET1(s1, s2)             \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( xmm(s2), xmm30, xmm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( xmm(s2), xmm31, xmm9 )     \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm11 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm13 )    \
	add( r8, rbx )

#define K_LOOP_9x4_SET2(s1, s2)             \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm15 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm17 )    \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm19 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm21 )    \
	add( r8, rbx )

/**
 * K_LOOP_8x4_SET1
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx) + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r9) + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + 2*r9) + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r13) + zmm12, store result in zmm12
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_8x4_SET1(s1)                                 \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm6 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm8 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm10 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm12 )    \
	add( r8, rbx )

#define K_LOOP_8x4_SET2(s1)                                 \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm7 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm9 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm11 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm13 )    \
	add( r8, rbx )

/**
 * K_LOOP_16MASKx4_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 *
 * Broadcast the scalar value at memory location rbx + r13(b[0, 3]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm12, store result in zmm12
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm13, store result in zmm13
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_16MASKx4_SET1(s1, s2)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	add( r8, rbx )

#define K_LOOP_16MASKx4_SET2(s1, s2)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm15 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm16 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm17 )    \
	vbroadcastsd( mem(rbx, r13, 1), zmm31 ) \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm20 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm21 )    \
	add( r8, rbx )

/**
 * K_LOOP_4x4_SET1
 * Broadcast the scalar value at memory location rbx to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm6, store result in ymm6
 * Broadcast the scalar value at memory location rbx + r9 to all elements of ymm31
 * Fused multiply-add: ymm(s1) * ymm31 + ymm8, store result in ymm8
 * Broadcast the scalar value at memory location rbx + 2*r9 to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm10, store result in ymm10
 * Broadcast the scalar value at memory location rbx + r13 to all elements of ymm31
 * Fused multiply-add: ymm(s1) * ymm31 + ymm12, store result in ymm12
 * Increment the second pointer of B by rs_b
 * Increment B by rs_b
 */
#define K_LOOP_4x4_SET1(s1)                 \
	vbroadcastsd( mem(rbx), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30,ymm6 )      \
	vbroadcastsd( mem(rbx, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm8 )     \
	vbroadcastsd( mem(rbx, r9, 2), ymm30 )  \
	vfmadd231pd( ymm(s1), ymm30, ymm10 )    \
	vbroadcastsd( mem(rbx, r13, 1), ymm31 ) \
	vfmadd231pd( ymm(s1), ymm31, ymm12 )    \
	add( r8, rbx )

#define K_LOOP_4x4_SET2(s1)                 \
	vbroadcastsd( mem(rbx), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm7 )     \
	vbroadcastsd( mem(rbx, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm9 )     \
	vbroadcastsd( mem(rbx, r9, 2), ymm30 )  \
	vfmadd231pd( ymm(s1), ymm30, ymm11 )    \
	vbroadcastsd( mem(rbx, r13, 1), ymm31 ) \
	vfmadd231pd( ymm(s1), ymm31, ymm13 )    \
	add( r8, rbx )

/**
 * K_LOOP_2x4_SET1
 * Move and duplicate the scalar value at memory location rbx to all elements of xmm30
 * Move and duplicate the scalar value at memory location rbx + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm6, store result in xmm6
 * Move and duplicate the scalar value at memory location rbx + 2*r9 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm8, store result in xmm8
 * Move and duplicate the scalar value at memory location rbx + r13 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm10, store result in xmm10
 * Fused multiply-add: xmm(s1) * xmm31 + xmm12, store result in xmm12
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_2x4_SET1(s1)                 \
	vmovddup ( mem(rbx), xmm30 )            \
	vmovddup ( mem(rbx, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm6 )     \
	vmovddup ( mem(rbx, r9, 2), xmm30 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm8 )     \
	vmovddup ( mem(rbx, r13, 1), xmm31 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm10 )    \
	vfmadd231pd( xmm(s1), xmm31, xmm12 )    \
	add( r8, rbx )

#define K_LOOP_2x4_SET2(s1)                 \
	vmovddup ( mem(rbx), xmm30 )            \
	vmovddup ( mem(rbx, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm7 )     \
	vmovddup ( mem(rbx, r9, 2), xmm30 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm9 )     \
	vmovddup ( mem(rbx, r13, 1), xmm31 )    \
	vfmadd231pd( xmm(s1), xmm30, xmm11 )    \
	vfmadd231pd( xmm(s1), xmm31, xmm13 )    \
	add( r8, rbx )

/**
 * K_LOOP_1x4_SET1
 * Move and duplicate the scalar value at memory location rbx to all elements of xmm30
 * Move and duplicate the scalar value at memory location rbx + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm6, store result in xmm6
 * Move and duplicate the scalar value at memory location rbx + 2*r9 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm8, store result in xmm8
 * Move and duplicate the scalar value at memory location rbx + r13 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm10, store result in xmm10
 * Fused multiply-add: xmm(s1) * xmm31 + xmm12, store result in xmm12
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_1x4_SET1(s1)                 \
	vmovddup( mem(rbx), xmm30 )             \
	vmovddup( mem(rbx, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm6 )     \
	vmovddup( mem(rbx, r9, 2), xmm30 )      \
	vfmadd231pd( xmm(s1), xmm31, xmm8 )     \
	vmovddup( mem(rbx, r13, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm10 )    \
	vfmadd231pd( xmm(s1), xmm31, xmm12 )    \
	add( r8, rbx )

#define K_LOOP_1x4_SET2(s1)                 \
	vmovddup( mem(rbx), xmm30 )             \
	vmovddup( mem(rbx, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm7 )     \
	vmovddup( mem(rbx, r9, 2), xmm30 )      \
	vfmadd231pd( xmm(s1), xmm31, xmm9 )     \
	vmovddup( mem(rbx, r13, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm11 )    \
	vfmadd231pd( xmm(s1), xmm31, xmm13 )    \
	add( r8, rbx )

/**
 * K_LOOP_8MASKx4_SET1
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx) + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r9) + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + 2*r9) + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r13) + zmm12, store result in zmm12
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_8MASKx4_SET1(s1)                             \
	vfmadd231pd( mem_1to8(rbx), zmm(s1),zmm6 )              \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm8 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm10 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm12 )    \
	add( r8, rbx )

#define K_LOOP_8MASKx4_SET2(s1)                             \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm7 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm9 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm11 )     \
	vfmadd231pd( mem_1to8(rbx, r13, 1), zmm(s1), zmm13 )    \
	add( r8,rbx )
/*********************End of 24x4 kernel family K-LOOP macros*********************/

/*********************Start of 24x3 kernel family K-LOOP macros*******************/
/**
 * K_LOOP_24x3_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm28, store result in zmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm29, store result in zmm29
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm26, store result in zmm26

 * Increment the second pointer of B by rs_b
 * Increment B by rs_b
 */
#define K_LOOP_24x3_SET1(s1, s2, s3)        \
    vbroadcastsd( mem(rbx), zmm30 )         \
    vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
    vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
    vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
    vfmadd231pd( zmm(s3), zmm30, zmm28 )    \
    vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
    vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
    vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
    vfmadd231pd( zmm(s3), zmm31, zmm29 )    \
    vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
    vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
    vfmadd231pd( zmm(s3), zmm30, zmm26 )    \
    add( r8, rbx )

#define K_LOOP_24x3_SET2(s1, s2, s3)            \
    vbroadcastsd( mem(rbx), zmm30 )             \
    vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
    vfmadd231pd( zmm(s1), zmm30, zmm12 )        \
    vfmadd231pd( zmm(s2), zmm30, zmm13 )        \
    vfmadd231pd( zmm(s3), zmm30, zmm14 )        \
    vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
    vfmadd231pd( zmm(s1), zmm31, zmm15 )        \
    vfmadd231pd( zmm(s2), zmm31, zmm16 )        \
    vfmadd231pd( zmm(s3), zmm31, zmm17 )        \
    vfmadd231pd( zmm(s1), zmm30, zmm18 )        \
    vfmadd231pd( zmm(s2), zmm30, zmm19 )        \
    vfmadd231pd( zmm(s3), zmm30, zmm20 )        \
    add( r8, rbx )

/**
 * K_LOOP_20x3_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: ymm(s3) * ymm30 + ymm28, store result in ymm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: ymm(s3) * ymm31 + ymm29, store result in ymm29
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: ymm(s3) * ymm30 + ymm26, store result in ymm26
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_20x3_SET1(s1, s2, s3)            \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
	vfmadd231pd( ymm(s3), ymm30, ymm28 )        \
                                                \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )         \
	vfmadd231pd( ymm(s3), ymm31, ymm29 )        \
                                                \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
	vfmadd231pd( ymm(s3), ymm30, ymm26 )        \
                                                \
	add( r8, rbx )

#define K_LOOP_20x3_SET2(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm13 )    \
	vfmadd231pd( ymm(s3), ymm30, ymm14 )    \
                                            \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm15 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm16 )    \
	vfmadd231pd( ymm(s3), ymm31, ymm17 )    \
                                            \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )    \
	vfmadd231pd( ymm(s3), ymm30, ymm20 )    \
                                            \
	add( r8, rbx )

/**
 * K_LOOP_18x3_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm29, store result in xmm29
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm30 + xmm26, store result in xmm26
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_18x3_SET1(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( xmm(s3), xmm30, xmm28 )    \
                                            \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( xmm(s3), xmm31, xmm29 )    \
                                            \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm26 )    \
                                            \
	add( r8, rbx )

#define K_LOOP_18x3_SET2(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm13 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm14 )    \
                                            \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm15 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm16 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm17 )    \
                                            \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm20 )    \
                                            \
	add( r8, rbx )

/**
 * K_LOOP_17x3_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm29, store result in xmm29
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm30 + xmm26, store result in xmm26
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_17x3_SET1(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( xmm(s3), xmm30, xmm28 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( xmm(s3), xmm31, xmm29 )    \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm26 )    \
	add( r8, rbx )

#define K_LOOP_17x3_SET2(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm13 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm14 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm15 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm16 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm17 )    \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm20 )    \
	add( r8, rbx )

/**
 * K_LOOP_16x3_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_16x3_SET1(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	add( r8, rbx )

#define K_LOOP_16x3_SET2(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm13 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm14 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm15 )    \
	vfmadd231pd( zmm(s1), zmm30, zmm16 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm17 )    \
	add( r8, rbx )

/**
 * K_LOOP_24MASKx3
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm28, store result in zmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm29, store result in zmm29
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm26, store result in zmm26
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_24MASKx3_SET1(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )         \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )         \
	vfmadd231pd( zmm(s3), zmm30, zmm28 )        \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )         \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )         \
	vfmadd231pd( zmm(s3), zmm31, zmm29 )        \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )        \
	vfmadd231pd( zmm(s3), zmm30, zmm26 )        \
	add( r8, rbx )

#define K_LOOP_24MASKx3_SET2(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm12 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm13 )        \
	vfmadd231pd( zmm(s3), zmm30, zmm14 )        \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm15 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm16 )        \
	vfmadd231pd( zmm(s3), zmm31, zmm17 )        \
	vfmadd231pd( zmm(s1), zmm30, zmm18 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm19 )        \
	vfmadd231pd( zmm(s3), zmm30, zmm20 )        \
	add( r8, rbx )

/**
 * K_LOOP_12x3
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm30 + ymm7, store result in ymm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm31 + ymm9, store result in ymm9
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm30 + ymm11, store result in ymm11
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_12x3_SET1(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( ymm(s2), ymm30, ymm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( ymm(s2), ymm31, ymm9 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( ymm(s2), ymm30, ymm11 )    \
	add( r8, rbx )

#define K_LOOP_12x3_SET2(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm12 )    \
	vfmadd231pd( ymm(s2), ymm30, ymm13 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm14 )    \
	vfmadd231pd( ymm(s2), ymm31, ymm15 )    \
	vfmadd231pd( zmm(s1), zmm30, zmm16 )    \
	vfmadd231pd( ymm(s2), ymm30, ymm17 )    \
	add( r8, rbx )

/**
 * K_LOOP_11x3_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm1 * xmm30 + xmm7, store result in xmm7
 * Fused multiply-add: xmm2 * xmm30 + xmm28, store result in xmm28
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm0 * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm1 * xmm31 + xmm9, store result in xmm9
 * Fused multiply-add: xmm2 * xmm31 + xmm29, store result in xmm29
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm1 * xmm30 + xmm11, store result in xmm11
 * Fused multiply-add: xmm2 * xmm30 + xmm26, store result in xmm26
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_11x3_SET1(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm0, zmm30, zmm6 )        \
	vfmadd231pd( xmm1, xmm30, xmm7 )        \
	vfmadd231pd( xmm2, xmm30, xmm28 )       \
                                            \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm0, zmm31, zmm8 )        \
	vfmadd231pd( xmm1, xmm31, xmm9 )        \
	vfmadd231pd( xmm2, xmm31, xmm29 )       \
                                            \
	vfmadd231pd( zmm0, zmm30, zmm10 )       \
	vfmadd231pd( xmm1, xmm30, xmm11 )       \
	vfmadd231pd( xmm2, xmm30, xmm26 )       \
                                            \
	add( r8, rbx )

#define K_LOOP_11x3_SET2(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm0, zmm30, zmm12 )       \
	vfmadd231pd( xmm1, xmm30, xmm13 )       \
	vfmadd231pd( xmm2, xmm30, xmm14 )       \
                                            \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm0, zmm31, zmm15 )       \
	vfmadd231pd( xmm1, xmm31, xmm16 )       \
	vfmadd231pd( xmm2, xmm31, xmm17 )       \
                                            \
	vfmadd231pd( zmm0, zmm30, zmm18 )       \
	vfmadd231pd( xmm1, xmm30, xmm19 )       \
	vfmadd231pd( xmm2, xmm30, xmm20 )       \
                                            \
	add( r8, rbx )

/**
 * K_LOOP_10x3_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm(s2) * xmm30 + xmm7, store result in xmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm(s2) * xmm31 + xmm9, store result in xmm9
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm(s2) * xmm30 + xmm11, store result in xmm11
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_10x3_SET1(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( xmm(s2), xmm30, xmm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( xmm(s2), xmm31, xmm9 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm11 )    \
	add( r8, rbx )

#define K_LOOP_10x3_SET2(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm12 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm13 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm14 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm15 )    \
	vfmadd231pd( zmm(s1), zmm30, zmm16 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm17 )    \
	add( r8, rbx )

/**
 * K_LOOP_9x3_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm(s2) * xmm30 + xmm7, store result in xmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm(s2) * xmm31 + xmm9, store result in xmm9
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: xmm(s2) * xmm30 + xmm11, store result in xmm11
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_9x3_SET1(s1, s2)             \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( xmm(s2), xmm30, xmm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( xmm(s2), xmm31, xmm9 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm11 )    \
	add( r8, rbx )

#define K_LOOP_9x3_SET2(s1, s2)             \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm12 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm13 )    \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm14 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm15 )    \
	vfmadd231pd( zmm(s1), zmm30, zmm16 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm17 )    \
	add( r8, rbx )

/**
 * K_LOOP_8x3_SET1
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx) + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r9) + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + 2*r9) + zmm10, store result in zmm10
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_8x3_SET1(s1)                             \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm6 )         \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm8 )  \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm10 ) \
	add( r8, rbx )

#define K_LOOP_8x3_SET2(s1)                             \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm7 )         \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm9 )  \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm11 ) \
	add( r8, rbx )

/**
 * K_LOOP_16MASKx3_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 *
 * Broadcast the scalar value at memory location rbx + 2*r9(b[0, 2]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm10, store result in zmm10
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm11, store result in zmm11
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_16MASKx3_SET1(s1, s2)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )  \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	add( r8,rbx )

#define K_LOOP_16MASKx3_SET2(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )             \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )      \
	vfmadd231pd( zmm(s1), zmm30, zmm12 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm13 )        \
	vbroadcastsd( mem(rbx, r9, 2), zmm30 )      \
	vfmadd231pd( zmm(s1), zmm31, zmm14 )        \
	vfmadd231pd( zmm(s2), zmm31, zmm15 )        \
	vfmadd231pd( zmm(s1), zmm30, zmm16 )        \
	vfmadd231pd( zmm(s2), zmm30, zmm17 )        \
	add( r8, rbx )

/**
 * K_LOOP_4x3_SET1
 * Broadcast the scalar value at memory location rbx to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm6, store result in ymm6
 * Broadcast the scalar value at memory location rbx + r9 to all elements of ymm31
 * Fused multiply-add: ymm(s1) * ymm31 + ymm8, store result in ymm8
 * Broadcast the scalar value at memory location rbx + 2*r9 to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm10, store result in ymm10
 * Increment the second pointer of B by rs_b
 * Increment B by rs_b
 */
#define K_LOOP_4x3_SET1(s1)                 \
	vbroadcastsd( mem(rbx), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm6 )     \
	vbroadcastsd( mem(rbx, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm8 )     \
	vbroadcastsd( mem(rbx, r9, 2), ymm30 )  \
	vfmadd231pd( ymm(s1), ymm30, ymm10 )    \
	add( r8, rbx )

#define K_LOOP_4x3_SET2(s1)                 \
	vbroadcastsd( mem(rbx), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm7 )     \
	vbroadcastsd( mem(rbx, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm9 )     \
	vbroadcastsd( mem(rbx, r9, 2), ymm30 )  \
	vfmadd231pd( ymm(s1), ymm30, ymm11 )    \
	add( r8, rbx )

/**
 * K_LOOP_2x3_SET1
 * Move and duplicate the scalar value at memory location rbx to all elements of xmm30
 * Move and duplicate the scalar value at memory location rbx + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm6, store result in xmm6
 * Move and duplicate the scalar value at memory location rbx + 2*r9 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm8, store result in xmm8
 * Fused multiply-add: xmm(s1) * xmm30 + xmm10, store result in xmm10
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_2x3_SET1(s1)                 \
	vmovddup ( mem(rbx), xmm30 )            \
	vmovddup ( mem(rbx, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm6 )     \
	vmovddup ( mem(rbx, r9, 2), xmm30 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm8 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm10 )    \
	add( r8, rbx )

#define K_LOOP_2x3_SET2(s1)                 \
	vmovddup ( mem(rbx), xmm30 )            \
	vmovddup ( mem(rbx, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm7 )     \
	vmovddup ( mem(rbx, r9, 2), xmm30 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm9 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm11 )    \
	add( r8, rbx )

/**
 * K_LOOP_1x3_SET1
 * Move and duplicate the scalar value at memory location rbx to all elements of xmm30
 * Move and duplicate the scalar value at memory location rbx + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm6, store result in xmm6
 * Move and duplicate the scalar value at memory location rbx + 2*r9 to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm31 + xmm8, store result in xmm8
 * Fused multiply-add: xmm(s1) * xmm30 + xmm10, store result in xmm10
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_1x3_SET1(s1)                 \
	vmovddup( mem(rbx), xmm30 )             \
	vmovddup( mem(rbx, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm6 )     \
	vmovddup( mem(rbx, r9, 2), xmm30 )      \
	vfmadd231pd( xmm(s1), xmm31, xmm8 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm10 )    \
	add( r8, rbx )

#define K_LOOP_1x3_SET2(s1)                 \
	vmovddup( mem(rbx), xmm30 )             \
	vmovddup( mem(rbx, r9, 1), xmm31 )      \
	vfmadd231pd( xmm(s1), xmm30, xmm7 )     \
	vmovddup( mem(rbx, r9, 2), xmm30 )      \
	vfmadd231pd( xmm(s1), xmm31, xmm9 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm11 )    \
	add( r8, rbx )

/**
 * K_LOOP_8MASKx3_SET1
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx) + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r9) + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + 2*r9) + zmm10, store result in zmm10
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_8MASKx3_SET1(s1)                             \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm6 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm8 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm10 )     \
	add( r8, rbx )

#define K_LOOP_8MASKx3_SET2(s1)                             \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm7 )             \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm9 )      \
	vfmadd231pd( mem_1to8(rbx, r9, 2), zmm(s1), zmm11 )     \
	add( r8, rbx )
/*********************End of 24x3 kernel family K-LOOP macros*********************/

/*********************Start of 24x2 kernel family K-LOOP macros*******************/
/**
 * K_LOOP_24x2_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm28, store result in zmm28
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm29, store result in zmm29

 * Increment the second pointer of B by rs_b
 * Increment B by rs_b
 */
#define K_LOOP_24x2_SET1(s1, s2, s3)        \
    vbroadcastsd( mem(rbx), zmm30 )         \
    vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
    vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
    vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
    vfmadd231pd( zmm(s3), zmm30, zmm28 )    \
    vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
    vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
    vfmadd231pd( zmm(s3), zmm31, zmm29 )    \
    add( r8, rbx )

#define K_LOOP_24x2_SET2(s1, s2, s3)        \
    vbroadcastsd( mem(rbx), zmm30 )         \
    vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
    vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
    vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
    vfmadd231pd( zmm(s3), zmm30, zmm12 )    \
    vfmadd231pd( zmm(s1), zmm31, zmm13 )    \
    vfmadd231pd( zmm(s2), zmm31, zmm14 )    \
    vfmadd231pd( zmm(s3), zmm31, zmm15 )    \
    add( r8, rbx )

/**
 * K_LOOP_20x2_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: ymm(s3) * ymm30 + ymm28, store result in ymm28
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: ymm(s3) * ymm31 + ymm29, store result in ymm29
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_20x2_SET1(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( ymm(s3), ymm30, ymm28 )    \
                                            \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( ymm(s3), ymm31, ymm29 )    \
                                            \
	add( r8, rbx )

#define K_LOOP_20x2_SET2(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( ymm(s3), ymm30, ymm12 )    \
                                            \
	vfmadd231pd( zmm(s1), zmm31, zmm13 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm14 )    \
	vfmadd231pd( ymm(s3), ymm31, ymm15 )    \
                                            \
	add( r8, rbx )

/**
 * K_LOOP_18x2_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm28, store result in xmm28
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm29, store result in xmm29
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_18x2_SET1(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( xmm(s3), xmm30, xmm28 )    \
                                            \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( xmm(s3), xmm31, xmm29 )    \
                                            \
	add( r8, rbx )

#define K_LOOP_18x2_SET2(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm12 )    \
                                            \
	vfmadd231pd( zmm(s1), zmm31, zmm13 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm14 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm15 )    \
                                            \
	add( r8, rbx )

/**
 * K_LOOP_17x2_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm28, store result in xmm28
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: xmm(s3)(a[16:17]) * xmm31 + xmm29, store result in xmm29
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_17x2_SET1(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( xmm(s3), xmm30, xmm28 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( xmm(s3), xmm31, xmm29 )    \
	add( r8, rbx )

#define K_LOOP_17x2_SET2(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( xmm(s3), xmm30, xmm12 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm13 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm14 )    \
	vfmadd231pd( xmm(s3), xmm31, xmm15 )    \
	add( r8, rbx )

/**
 * K_LOOP_16x2_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_16x2_SET1(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	add( r8, rbx )

#define K_LOOP_16x2_SET2(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	add( r8, rbx )

/**
 * K_LOOP_24MASKx2
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm28, store result in zmm28
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm31 + zmm29, store result in zmm29
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_24MASKx2_SET1(s1, s2, s3)    \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( zmm(s3), zmm30, zmm28 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	vfmadd231pd( zmm(s3), zmm31, zmm29 )    \
	add( r8, rbx )

#define K_LOOP_24MASKx2_SET2(s1, s2, s3)    \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( zmm(s3), zmm30, zmm12 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm13 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm14 )    \
	vfmadd231pd( zmm(s3), zmm31, zmm15 )    \
	add( r8, rbx )

/**
 * K_LOOP_12x2
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm30 + ymm7, store result in ymm7
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm31 + ymm9, store result in ymm9
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_12x2_SET1(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( ymm(s2), ymm30, ymm7 )     \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( ymm(s2), ymm31, ymm9 )     \
	add( r8, rbx )

#define K_LOOP_12x2_SET2(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( ymm(s2), ymm30, ymm11 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( ymm(s2), ymm31, ymm13 )    \
	add( r8, rbx )

/**
 * K_LOOP_11x2_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm1 * xmm30 + xmm7, store result in xmm7
 * Fused multiply-add: xmm2 * xmm30 + xmm28, store result in xmm28
 *
 * Fused multiply-add: zmm0 * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm1 * xmm31 + xmm9, store result in xmm9
 * Fused multiply-add: xmm2 * xmm31 + xmm29, store result in xmm29
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_11x2_SET1(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm0, zmm30, zmm6 )        \
	vfmadd231pd( xmm1, xmm30, xmm7 )        \
	vfmadd231pd( xmm2, xmm30, xmm28 )       \
                                            \
	vfmadd231pd( zmm0, zmm31, zmm8 )        \
	vfmadd231pd( xmm1, xmm31, xmm9 )        \
	vfmadd231pd( xmm2, xmm31, xmm29 )       \
                                            \
	add( r8, rbx )

#define K_LOOP_11x2_SET2(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm0, zmm30, zmm10 )       \
	vfmadd231pd( xmm1, xmm30, xmm11 )       \
	vfmadd231pd( xmm2, xmm30, xmm12 )       \
                                            \
	vfmadd231pd( zmm0, zmm31, zmm13 )       \
	vfmadd231pd( xmm1, xmm31, xmm14 )       \
	vfmadd231pd( xmm2, xmm31, xmm15 )       \
                                            \
	add( r8, rbx )

/**
 * K_LOOP_10x2_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm(s2) * xmm30 + xmm7, store result in xmm7
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm(s2) * xmm31 + xmm9, store result in xmm9
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_10x2_SET1(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( xmm(s2), xmm30, xmm7 )     \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( xmm(s2), xmm31, xmm9 )     \
	add( r8, rbx )

#define K_LOOP_10x2_SET2(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm11 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm13 )    \
	add( r8, rbx )

/**
 * K_LOOP_9x2_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm(s2) * xmm30 + xmm7, store result in xmm7
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: xmm(s2) * xmm31 + xmm9, store result in xmm9
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_9x2_SET1(s1, s2)             \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( xmm(s2), xmm30, xmm7 )     \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( xmm(s2), xmm31, xmm9 )     \
	add( r8, rbx )

#define K_LOOP_9x2_SET2(s1, s2)             \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( xmm(s2), xmm30, xmm11 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( xmm(s2), xmm31, xmm13 )    \
	add( r8, rbx )

/**
 * K_LOOP_8x2_SET1
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx) + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r9) + zmm8, store result in zmm8
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_8x2_SET1(s1)                             \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm6 )         \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm8 )  \
	add( r8, rbx )

#define K_LOOP_8x2_SET2(s1)                             \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm7 )         \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm9 )  \
	add( r8, rbx )

/**
 * K_LOOP_16MASKx2_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 * Broadcast the scalar value at memory location rbx + r9(b[0, 1]) to all elements of zmm31
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm31 + zmm8, store result in zmm8
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm31 + zmm9, store result in zmm9
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_16MASKx2_SET1(s1, s2)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( zmm(s1), zmm31, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm31, zmm9 )     \
	add( r8, rbx )

#define K_LOOP_16MASKx2_SET2(s1, s2)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vbroadcastsd( mem(rbx, r9, 1), zmm31 )  \
	vfmadd231pd( zmm(s1), zmm30, zmm10 )    \
	vfmadd231pd( zmm(s2), zmm30, zmm11 )    \
	vfmadd231pd( zmm(s1), zmm31, zmm12 )    \
	vfmadd231pd( zmm(s2), zmm31, zmm13 )    \
	add( r8, rbx )

/**
 * K_LOOP_4x2_SET1
 * Broadcast the scalar value at memory location rbx to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm6, store result in ymm6
 * Broadcast the scalar value at memory location rbx + r9 to all elements of ymm31
 * Fused multiply-add: ymm(s1) * ymm31 + ymm8, store result in ymm8
 * Increment the second pointer of B by rs_b
 * Increment B by rs_b
 */
#define K_LOOP_4x2_SET1(s1)                 \
	vbroadcastsd( mem(rbx), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm6 )     \
	vbroadcastsd( mem(rbx, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm8 )     \
	add( r8, rbx )

#define K_LOOP_4x2_SET2(s1)                 \
	vbroadcastsd( mem(rbx), ymm30 )         \
	vfmadd231pd( ymm(s1), ymm30, ymm7 )     \
	vbroadcastsd( mem(rbx, r9, 1), ymm31 )  \
	vfmadd231pd( ymm(s1), ymm31, ymm9 )     \
	add( r8, rbx )

/**
 * K_LOOP_2x2_SET1
 * Move and duplicate the scalar value at memory location rbx to all elements of xmm30
 * Move and duplicate the scalar value at memory location rbx + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm6, store result in xmm6
 * Fused multiply-add: xmm(s1) * xmm31 + xmm8, store result in xmm8
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_2x2_SET1(s1)                 \
	vmovddup ( mem(rbx), xmm30 )            \
	vmovddup ( mem(rbx, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm6 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm8 )     \
	add( r8, rbx )

#define K_LOOP_2x2_SET2(s1)                 \
	vmovddup ( mem(rbx), xmm30 )            \
	vmovddup ( mem(rbx, r9, 1), xmm31 )     \
	vfmadd231pd( xmm(s1), xmm30, xmm7 )     \
	vfmadd231pd( xmm(s1), xmm31, xmm9 )     \
	add( r8, rbx )

/**
 * K_LOOP_1x2_SET1
 * Move and duplicate the scalar value at memory location rbx to all elements of xmm30
 * Move and duplicate the scalar value at memory location rbx + r9 to all elements of xmm31
 * Fused multiply-add: xmm(s1) * xmm30 + xmm6, store result in xmm6
 * Fused multiply-add: xmm(s1) * xmm31 + xmm8, store result in xmm8
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_1x2_SET1(s1)             \
	vmovddup( mem(rbx), xmm30 )         \
	vmovddup( mem(rbx, r9, 1), xmm31 )  \
	vfmadd231pd( xmm(s1), xmm30, xmm6 ) \
	vfmadd231pd( xmm(s1), xmm31, xmm8 ) \
	add( r8, rbx )

#define K_LOOP_1x2_SET2(s1)             \
	vmovddup( mem(rbx), xmm30 )         \
	vmovddup( mem(rbx, r9, 1), xmm31 )  \
	vfmadd231pd( xmm(s1), xmm30, xmm7 ) \
	vfmadd231pd( xmm(s1), xmm31, xmm9 ) \
	add( r8, rbx )

/**
 * K_LOOP_8MASKx3_SET1
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx) + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx + r9) + zmm8, store result in zmm8
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_8MASKx2_SET1(s1)                         \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm6 )         \
	vfmadd231pd( mem_1to8(rbx, r9, 1), zmm(s1), zmm8 )  \
	add( r8, rbx )

#define K_LOOP_8MASKx2_SET2(s1)                         \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm7 )         \
	vfmadd231pd( mem_1to8(rbx, r9, 1),zmm(s1), zmm9 )   \
	add( r8, rbx )
/*********************End of 24x2 kernel family K-LOOP macros*********************/

/*********************Start of 24x1 kernel family K-LOOP macros*******************/
/**
 * K_LOOP_24x1_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm28, store result in zmm28
 * Increment the second pointer of B by rs_b
 * Increment B by rs_b
 */
#define K_LOOP_24x1_SET1(s1, s2, s3)        \
    vbroadcastsd( mem(rbx), zmm30 )         \
    vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
    vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
    vfmadd231pd( zmm(s3), zmm30, zmm28 )    \
    add( r8, rbx )

#define K_LOOP_24x1_SET2(s1, s2, s3)        \
    vbroadcastsd( mem(rbx), zmm30 )         \
    vfmadd231pd( zmm(s1), zmm30, zmm8 )     \
    vfmadd231pd( zmm(s2), zmm30, zmm9 )     \
    vfmadd231pd( zmm(s3), zmm30, zmm29 )    \
    add( r8, rbx )

/**
 * K_LOOP_20x1_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: ymm(s3) * ymm30 + ymm28, store result in ymm28
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_20x1_SET1(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( ymm(s3), ymm30, ymm28 )    \
	add( r8, rbx )

#define K_LOOP_20x1_SET2(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm30, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm9 )     \
	vfmadd231pd( ymm(s3), ymm30, ymm29 )    \
	add( r8, rbx )

/**
 * K_LOOP_18x1_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm28, store result in xmm28
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_18x1_SET1(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( xmm(s3), xmm30, xmm28 )    \
\
	add( r8,rbx )

#define K_LOOP_18x1_SET2(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm30, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm9 )     \
	vfmadd231pd( xmm(s3), xmm30, xmm29 )    \
	add( r8, rbx )

/**
 * K_LOOP_17x1_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: xmm(s3) (a[16:17])* xmm30 + xmm28, store result in xmm28
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_17x1_SET1(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( xmm(s3), xmm30, xmm28 )    \
	add( r8, rbx )

#define K_LOOP_17x1_SET2(s1, s2, s3)        \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm30, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm9 )     \
	vfmadd231pd( xmm(s3), xmm30, xmm29 )    \
	add( r8, rbx )

/**
 * K_LOOP_16x1_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_16x1_SET1(s1, s2)        \
	vbroadcastsd( mem(rbx), zmm30 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm6 ) \
	vfmadd231pd( zmm(s2), zmm30, zmm7 ) \
	add( r8, rbx )

#define K_LOOP_16x1_SET2(s1, s2)        \
	vbroadcastsd( mem(rbx), zmm30 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm8 ) \
	vfmadd231pd( zmm(s2), zmm30, zmm9 ) \
	add( r8, rbx )

/**
 * K_LOOP_24MASKx1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Fused multiply-add: zmm(s3)(a[15:23]) * zmm30 + zmm28, store result in zmm28
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_24MASKx1_SET1(s1, s2, s3)    \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm7 )     \
	vfmadd231pd( zmm(s3), zmm30, zmm28 )    \
	add( r8, rbx )

#define K_LOOP_24MASKx1_SET2(s1, s2, s3)    \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm30, zmm8 )     \
	vfmadd231pd( zmm(s2), zmm30, zmm9 )     \
	vfmadd231pd( zmm(s3), zmm30, zmm29 )    \
	add( r8, rbx )

/**
 * K_LOOP_12x1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: ymm(s2)(a[8:11]) * ymm30 + ymm7, store result in ymm7
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_12x1_SET1(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm30, zmm6 )     \
	vfmadd231pd( ymm(s2), ymm30, ymm7 )     \
	add( r8, rbx )

#define K_LOOP_12x1_SET2(s1, s2)            \
	vbroadcastsd( mem(rbx), zmm30 )         \
	vfmadd231pd( zmm(s1), zmm30, zmm8 )     \
	vfmadd231pd( ymm(s2), ymm30, ymm9 )     \
	add( r8, rbx )

/**
 * K_LOOP_11x1_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 *
 * Fused multiply-add: zmm0 * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm1 * xmm30 + xmm7, store result in xmm7
 * Fused multiply-add: xmm2 * xmm30 + xmm28, store result in xmm28
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_11x1_SET1(s1, s2, s3)    \
	vbroadcastsd( mem(rbx), zmm30 )     \
	vfmadd231pd( zmm0, zmm30, zmm6 )    \
	vfmadd231pd( xmm1, xmm30, xmm7 )    \
	vfmadd231pd( xmm2, xmm30, xmm28 )   \
	add( r8, rbx )

#define K_LOOP_11x1_SET2(s1, s2, s3)    \
	vbroadcastsd( mem(rbx), zmm30 )     \
	vfmadd231pd( zmm0, zmm30, zmm8 )    \
	vfmadd231pd( xmm1, xmm30, xmm9 )    \
	vfmadd231pd( xmm2, xmm30, xmm29 )   \
	add( r8, rbx )

/**
 * K_LOOP_10x1_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm(s2) * xmm30 + xmm7, store result in xmm7
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_10x1_SET1(s1, s2)        \
	vbroadcastsd( mem(rbx), zmm30 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm6 ) \
	vfmadd231pd( xmm(s2), xmm30, xmm7 ) \
	add( r8, rbx )

#define K_LOOP_10x1_SET2(s1, s2)        \
	vbroadcastsd( mem(rbx), zmm30 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm8 ) \
	vfmadd231pd( xmm(s2), xmm30, xmm9 ) \
	add( r8, rbx )

/**
 * K_LOOP_9x1_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: xmm(s2) * xmm30 + xmm7, store result in xmm7
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_9x1_SET1(s1, s2)         \
	vbroadcastsd( mem(rbx), zmm30 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm6 ) \
	vfmadd231pd( xmm(s2), xmm30, xmm7 ) \
	add( r8, rbx )

#define K_LOOP_9x1_SET2(s1, s2)         \
	vbroadcastsd( mem(rbx), zmm30 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm8 ) \
	vfmadd231pd( xmm(s2), xmm30, xmm9 ) \
	add( r8, rbx )

/**
 * K_LOOP_8x1_SET1
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx) + zmm6, store result in zmm6
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */

#define K_LOOP_8x1_SET1(s1)                     \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm6 ) \
	add( r8, rbx )

#define K_LOOP_8x1_SET2(s1)                     \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm7 ) \
	add( r8, rbx )

/**
 * K_LOOP_16MASKx1_SET1
 * Broadcast the scalar value at memory location rbx(b[0, 0]) to all elements of zmm30
 *
 * Fused multiply-add: zmm(s1)(a[0:7]) * zmm30 + zmm6, store result in zmm6
 * Fused multiply-add: zmm(s2)(a[8:15]) * zmm30 + zmm7, store result in zmm7
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_16MASKx1_SET1(s1, s2)    \
	vbroadcastsd( mem(rbx), zmm30 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm6 ) \
	vfmadd231pd( zmm(s2), zmm30, zmm7 ) \
	add( r8, rbx )

#define K_LOOP_16MASKx1_SET2(s1, s2)    \
	vbroadcastsd( mem(rbx), zmm30 )     \
	vfmadd231pd( zmm(s1), zmm30, zmm8 ) \
	vfmadd231pd( zmm(s2), zmm30, zmm9 ) \
	add( r8, rbx )

/**
 * K_LOOP_4x1_SET1
 * Broadcast the scalar value at memory location rbx to all elements of ymm30
 * Fused multiply-add: ymm(s1) * ymm30 + ymm6, store result in ymm6
 * Increment the second pointer of B by rs_b
 * Increment B by rs_b
 */
#define K_LOOP_4x1_SET1(s1)             \
	vbroadcastsd( mem(rbx), ymm30 )     \
	vfmadd231pd( ymm(s1), ymm30, ymm6 ) \
	add( r8, rbx )

#define K_LOOP_4x1_SET2(s1)             \
	vbroadcastsd( mem(rbx), ymm30 )     \
	vfmadd231pd( ymm(s1), ymm30, ymm7 ) \
	add( r8, rbx )

/**
 * K_LOOP_2x1_SET1
 * Move and duplicate the scalar value at memory location rbx to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm30 + xmm6, store result in xmm6
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_2x1_SET1(s1)             \
	vmovddup ( mem(rbx), xmm30 )        \
	vfmadd231pd( xmm(s1), xmm30, xmm6 ) \
	add( r8, rbx )

#define K_LOOP_2x1_SET2(s1)             \
	vmovddup ( mem(rbx), xmm30 )        \
	vfmadd231pd( xmm(s1), xmm30, xmm7 ) \
	add( r8, rbx )

/**
 * K_LOOP_1x1_SET1
 * Move and duplicate the scalar value at memory location rbx to all elements of xmm30
 * Fused multiply-add: xmm(s1) * xmm30 + xmm6, store result in xmm6
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_1x1_SET1(s1)             \
	vmovddup( mem(rbx), xmm30 )         \
	vfmadd231pd( xmm(s1), xmm30, xmm6 ) \
	add( r8, rbx )

#define K_LOOP_1x1_SET2(s1)             \
	vmovddup( mem(rbx), xmm30 )         \
	vfmadd231pd( xmm(s1), xmm30, xmm7 ) \
	add( r8, rbx )

/**
 * K_LOOP_8MASKx1_SET1
 * Fused multiply-add: zmm(s1) * mem_1to8(rbx) + zmm6, store result in zmm6
 * Increment B by rs_b
 * Increment the second pointer of B by rs_b
 */
#define K_LOOP_8MASKx1_SET1(s1)                 \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm6 ) \
	add( r8, rbx )

#define K_LOOP_8MASKx1_SET2(s1)                 \
	vfmadd231pd( mem_1to8(rbx), zmm(s1), zmm7 ) \
	add( r8, rbx )
/*********************End of 24x1 kernel family K-LOOP macros*********************/

#define UNROLL_K_LOOP_24x8_L1                                       \
                                /* column 0 */                      \
    vmovupd( mem(rax), zmm0 )   /* zmm0 = a(0:7) */                 \
    vmovupd( 0x40(rax), zmm1 )  /* zmm1 = a(8:15) */                \
    vmovupd( 0x80(rax), zmm2 )  /* zmm2 = a(16:23) */               \
    add( r10, rax )             /* rax = next column of a matrix */ \
                                                                    \
                                /* column 1 */                      \
    vmovupd( mem(rax), zmm3 )   /* zmm3 = a(0:7) */                 \
    vmovupd( 0x40(rax), zmm4 )  /* zmm4 = a(8:15) */                \
    vmovupd( 0x80(rax), zmm5 )  /* zmm5 = a(16:23) */               \
    add( r10, rax )             /* rax = next column of a matrix */ \
                                                                    \
    K_LOOP_PREFETCH_A                                               \
    prefetch( 0, mem(r11) )                                         \
    K_LOOP_24x8(0, 1, 2)                                            \
                                                                    \
                                /* column 2 */                      \
    vmovupd( mem(rax), zmm0 )   /* zmm0 = a(0:7) */                 \
    vmovupd( 0x40(rax), zmm1 )  /* zmm1 = a(8:15) */                \
    vmovupd( 0x80(rax), zmm2 )  /* zmm2 = a(15:23) */               \
    add( r10, rax )             /* rax = next column of a matrix */ \
                                                                    \
    K_LOOP_PREFETCH_A                                               \
    prefetch( 0, mem(r11, r9, 1) )                                  \
    K_LOOP_24x8(3, 4, 5)                                            \
                                                                    \
                                /* column 3 */                      \
    vmovupd( mem(rax), zmm3 )   /* zmm3 = a(0:7) */                 \
    vmovupd( 0x40(rax), zmm4 )  /* zmm4 = a(8:15) */                \
    vmovupd( 0x80(rax), zmm5 )  /* zmm5 = a(15:23) */               \
    add( r10, rax )             /* rax = next column of a matrix */ \
                                                                    \
    K_LOOP_PREFETCH_A                                               \
    prefetch( 0, mem(r11, r9, 2) )                                  \
    K_LOOP_24x8(0, 1, 2)                                            \
                                                                    \
                                /* column 4 */                      \
    vmovupd( mem(rax), zmm0 )   /* zmm0 = a(0:7) */                 \
    vmovupd( 0x40(rax), zmm1 )  /* zmm1 = a(8:15) */                \
    vmovupd( 0x80(rax), zmm2 )  /* zmm2 = a(15:23) */               \
    add( r10, rax )             /* rax = next column of a matrix */ \
                                                                    \
    K_LOOP_PREFETCH_A                                               \
    prefetch( 0, mem(r11, r13, 1) )                                 \
    K_LOOP_24x8(3, 4, 5)                                            \
                                                                    \
                                /* column 5 */                      \
    vmovupd( mem(rax), zmm3 )   /* zmm3 = a(0:7) */                 \
    vmovupd( 0x40(rax), zmm4 )  /* zmm4 = a(8:15) */                \
    vmovupd( 0x80(rax), zmm5 )  /* zmm5 = a(16:23) */               \
    add( r10, rax )             /* rax = next column of a matrix */ \
                                                                    \
    K_LOOP_PREFETCH_A                                               \
    prefetch( 0, mem(r15) )                                         \
    K_LOOP_24x8(0, 1, 2)                                            \
                                                                    \
                                /* column 6 */                      \
    vmovupd( mem(rax), zmm0 )   /* zmm0 = a(0:7) */                 \
    vmovupd( 0x40(rax), zmm1 )  /* zmm1 = a(8:15) */                \
    vmovupd( 0x80(rax), zmm2 )  /* zmm2 = a(16:23) */               \
    add( r10, rax )             /* rax = next column of a matrix */ \
                                                                    \
    K_LOOP_PREFETCH_A                                               \
    prefetch( 0, mem(r15, r9, 1) )                                  \
    K_LOOP_24x8(3, 4, 5)                                            \
                                                                    \
                                /* column 7 */                      \
    vmovupd( mem(rax), zmm3 )   /* zmm3 = a(0:7) */                 \
    vmovupd( 0x40(rax), zmm4 )  /* zmm4 = a(8:15) */                \
    vmovupd( 0x80(rax), zmm5 )  /* zmm5 = a(16:23) */               \
    add( r10, rax )             /* rax = next column of a matrix */ \
                                                                    \
    K_LOOP_PREFETCH_A                                               \
    prefetch( 0, mem(r15, r9, 2) )                                  \
    K_LOOP_24x8(0, 1, 2)                                            \
                                                                    \
    K_LOOP_PREFETCH_A                                               \
    prefetch( 0, mem(r15, r13, 1) )                                 \
    K_LOOP_24x8(3, 4, 5)                                            \
    lea( mem(r11, r8, 8), r11)


#define UNROLL_K_LOOP_24x8_L2                                       \
    /* prefetch C[7:15] matrixin L1 */                              \
    /* prefetchw0 prefetch data into cache */                       \
    /* with the intention of writing to it */                       \
    /* soon. */                                                     \
    prefetchw0( mem(rdx) )                                          \
                                /* column 0 */                      \
    vmovupd( mem(rax), zmm0 )   /* zmm0 = a(0:7) */                 \
    vmovupd( 0x40(rax), zmm1 )  /* zmm1 = a(8:15) */                \
    vmovupd( 0x80(rax), zmm2 )  /* zmm2 = a(16:23) */               \
    add( r10, rax )             /* rax = next column of a matrix */ \
                                                                    \
                                /* column 1 */                      \
    vmovupd( mem(rax), zmm3 )   /* zmm3 = a(0:7) */                 \
    vmovupd( 0x40(rax), zmm4 )  /* zmm4 = a(8:15) */                \
    vmovupd( 0x80(rax), zmm5 )  /* zmm5 = a(16:23) */               \
    add( r10, rax )             /* rax = next column of a matrix */ \
    K_LOOP_PREFETCH_A                                               \
    prefetch( 0, mem(r11) )                                         \
    K_LOOP_24x8(0, 1, 2)                                            \
                                                                    \
    prefetchw0( mem(rdx, 64))   /* prefetch C[16:24] matrixin L1 */ \
                                /* column 2 */                      \
    vmovupd( mem(rax), zmm0 )   /* zmm0 = a(0:7) */                 \
    vmovupd( 0x40(rax), zmm1 )  /* zmm1 = a(8:15) */                \
    vmovupd( 0x80(rax), zmm2 )  /* zmm2 = a(15:23) */               \
    add( r10, rax )             /* rax = next column of a matrix */ \
                                                                    \
    K_LOOP_PREFETCH_A                                               \
    prefetch( 0, mem(r11, r9, 1) )                                  \
    K_LOOP_24x8(3, 4, 5)                                            \
                                                                    \
    prefetchw0( mem(rdx, 128))  /* prefetch C[24:32] matrixin L1 */ \
                                /* column 3 */                      \
    vmovupd( mem(rax), zmm3 )   /* zmm3 = a(0:7) */                 \
    vmovupd( 0x40(rax), zmm4 )  /* zmm4 = a(8:15) */                \
    vmovupd( 0x80(rax), zmm5 )  /* zmm5 = a(15:23) */               \
    add( r10, rax )             /* rax = next column of a matrix */ \
                                                                    \
    K_LOOP_PREFETCH_A                                               \
    prefetch( 0, mem(r11, r9, 2) )                                  \
    K_LOOP_24x8(0, 1, 2)                                            \
                                                                    \
                                /* column 4 */                      \
    vmovupd( mem(rax), zmm0 )   /* zmm0 = a(0:7) */                 \
    vmovupd( 0x40(rax), zmm1 )  /* zmm1 = a(8:15) */                \
    vmovupd( 0x80(rax), zmm2 )  /* zmm2 = a(15:23) */               \
    add( r10, rax )             /* rax = next column of a matrix */ \
                                                                    \
    K_LOOP_PREFETCH_A                                               \
    prefetch( 0, mem(r11, r13, 1) )                                 \
    K_LOOP_24x8(3, 4, 5)                                            \
                                                                    \
                                /* column 5 */                      \
    vmovupd( mem(rax), zmm3 )   /* zmm3 = a(0:7) */                 \
    vmovupd( 0x40(rax), zmm4 )  /* zmm4 = a(8:15) */                \
    vmovupd( 0x80(rax), zmm5 )  /* zmm5 = a(16:23) */               \
    add( r10, rax )             /* rax = next column of a matrix */ \
                                                                    \
    K_LOOP_PREFETCH_A                                               \
    prefetch( 0, mem(r15) )                                         \
    K_LOOP_24x8(0, 1, 2)                                            \
                                                                    \
                                /* column 6 */                      \
    vmovupd( mem(rax), zmm0 )   /* zmm0 = a(0:7) */                 \
    vmovupd( 0x40(rax), zmm1 )  /* zmm1 = a(8:15) */                \
    vmovupd( 0x80(rax), zmm2 )  /* zmm2 = a(16:23) */               \
    add( r10, rax )             /* rax = next column of a matrix */ \
                                                                    \
    K_LOOP_PREFETCH_A                                               \
    prefetch( 0, mem(r15, r9, 1) )                                  \
    K_LOOP_24x8(3, 4, 5)                                            \
                                                                    \
                                /* column 7 */                      \
    vmovupd( mem(rax), zmm3 )   /* zmm3 = a(0:7) */                 \
    vmovupd( 0x40(rax), zmm4 )  /* zmm4 = a(8:15) */                \
    vmovupd( 0x80(rax), zmm5 )  /* zmm5 = a(16:23) */               \
    add( r10, rax )             /* rax = next column of a matrix */ \
                                                                    \
    K_LOOP_PREFETCH_A                                               \
    prefetch( 0, mem(r15, r9, 2) )                                  \
    K_LOOP_24x8(0, 1, 2)                                            \
                                                                    \
    K_LOOP_PREFETCH_A                                               \
    prefetch( 0, mem(r15, r13, 1) )                                 \
    K_LOOP_24x8(3, 4, 5)                                            \
    lea(mem(rdx, rdi, 1), rdx)                                      \
    lea(mem(r11, r8, 8), r11)


#define UNROLL_K_LOOP_20x8_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_20x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_20x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_20x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_20x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_20x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_20x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_20x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_20x8(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)

#define UNROLL_K_LOOP_20x8_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_20x8(0, 1, 2)                        \
                                                \
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_20x8(0, 1, 2)                        \
                                                \
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_20x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_20x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_20x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_20x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_20x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_20x8(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \
	lea(mem(r15, r8, 8), r15)

#define UNROLL_K_LOOP_18x8_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_18x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_18x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_18x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_18x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_18x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_18x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_18x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_18x8(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)

#define UNROLL_K_LOOP_18x8_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_18x8(0, 1, 2)                        \
                                                \
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_18x8(0, 1, 2)                        \
                                                \
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_18x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_18x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_18x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_18x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_18x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_18x8(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_17x8_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_17x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_17x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_17x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_17x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_17x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_17x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_17x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_17x8(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)

#define UNROLL_K_LOOP_17x8_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_17x8(0, 1, 2)                        \
                                                \
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_17x8(0, 1, 2)                        \
                                                \
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_17x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_17x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_17x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_17x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_17x8(0, 1, 2)                        \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_17x8(0, 1, 2)                        \
	lea(mem(rdx, rdi, 1), rdx)                  \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)

#define UNROLL_K_LOOP_16x8_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16x8(0, 1)                           \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16x8(3, 4)                           \
                                                \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16x8(0, 1)                           \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16x8(3, 4)                           \
                                                \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_16x8(0, 1)                           \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_16x8(3, 4)                           \
                                                \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_16x8(0, 1)                           \
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_16x8(3, 4)                           \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)

#define UNROLL_K_LOOP_16x8_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16x8(0, 1)                           \
                                                \
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16x8(3, 4)                           \
                                                \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16x8(0, 1)                           \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16x8(3, 4)                           \
                                                \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_16x8(0, 1)                           \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_16x8(3, 4)                           \
                                                \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_16x8(0, 1)                           \
	prefetch( 0,mem(r15, r13, 1) )              \
	K_LOOP_16x8(3, 4)                           \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_24MASKx8_L1               \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_24MASKx8(0, 1, 2)                    \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_24MASKx8(3, 4, 5)                    \
                                                \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_24MASKx8(0, 1, 2)                    \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_24MASKx8(3, 4, 5)                    \
                                                \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_24MASKx8(0, 1, 2)                    \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_24MASKx8(3, 4, 5)                    \
                                                \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_24MASKx8(0, 1, 2)                    \
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_24MASKx8(3, 4, 5)                    \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)

#define UNROLL_K_LOOP_24MASKx8_L2               \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_24MASKx8(0, 1, 2)                    \
                                                \
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_24MASKx8(3, 4, 5)                    \
                                                \
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_24MASKx8(0, 1, 2)                    \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_24MASKx8(3, 4, 5)                    \
                                                \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_24MASKx8(0, 1, 2)                    \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_24MASKx8(3, 4, 5)                    \
                                                \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_24MASKx8(0, 1, 2)                    \
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_24MASKx8(3, 4, 5)                    \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_12x8_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_12x8(0, 1)                           \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_12x8(3, 4)                           \
                                                \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_12x8(0, 1)                           \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_12x8(3, 4)                           \
                                                \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_12x8(0, 1)                           \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_12x8(3, 4)                           \
                                                \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_12x8(0, 1)                           \
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_12x8(3, 4)                           \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)

#define UNROLL_K_LOOP_12x8_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_12x8(0, 1)                           \
                                                \
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_12x8(3, 4)                           \
                                                \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_12x8(0, 1)                           \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_12x8(3, 4)                           \
                                                \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_12x8(0, 1)                           \
                                                \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_12x8(3, 4)                           \
                                                \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
                                                \
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_12x8(0, 1)                           \
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_12x8(3, 4)                           \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_11x8_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_11x8(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_11x8_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_11x8(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_11x8_L3                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_11x8(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_11x8(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_10x8_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_10x8(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_10x8(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_10x8(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_10x8(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_10x8(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_10x8(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_10x8(0, 1)                           \
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_10x8(3, 4)                           \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_10x8_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_10x8(0, 1)                           \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_10x8(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_10x8(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_10x8(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_10x8(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_10x8(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_10x8(0, 1)                           \
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_10x8(3, 4)                           \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_10x8_L3                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_10x8(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_10x8(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_10x8(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_10x8(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_10x8(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_10x8(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_10x8(0, 1)                           \
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_10x8(3, 4)                           \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_9x8_L1                    \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_9x8(0, 1)                            \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_9x8(3, 4)                            \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_9x8(0, 1)                            \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_9x8(3, 4)                            \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_9x8(0, 1)                            \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_9x8(3, 4)                            \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_9x8(0, 1)                            \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_9x8(3, 4)                            \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_9x8_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_9x8(0, 1)                            \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_9x8(3, 4)                            \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_9x8(0, 1)                            \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_9x8(3, 4)                            \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_9x8(0, 1)                            \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_9x8(3, 4)                            \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_9x8(0, 1)                            \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_9x8(3, 4)                            \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_9x8_L3                    \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_9x8(0, 1)                            \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_9x8(3, 4)                            \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_9x8(0, 1)                            \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_9x8(3, 4)                            \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_9x8(0, 1)                            \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_9x8(3, 4)                            \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_9x8(0, 1)                            \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_9x8(3, 4)                            \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_8x8_L1                    \
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8x8_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8x8_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8x8_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8x8_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_8x8_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_8x8_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_8x8_SET1(0)                          \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_8x8_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_8x8_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8x8_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8x8_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8x8_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8x8_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_8x8_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_8x8_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_8x8_SET1(0)                          \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_8x8_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_8x8_L3                    \
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8x8_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8x8_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8x8_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8x8_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_8x8_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_8x8_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_8x8_SET1(0)                          \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_8x8_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_16MASKx8_L1               \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16MASKx8(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16MASKx8(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16MASKx8(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16MASKx8(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_16MASKx8(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_16MASKx8(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_16MASKx8(0, 1)                       \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_16MASKx8(3, 4)                       \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_16MASKx8_L2               \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16MASKx8(0, 1)                       \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16MASKx8(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16MASKx8(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16MASKx8(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_16MASKx8(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_16MASKx8(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_16MASKx8(0, 1)                       \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_16MASKx8(3, 4)                       \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_16MASKx8_L3               \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16MASKx8(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16MASKx8(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16MASKx8(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16MASKx8(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_16MASKx8(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_16MASKx8(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_16MASKx8(0, 1)                       \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_16MASKx8(3, 4)                       \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_4x8_L1                    \
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_4x8_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_4x8_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_4x8_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_4x8_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_4x8_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_4x8_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_4x8_SET1(0)                          \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_4x8_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_4x8_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_4x8_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_4x8_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_4x8_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_4x8_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_4x8_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_4x8_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_4x8_SET1(0)                          \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_4x8_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_4x8_L3                    \
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_4x8_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_4x8_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_4x8_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_4x8_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_4x8_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_4x8_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_4x8_SET1(0)                          \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_4x8_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_2x8_L1                    \
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_2x8_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_2x8_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_2x8_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_2x8_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_2x8_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_2x8_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_2x8_SET1(0)                          \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_2x8_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_2x8_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_2x8_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_2x8_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_2x8_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_2x8_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_2x8_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_2x8_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_2x8_SET1(0)                          \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_2x8_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_1x8_L1                    \
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_1x8_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_1x8_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_1x8_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_1x8_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_1x8_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_1x8_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_1x8_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_1x8_SET2(0)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_1x8_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_1x8_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_1x8_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_1x8_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_1x8_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_1x8_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_1x8_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_1x8_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_1x8_SET2(0)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_8MASKx8_L1                \
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8MASKx8_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8MASKx8_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8MASKx8_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8MASKx8_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_8MASKx8_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_8MASKx8_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_8MASKx8_SET1(0)                      \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_8MASKx8_SET2(3)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_8MASKx8_L2                \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8MASKx8_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8MASKx8_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8MASKx8_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8MASKx8_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_8MASKx8_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_8MASKx8_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_8MASKx8_SET1(0)                      \
												\
	prefetch( 0, mem(r15, r13, 1) )             \
	K_LOOP_8MASKx8_SET2(3)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

/**************************END 24x8 UNROLL_K_LOOP**************************/
#define UNROLL_K_LOOP_24x7_L1                   \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11) )                     \
    K_LOOP_24x7(0, 1, 2)                        \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 1) )              \
    K_LOOP_24x7(3, 4, 5)                        \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 2) )              \
    K_LOOP_24x7(0, 1, 2)                        \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r13, 1) )             \
    K_LOOP_24x7(3, 4, 5)                        \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r15) )                     \
    K_LOOP_24x7(0, 1, 2)                        \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r15, r9, 1) )              \
    K_LOOP_24x7(3, 4, 5)                        \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r15, r9, 2) )              \
    K_LOOP_24x7(0, 1, 2)                        \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x7(3, 4, 5)                        \
    lea(mem(r11, r8, 8), r11)                   \
    lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_24x7_L2                   \
    prefetchw0( mem(rdx))                       \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11) )                     \
    K_LOOP_24x7(0, 1, 2)                        \
												\
    prefetchw0( mem(rdx, 64))                   \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 1) )              \
    K_LOOP_24x7(3, 4, 5)                        \
												\
    prefetchw0( mem(rdx, 128))                  \
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 2) )              \
    K_LOOP_24x7(0, 1, 2)                        \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r13, 1) )             \
    K_LOOP_24x7(3, 4, 5)                        \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r15) )                     \
    K_LOOP_24x7(0, 1, 2)                        \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r15, r9, 1) )              \
    K_LOOP_24x7(3, 4, 5)                        \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r15, r9, 2) )              \
    K_LOOP_24x7(0, 1, 2)                        \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x7(3, 4, 5)                        \
    lea(mem(r11, r8, 8), r11)                   \
    lea(mem(r15, r8, 8), r15)                   \
    lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_20x7_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_20x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_20x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_20x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_20x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_20x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_20x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_20x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x7(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_20x7_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_20x7(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_20x7(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_20x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_20x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_20x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_20x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_20x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x7(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_18x7_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_18x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_18x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_18x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_18x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_18x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_18x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_18x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x7(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_18x7_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_18x7(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_18x7(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_18x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_18x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_18x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_18x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_18x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x7(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_17x7_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_17x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_17x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_17x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_17x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_17x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_17x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_17x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x7(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_17x7_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_17x7(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_17x7(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_17x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_17x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_17x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_17x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_17x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x7(0, 1, 2)                        \
	lea(mem(rdx, rdi, 1), rdx)                  \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_16x7_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16x7(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16x7(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_16x7(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_16x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_16x7(0, 1)                           \
	K_LOOP_16x7(3, 4)                           \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_16x7_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16x7(0, 1)                           \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16x7(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_16x7(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_16x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_16x7(0, 1)                           \
	K_LOOP_16x7(3, 4)                           \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_24MASKx7_L1               \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_24MASKx7(0, 1, 2)                    \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_24MASKx7(3, 4, 5)                    \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_24MASKx7(0, 1, 2)                    \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_24MASKx7(3, 4, 5)                    \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_24MASKx7(0, 1, 2)                    \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_24MASKx7(3, 4, 5)                    \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_24MASKx7(0, 1, 2)                    \
	K_LOOP_24MASKx7(3, 4, 5)                    \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_24MASKx7_L2               \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_24MASKx7(0, 1, 2)                    \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_24MASKx7(3, 4, 5)                    \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_24MASKx7(0, 1, 2)                    \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_24MASKx7(3, 4, 5)                    \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_24MASKx7(0, 1, 2)                    \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_24MASKx7(3, 4, 5)                    \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_24MASKx7(0, 1, 2)                    \
	K_LOOP_24MASKx7(3, 4, 5)                    \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_12x7_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_12x7(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_12x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_12x7(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_12x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_12x7(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_12x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_12x7(0, 1)                           \
	K_LOOP_12x7(3, 4)                           \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_12x7_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_12x7(0, 1)                           \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_12x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_12x7(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_12x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_12x7(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_12x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_12x7(0, 1)                           \
	K_LOOP_12x7(3, 4)                           \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_11x7_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_11x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_11x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_11x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_11x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_11x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_11x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_11x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x7(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_11x7_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_11x7(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_11x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_11x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_11x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_11x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_11x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_11x7(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x7(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_10x7_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_10x7(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_10x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_10x7(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_10x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_10x7(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_10x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_10x7(0, 1)                           \
	K_LOOP_10x7(3, 4)                           \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_10x7_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_10x7(0, 1)                           \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_10x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_10x7(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_10x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_10x7(0, 1)                           \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_10x7(3, 4)                           \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_10x7(0, 1)                           \
	K_LOOP_10x7(3, 4)                           \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_9x7_L1                    \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_9x7(0, 1)                            \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_9x7(3, 4)                            \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_9x7(0, 1)                            \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_9x7(3, 4)                            \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_9x7(0, 1)                            \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_9x7(3, 4)                            \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_9x7(0, 1)                            \
												\
	K_LOOP_9x7(3, 4)                            \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_9x7_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_9x7(0, 1)                            \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_9x7(3, 4)                            \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_9x7(0, 1)                            \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_9x7(3, 4)                            \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_9x7(0, 1)                            \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_9x7(3, 4)                            \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_9x7(0, 1)                            \
												\
	K_LOOP_9x7(3, 4)                            \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_8x7_L1                    \
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8x7_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8x7_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8x7_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8x7_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_8x7_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_8x7_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_8x7_SET1(0)                          \
												\
	K_LOOP_8x7_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_8x7_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8x7_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8x7_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8x7_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8x7_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_8x7_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_8x7_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_8x7_SET1(0)                          \
												\
	K_LOOP_8x7_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_16MASKx7_L1               \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16MASKx7(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16MASKx7(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16MASKx7(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16MASKx7(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_16MASKx7(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_16MASKx7(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_16MASKx7(0, 1)                       \
												\
	K_LOOP_16MASKx7(3, 4)                       \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_16MASKx7_L2               \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16MASKx7(0, 1)                       \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16MASKx7(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16MASKx7(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16MASKx7(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_16MASKx7(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_16MASKx7(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_16MASKx7(0, 1)                       \
												\
	K_LOOP_16MASKx7(3, 4)                       \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_4x7_L1                    \
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_4x7_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_4x7_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_4x7_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_4x7_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_4x7_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_4x7_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_4x7_SET1(0)                          \
												\
	K_LOOP_4x7_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_4x7_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_4x7_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_4x7_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_4x7_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_4x7_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_4x7_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_4x7_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_4x7_SET1(0)                          \
												\
	K_LOOP_4x7_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_2x7_L1                    \
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_2x7_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_2x7_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_2x7_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_2x7_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_2x7_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_2x7_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_2x7_SET1(0)                          \
												\
	K_LOOP_2x7_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_2x7_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_2x7_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_2x7_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_2x7_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_2x7_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_2x7_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_2x7_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_2x7_SET1(0)                          \
												\
	K_LOOP_2x7_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_1x7_L1                    \
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_1x7_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_1x7_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_1x7_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_1x7_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_1x7_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_1x7_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_1x7_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x7_SET2(0)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_1x7_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_1x7_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_1x7_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_1x7_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_1x7_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_1x7_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_1x7_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_1x7_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x7_SET2(0)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_8MASKx7_L1                \
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8MASKx7_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8MASKx7_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8MASKx7_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8MASKx7_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_8MASKx7_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_8MASKx7_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_8MASKx7_SET1(0)                      \
												\
	K_LOOP_8MASKx7_SET2(3)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_8MASKx7_L2                \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8MASKx7_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8MASKx7_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8MASKx7_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8MASKx7_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_8MASKx7_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_8MASKx7_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 2) )              \
	K_LOOP_8MASKx7_SET1(0)                      \
												\
	K_LOOP_8MASKx7_SET2(3)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)

/********************END of UNROLL_K_LOOP 24x7*******************************/
#define UNROLL_K_LOOP_24x6_L1                   \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11) )                     \
    K_LOOP_24x6(0, 1, 2)                        \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 1) )              \
    K_LOOP_24x6(3, 4, 5)                        \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 2) )              \
    K_LOOP_24x6(0, 1, 2)                        \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r13, 1) )             \
    K_LOOP_24x6(3, 4, 5)                        \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r15) )                     \
    K_LOOP_24x6(0, 1, 2)                        \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r15, r9, 1) )              \
    K_LOOP_24x6(3, 4, 5)                        \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x6(0, 1, 2)                        \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x6(3, 4, 5)                        \
    lea(mem(r11, r8, 8), r11)                   \
    lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_24x6_L2                   \
    prefetchw0( mem(rdx))                       \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11) )                     \
    K_LOOP_24x6(0, 1, 2)                        \
												\
    prefetchw0( mem(rdx, 64))                   \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 1) )              \
    K_LOOP_24x6(3, 4, 5)                        \
												\
    prefetchw0( mem(rdx, 128))                  \
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 2) )              \
    K_LOOP_24x6(0, 1, 2)                        \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r13, 1) )             \
    K_LOOP_24x6(3, 4, 5)                        \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r15) )                     \
    K_LOOP_24x6(0, 1, 2)                        \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r15, r9, 1) )              \
    K_LOOP_24x6(3, 4, 5)                        \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x6(0, 1, 2)                        \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x6(3, 4, 5)                        \
    lea(mem(r11, r8, 8), r11)                   \
    lea(mem(r15, r8, 8), r15)                   \
    lea(mem(rdx, rdi, 1), rdx)                  \


#define UNROLL_K_LOOP_20x6_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_20x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_20x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_20x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_20x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_20x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_20x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x6(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_20x6_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_20x6(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_20x6(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_20x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_20x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_20x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_20x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x6(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_18x6_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_18x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_18x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_18x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_18x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_18x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_18x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x6(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_18x6_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_18x6(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_18x6(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_18x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_18x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_18x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_18x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x6(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_17x6_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_17x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_17x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_17x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_17x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_17x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_17x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x6(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_17x6_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_17x6(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_17x6(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_17x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_17x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_17x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_17x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x6(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_16x6_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16x6_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16x6_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_16x6_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_16x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x6_SET1(0, 1)                      \
	K_LOOP_16x6_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_16x6_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16x6_SET1(0, 1)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16x6_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_16x6_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_16x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x6_SET1(0, 1)                      \
	K_LOOP_16x6_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_24MASKx6_L1               \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_24MASKx6(0, 1, 2)                    \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_24MASKx6(3, 4, 5)                    \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_24MASKx6(0, 1, 2)                    \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_24MASKx6(3, 4, 5)                    \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_24MASKx6(0, 1, 2)                    \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_24MASKx6(3, 4, 5)                    \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx6(0, 1, 2)                    \
	K_LOOP_24MASKx6(3, 4, 5)                    \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_24MASKx6_L2               \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_24MASKx6(0, 1, 2)                    \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_24MASKx6(3, 4, 5)                    \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_24MASKx6(0, 1, 2)                    \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_24MASKx6(3, 4, 5)                    \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_24MASKx6(0, 1, 2)                    \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_24MASKx6(3, 4, 5)                    \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx6(0, 1, 2)                    \
	K_LOOP_24MASKx6(3, 4, 5)                    \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_12x6_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_12x6_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_12x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_12x6_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_12x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_12x6_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_12x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x6_SET1(0, 1)                      \
	K_LOOP_12x6_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_12x6_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_12x6_SET1(0, 1)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_12x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_12x6_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_12x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_12x6_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_12x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x6_SET1(0, 1)                      \
	K_LOOP_12x6_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_11x6_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_11x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_11x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_11x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_11x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_11x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_11x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x6(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_11x6_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_11x6(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_11x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_11x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_11x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_11x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_11x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x6(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x6(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_10x6_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_10x6_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_10x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_10x6_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_10x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_10x6_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_10x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x6_SET1(0, 1)                      \
	K_LOOP_10x6_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_10x6_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_10x6_SET1(0, 1)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_10x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_10x6_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_10x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_10x6_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_10x6_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x6_SET1(0, 1)                      \
	K_LOOP_10x6_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_9x6_L1                    \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_9x6_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_9x6_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_9x6_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_9x6_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_9x6_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x6_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x6_SET1(0, 1)                       \
												\
	K_LOOP_9x6_SET2(3, 4)                       \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_9x6_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_9x6_SET1(0, 1)                       \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_9x6_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_9x6_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_9x6_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_9x6_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x6_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x6_SET1(0, 1)                       \
												\
	K_LOOP_9x6_SET2(3, 4)                       \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_8x6_L1                    \
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8x6_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8x6_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8x6_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8x6_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_8x6_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_8x6_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x6_SET1(0)                          \
												\
	K_LOOP_8x6_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_8x6_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8x6_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8x6_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8x6_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8x6_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_8x6_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_8x6_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x6_SET1(0)                          \
												\
	K_LOOP_8x6_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_16MASKx6_L1               \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16MASKx6_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16MASKx6_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16MASKx6_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16MASKx6_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_16MASKx6_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_16MASKx6_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx6_SET1(0, 1)                  \
												\
	K_LOOP_16MASKx6_SET2(3, 4)                  \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_16MASKx6_L2               \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16MASKx6_SET1(0, 1)                  \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16MASKx6_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16MASKx6_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16MASKx6_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_16MASKx6_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_16MASKx6_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx6_SET1(0, 1)                  \
												\
	K_LOOP_16MASKx6_SET2(3, 4)                  \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_4x6_L1                    \
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_4x6_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_4x6_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_4x6_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_4x6_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_4x6_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_4x6_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x6_SET1(0)                          \
												\
	K_LOOP_4x6_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_4x6_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_4x6_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_4x6_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_4x6_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_4x6_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_4x6_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_4x6_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x6_SET1(0)                          \
												\
	K_LOOP_4x6_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_2x6_L1                    \
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_2x6_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_2x6_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_2x6_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_2x6_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_2x6_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_2x6_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x6_SET1(0)                          \
												\
	K_LOOP_2x6_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_2x6_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_2x6_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_2x6_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_2x6_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_2x6_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_2x6_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_2x6_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x6_SET1(0)                          \
												\
	K_LOOP_2x6_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_1x6_L1                    \
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_1x6_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_1x6_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_1x6_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_1x6_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_1x6_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_1x6_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x6_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x6_SET2(0)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_1x6_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_1x6_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_1x6_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_1x6_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_1x6_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_1x6_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_1x6_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x6_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x6_SET2(0)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_8MASKx6_L1                \
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8MASKx6_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8MASKx6_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8MASKx6_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8MASKx6_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_8MASKx6_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_8MASKx6_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx6_SET1(0)                      \
												\
	K_LOOP_8MASKx6_SET2(3)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_8MASKx6_L2                \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8MASKx6_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8MASKx6_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8MASKx6_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8MASKx6_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_8MASKx6_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15, r9, 1) )              \
	K_LOOP_8MASKx6_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx6_SET1(0)                      \
												\
	K_LOOP_8MASKx6_SET2(3)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)

/********************END of UNROLL_K_LOOP 24x6*******************************/
#define UNROLL_K_LOOP_24x5_L1                   \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11) )                     \
    K_LOOP_24x5(0, 1, 2)                        \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 1) )              \
    K_LOOP_24x5(3, 4, 5)                        \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 2) )              \
    K_LOOP_24x5(0, 1, 2)                        \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r13, 1) )             \
    K_LOOP_24x5(3, 4, 5)                        \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r15) )                     \
    K_LOOP_24x5(0, 1, 2)                        \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x5(3, 4, 5)                        \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x5(0, 1, 2)                        \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x5(3, 4, 5)                        \
    lea(mem(r11, r8, 8), r11)                   \
    lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_24x5_L2                   \
    prefetchw0( mem(rdx))                       \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    prefetchw0( mem(rdx, 64))                   \
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11) )                     \
    K_LOOP_24x5(0, 1, 2)                        \
												\
    prefetchw0( mem(rdx, 128))                  \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 1) )              \
    K_LOOP_24x5(3, 4, 5)                        \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 2) )              \
    K_LOOP_24x5(0, 1, 2)                        \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r13, 1) )             \
    K_LOOP_24x5(3, 4, 5)                        \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r15) )                     \
    K_LOOP_24x5(0, 1, 2)                        \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x5(3, 4, 5)                        \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x5(0, 1, 2)                        \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x5(3, 4, 5)                        \
    lea(mem(r11, r8, 8), r11)                   \
    lea(mem(r15, r8, 8), r15)                   \
    lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_20x5_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_20x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_20x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_20x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_20x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_20x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x5(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)

#define UNROLL_K_LOOP_20x5_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_20x5(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_20x5(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_20x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_20x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_20x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x5(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_18x5_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_18x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_18x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_18x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_18x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_18x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x5(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_18x5_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_18x5(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_18x5(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_18x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_18x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_18x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x5(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_17x5_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_17x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_17x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_17x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_17x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_17x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x5(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)

#define UNROLL_K_LOOP_17x5_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_17x5(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_17x5(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_17x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_17x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_17x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x5(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_16x5_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16x5_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16x5_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_16x5_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x5_SET1(0, 1)                      \
	K_LOOP_16x5_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)

#define UNROLL_K_LOOP_16x5_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16x5_SET1(0, 1)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16x5_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_16x5_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x5_SET1(0, 1)                      \
	K_LOOP_16x5_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_24MASKx5_L1               \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_24MASKx5(0, 1, 2)                    \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_24MASKx5(3, 4, 5)                    \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_24MASKx5(0, 1, 2)                    \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_24MASKx5(3, 4, 5)                    \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_24MASKx5(0, 1, 2)                    \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx5(3, 4, 5)                    \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx5(0, 1, 2)                    \
	K_LOOP_24MASKx5(3, 4, 5)                    \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_24MASKx5_L2               \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_24MASKx5(0, 1, 2)                    \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_24MASKx5(3, 4, 5)                    \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_24MASKx5(0, 1, 2)                    \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_24MASKx5(3, 4, 5)                    \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_24MASKx5(0, 1, 2)                    \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx5(3, 4, 5)                    \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx5(0, 1, 2)                    \
	K_LOOP_24MASKx5(3, 4, 5)                    \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_12x5_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_12x5_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_12x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_12x5_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_12x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_12x5_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x5_SET1(0, 1)                      \
	K_LOOP_12x5_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_12x5_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_12x5_SET1(0, 1)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_12x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_12x5_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_12x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_12x5_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x5_SET1(0, 1)                      \
	K_LOOP_12x5_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_11x5_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_11x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_11x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_11x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_11x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_11x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x5(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_11x5_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_11x5(0, 1, 2)                        \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_11x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_11x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_11x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r15) )                     \
	K_LOOP_11x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x5(0, 1, 2)                        \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x5(0, 1, 2)                        \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_10x5_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_10x5_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_10x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_10x5_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_10x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_10x5_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x5_SET1(0, 1)                      \
	K_LOOP_10x5_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_10x5_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_10x5_SET1(0, 1)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_10x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_10x5_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_10x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_10x5_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x5_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x5_SET1(0, 1)                      \
	K_LOOP_10x5_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_9x5_L1                    \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_9x5_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_9x5_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_9x5_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_9x5_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_9x5_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x5_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x5_SET1(0, 1)                       \
												\
	K_LOOP_9x5_SET2(3, 4)                       \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_9x5_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_9x5_SET1(0, 1)                       \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_9x5_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_9x5_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_9x5_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_9x5_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x5_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x5_SET1(0, 1)                       \
												\
	K_LOOP_9x5_SET2(3, 4)                       \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_8x5_L1                    \
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8x5_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8x5_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8x5_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8x5_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_8x5_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x5_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x5_SET1(0)                          \
												\
	K_LOOP_8x5_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_8x5_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8x5_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8x5_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8x5_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8x5_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_8x5_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x5_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x5_SET1(0)                          \
												\
	K_LOOP_8x5_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_16MASKx5_L1               \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16MASKx5_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16MASKx5_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16MASKx5_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16MASKx5_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_16MASKx5_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx5_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx5_SET1(0, 1)                  \
												\
	K_LOOP_16MASKx5_SET2(3, 4)                  \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_16MASKx5_L2               \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16MASKx5_SET1(0, 1)                  \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16MASKx5_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16MASKx5_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16MASKx5_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_16MASKx5_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx5_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx5_SET1(0, 1)                  \
												\
	K_LOOP_16MASKx5_SET2(3, 4)                  \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_4x5_L1                    \
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_4x5_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_4x5_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_4x5_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_4x5_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_4x5_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x5_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x5_SET1(0)                          \
												\
	K_LOOP_4x5_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_4x5_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_4x5_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_4x5_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_4x5_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_4x5_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_4x5_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x5_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x5_SET1(0)                          \
												\
	K_LOOP_4x5_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_2x5_L1                    \
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_2x5_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_2x5_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_2x5_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_2x5_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_2x5_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x5_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x5_SET1(0)                          \
												\
	K_LOOP_2x5_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_2x5_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_2x5_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_2x5_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_2x5_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_2x5_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_2x5_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x5_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x5_SET1(0)                          \
												\
	K_LOOP_2x5_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_1x5_L1                    \
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_1x5_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_1x5_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_1x5_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_1x5_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_1x5_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x5_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x5_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x5_SET2(0)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_1x5_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_1x5_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_1x5_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_1x5_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_1x5_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_1x5_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x5_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x5_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x5_SET2(0)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)                  \

#define UNROLL_K_LOOP_8MASKx5_L1                \
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8MASKx5_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8MASKx5_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8MASKx5_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8MASKx5_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_8MASKx5_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx5_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx5_SET1(0)                      \
												\
	K_LOOP_8MASKx5_SET2(3)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \

#define UNROLL_K_LOOP_8MASKx5_L2                \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8MASKx5_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8MASKx5_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8MASKx5_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8MASKx5_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r15) )                     \
	K_LOOP_8MASKx5_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx5_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx5_SET1(0)                      \
												\
	K_LOOP_8MASKx5_SET2(3)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(r15, r8, 8), r15)                   \
	lea(mem(rdx, rdi, 1), rdx)

/********************END of UNROLL_K_LOOP 24x5**\*****************************/
#define UNROLL_K_LOOP_24x4_L1                   \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11) )                     \
    K_LOOP_24x4_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 1) )              \
    K_LOOP_24x4_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 2) )              \
    K_LOOP_24x4_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r13, 1) )             \
    K_LOOP_24x4_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x4_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x4_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x4_SET1(0, 1, 2)                   \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x4_SET2(3, 4, 5)                   \
    lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_24x4_L2                   \
    prefetchw0( mem(rdx))                       \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11) )                     \
    K_LOOP_24x4_SET1(0, 1, 2)                   \
												\
    prefetchw0( mem(rdx, 64))                   \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 1) )              \
    K_LOOP_24x4_SET2(3, 4, 5)                   \
												\
    prefetchw0( mem(rdx, 128))                  \
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 2) )              \
    K_LOOP_24x4_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r13, 1) )             \
    K_LOOP_24x4_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x4_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x4_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x4_SET1(0, 1, 2)                   \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x4_SET2(3, 4, 5)                   \
    lea(mem(r11, r8, 8), r11)                   \
    lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_20x4_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_20x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_20x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_20x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_20x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x4_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_20x4_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_20x4_SET1(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_20x4_SET2(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_20x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_20x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x4_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_18x4_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_18x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_18x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_18x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_18x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x4_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_18x4_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_18x4_SET1(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_18x4_SET2(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_18x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_18x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x4_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_17x4_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_17x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_17x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_17x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_17x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x4_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_17x4_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_17x4_SET1(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_17x4_SET2(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_17x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_17x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x4_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_16x4_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16x4_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16x4_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x4_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x4_SET1(0, 1)                      \
	K_LOOP_16x4_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_16x4_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16x4_SET1(0, 1)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16x4_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x4_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x4_SET1(0, 1)                      \
	K_LOOP_16x4_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_24MASKx4_L1               \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_24MASKx4_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_24MASKx4_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_24MASKx4_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_24MASKx4_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx4_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx4_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx4_SET1(0, 1, 2)               \
	K_LOOP_24MASKx4_SET2(3, 4, 5)               \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_24MASKx4_L2               \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_24MASKx4_SET1(0, 1, 2)               \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_24MASKx4_SET2(3, 4, 5)               \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_24MASKx4_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_24MASKx4_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx4_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx4_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx4_SET1(0, 1, 2)               \
	K_LOOP_24MASKx4_SET2(3, 4, 5)               \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_12x4_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_12x4_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_12x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_12x4_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_12x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x4_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x4_SET1(0, 1)                      \
	K_LOOP_12x4_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_12x4_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_12x4_SET1(0, 1)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_12x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_12x4_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_12x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x4_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x4_SET1(0, 1)                      \
	K_LOOP_12x4_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_11x4_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_11x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_11x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_11x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_11x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x4_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_11x4_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_11x4_SET1(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_11x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_11x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_11x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x4_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x4_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x4_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_10x4_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_10x4_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_10x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_10x4_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_10x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x4_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x4_SET1(0, 1)                      \
	K_LOOP_10x4_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_10x4_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_10x4_SET1(0, 1)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_10x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_10x4_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_10x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x4_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x4_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x4_SET1(0, 1)                      \
	K_LOOP_10x4_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_9x4_L1                    \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_9x4_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_9x4_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_9x4_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_9x4_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x4_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x4_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x4_SET1(0, 1)                       \
												\
	K_LOOP_9x4_SET2(3, 4)                       \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_9x4_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_9x4_SET1(0, 1)                       \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_9x4_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_9x4_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_9x4_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x4_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x4_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x4_SET1(0, 1)                       \
												\
	K_LOOP_9x4_SET2(3, 4)                       \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_8x4_L1                    \
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8x4_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8x4_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8x4_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8x4_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x4_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x4_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x4_SET1(0)                          \
												\
	K_LOOP_8x4_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_8x4_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8x4_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8x4_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8x4_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8x4_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x4_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x4_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x4_SET1(0)                          \
												\
	K_LOOP_8x4_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_16MASKx4_L1               \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16MASKx4_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16MASKx4_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16MASKx4_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16MASKx4_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx4_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx4_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx4_SET1(0, 1)                  \
												\
	K_LOOP_16MASKx4_SET2(3, 4)                  \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_16MASKx4_L2               \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16MASKx4_SET1(0, 1)                  \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16MASKx4_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16MASKx4_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_16MASKx4_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx4_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx4_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx4_SET1(0, 1)                  \
												\
	K_LOOP_16MASKx4_SET2(3, 4)                  \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_4x4_L1                    \
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_4x4_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_4x4_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_4x4_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_4x4_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x4_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x4_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x4_SET1(0)                          \
												\
	K_LOOP_4x4_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_4x4_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_4x4_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_4x4_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_4x4_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_4x4_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x4_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x4_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x4_SET1(0)                          \
												\
	K_LOOP_4x4_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_2x4_L1                    \
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_2x4_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_2x4_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_2x4_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_2x4_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x4_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x4_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x4_SET1(0)                          \
												\
	K_LOOP_2x4_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_2x4_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_2x4_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_2x4_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_2x4_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_2x4_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x4_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x4_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x4_SET1(0)                          \
												\
	K_LOOP_2x4_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_1x4_L1                    \
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_1x4_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_1x4_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_1x4_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_1x4_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x4_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x4_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x4_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x4_SET2(0)                          \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_1x4_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_1x4_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_1x4_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_1x4_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_1x4_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x4_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x4_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x4_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x4_SET2(0)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_8MASKx4_L1                \
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8MASKx4_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8MASKx4_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8MASKx4_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8MASKx4_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx4_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx4_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx4_SET1(0)                      \
												\
	K_LOOP_8MASKx4_SET2(3)                      \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_8MASKx4_L2                \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8MASKx4_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8MASKx4_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8MASKx4_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r13, 1) )             \
	K_LOOP_8MASKx4_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx4_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx4_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx4_SET1(0)                      \
												\
	K_LOOP_8MASKx4_SET2(3)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

/********************END of UNROLL_K_LOOP 24x4**\*****************************/
#define UNROLL_K_LOOP_24x3_L1                   \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11) )                     \
    K_LOOP_24x3_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 1) )              \
    K_LOOP_24x3_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 2) )              \
    K_LOOP_24x3_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x3_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x3_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x3_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x3_SET1(0, 1, 2)                   \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x3_SET2(3, 4, 5)                   \
    lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_24x3_L2                   \
    prefetchw0( mem(rdx))                       \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11) )                     \
    K_LOOP_24x3_SET1(0, 1, 2)                   \
												\
    prefetchw0( mem(rdx, 64))                   \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 1) )              \
    K_LOOP_24x3_SET2(3, 4, 5)                   \
												\
    prefetchw0( mem(rdx, 128))                  \
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 2) )              \
    K_LOOP_24x3_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x3_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x3_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x3_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x3_SET1(0, 1, 2)                   \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x3_SET2(3, 4, 5)                   \
    lea(mem(r11, r8, 8), r11)                   \
    lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_20x3_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_20x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_20x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_20x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x3_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_20x3_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_20x3_SET1(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_20x3_SET2(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_20x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x3_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_18x3_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_18x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_18x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_18x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x3_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_18x3_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_18x3_SET1(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_18x3_SET2(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_18x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x3_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_17x3_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_17x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_17x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_17x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x3_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_17x3_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_17x3_SET1(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_17x3_SET2(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_17x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x3_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_16x3_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16x3_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16x3_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16x3_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x3_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x3_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x3_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x3_SET1(0, 1)                      \
	K_LOOP_16x3_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_16x3_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16x3_SET1(0, 1)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16x3_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16x3_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x3_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x3_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x3_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x3_SET1(0, 1)                      \
	K_LOOP_16x3_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_24MASKx3_L1               \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_24MASKx3_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_24MASKx3_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_24MASKx3_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx3_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx3_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx3_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx3_SET1(0, 1, 2)               \
	K_LOOP_24MASKx3_SET2(3, 4, 5)               \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_24MASKx3_L2               \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_24MASKx3_SET1(0, 1, 2)               \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_24MASKx3_SET2(3, 4, 5)               \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_24MASKx3_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx3_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx3_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx3_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx3_SET1(0, 1, 2)               \
	K_LOOP_24MASKx3_SET2(3, 4, 5)               \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_12x3_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_12x3_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_12x3_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_12x3_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x3_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x3_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x3_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x3_SET1(0, 1)                      \
	K_LOOP_12x3_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_12x3_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_12x3_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_12x3_SET2(3, 4)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_12x3_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x3_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x3_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x3_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x3_SET1(0, 1)                      \
	K_LOOP_12x3_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_11x3_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_11x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_11x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_11x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x3_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_11x3_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_11x3_SET1(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_11x3_SET2(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_11x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x3_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x3_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x3_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_10x3_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_10x3_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_10x3_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_10x3_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x3_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x3_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x3_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x3_SET1(0, 1)                      \
	K_LOOP_10x3_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_10x3_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_10x3_SET1(0, 1)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_10x3_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_10x3_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x3_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x3_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x3_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x3_SET1(0, 1)                      \
	K_LOOP_10x3_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_9x3_L1                    \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_9x3_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_9x3_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_9x3_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x3_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x3_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x3_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x3_SET1(0, 1)                       \
												\
	K_LOOP_9x3_SET2(3, 4)                       \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_9x3_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_9x3_SET1(0, 1)                       \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_9x3_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_9x3_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x3_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x3_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x3_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x3_SET1(0, 1)                       \
												\
	K_LOOP_9x3_SET2(3, 4)                       \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_8x3_L1                    \
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8x3_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8x3_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8x3_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x3_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x3_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x3_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x3_SET1(0)                          \
												\
	K_LOOP_8x3_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_8x3_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8x3_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8x3_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8x3_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x3_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x3_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x3_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x3_SET1(0)                          \
												\
	K_LOOP_8x3_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_16MASKx3_L1               \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16MASKx3_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16MASKx3_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16MASKx3_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx3_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx3_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx3_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx3_SET1(0, 1)                  \
												\
	K_LOOP_16MASKx3_SET2(3, 4)                  \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_16MASKx3_L2               \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16MASKx3_SET1(0, 1)                  \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16MASKx3_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_16MASKx3_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx3_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx3_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx3_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx3_SET1(0, 1)                  \
												\
	K_LOOP_16MASKx3_SET2(3, 4)                  \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_4x3_L1                    \
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_4x3_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_4x3_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_4x3_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x3_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x3_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x3_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x3_SET1(0)                          \
												\
	K_LOOP_4x3_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_4x3_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_4x3_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_4x3_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_4x3_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x3_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x3_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x3_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x3_SET1(0)                          \
												\
	K_LOOP_4x3_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_2x3_L1                    \
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_2x3_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_2x3_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_2x3_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x3_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x3_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x3_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x3_SET1(0)                          \
												\
	K_LOOP_2x3_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_2x3_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_2x3_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_2x3_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_2x3_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x3_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x3_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x3_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x3_SET1(0)                          \
												\
	K_LOOP_2x3_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_1x3_L1                    \
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_1x3_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_1x3_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_1x3_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x3_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x3_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x3_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x3_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x3_SET2(0)                          \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_1x3_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_1x3_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_1x3_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_1x3_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x3_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x3_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x3_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x3_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x3_SET2(0)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_8MASKx3_L1                \
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8MASKx3_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8MASKx3_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8MASKx3_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx3_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx3_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx3_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx3_SET1(0)                      \
												\
	K_LOOP_8MASKx3_SET2(3)                      \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_8MASKx3_L2                \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8MASKx3_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8MASKx3_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 2) )              \
	K_LOOP_8MASKx3_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx3_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx3_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx3_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx3_SET1(0)                      \
												\
	K_LOOP_8MASKx3_SET2(3)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

/********************END of UNROLL_K_LOOP 24x3**\*****************************/
#define UNROLL_K_LOOP_24x2_L1                   \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11) )                     \
    K_LOOP_24x2_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 1) )              \
    K_LOOP_24x2_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x2_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x2_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x2_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x2_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x2_SET1(0, 1, 2)                   \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x2_SET2(3, 4, 5)                   \
    lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_24x2_L2                   \
    prefetchw0( mem(rdx))                       \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11) )                     \
    K_LOOP_24x2_SET1(0, 1, 2)                   \
												\
    prefetchw0( mem(rdx, 64))                   \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11, r9, 1) )              \
    K_LOOP_24x2_SET2(3, 4, 5)                   \
												\
    prefetchw0( mem(rdx, 128))                  \
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x2_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x2_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x2_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x2_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x2_SET1(0, 1, 2)                   \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x2_SET2(3, 4, 5)                   \
    lea(mem(r11, r8, 8), r11)                   \
    lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_20x2_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_20x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_20x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x2_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_20x2_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_20x2_SET1(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_20x2_SET2(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x2_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_18x2_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_18x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_18x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x2_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_18x2_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_18x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_18x2_SET2(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x2_SET1(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x2_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_17x2_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_17x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_17x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x2_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_17x2_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_17x2_SET1(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_17x2_SET2(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x2_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_16x2_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16x2_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x2_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x2_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x2_SET1(0, 1)                      \
	K_LOOP_16x2_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_16x2_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16x2_SET1(0, 1)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x2_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x2_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x2_SET1(0, 1)                      \
	K_LOOP_16x2_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_24MASKx2_L1               \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_24MASKx2_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_24MASKx2_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx2_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx2_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx2_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx2_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx2_SET1(0, 1, 2)               \
	K_LOOP_24MASKx2_SET2(3, 4, 5)               \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_24MASKx2_L2               \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_24MASKx2_SET1(0, 1, 2)               \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_24MASKx2_SET2(3, 4, 5)               \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx2_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx2_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx2_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx2_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx2_SET1(0, 1, 2)               \
	K_LOOP_24MASKx2_SET2(3, 4, 5)               \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_12x2_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_12x2_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_12x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x2_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x2_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x2_SET1(0, 1)                      \
	K_LOOP_12x2_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_12x2_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_12x2_SET1(0, 1)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_12x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x2_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x2_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x2_SET1(0, 1)                      \
	K_LOOP_12x2_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_11x2_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_11x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_11x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x2_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_11x2_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_11x2_SET1(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_11x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x2_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x2_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x2_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_10x2_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_10x2_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_10x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x2_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x2_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x2_SET1(0, 1)                      \
	K_LOOP_10x2_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_10x2_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_10x2_SET1(0, 1)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_10x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x2_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x2_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x2_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x2_SET1(0, 1)                      \
	K_LOOP_10x2_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_9x2_L1                    \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_9x2_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_9x2_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x2_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x2_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x2_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x2_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x2_SET1(0, 1)                       \
												\
	K_LOOP_9x2_SET2(3, 4)                       \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_9x2_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_9x2_SET1(0, 1)                       \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_9x2_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x2_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x2_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x2_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x2_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x2_SET1(0, 1)                       \
												\
	K_LOOP_9x2_SET2(3, 4)                       \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_8x2_L1                    \
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8x2_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8x2_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x2_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x2_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x2_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x2_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x2_SET1(0)                          \
												\
	K_LOOP_8x2_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_8x2_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8x2_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8x2_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x2_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x2_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x2_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x2_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x2_SET1(0)                          \
												\
	K_LOOP_8x2_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_16MASKx2_L1               \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16MASKx2_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16MASKx2_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx2_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx2_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx2_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx2_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx2_SET1(0, 1)                  \
												\
	K_LOOP_16MASKx2_SET2(3, 4)                  \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_16MASKx2_L2               \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16MASKx2_SET1(0, 1)                  \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_16MASKx2_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx2_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx2_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx2_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx2_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx2_SET1(0, 1)                  \
												\
	K_LOOP_16MASKx2_SET2(3, 4)                  \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_4x2_L1                    \
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_4x2_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_4x2_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x2_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x2_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x2_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x2_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x2_SET1(0)                          \
												\
	K_LOOP_4x2_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_4x2_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_4x2_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_4x2_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x2_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x2_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x2_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x2_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x2_SET1(0)                          \
												\
	K_LOOP_4x2_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_2x2_L1                    \
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_2x2_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_2x2_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x2_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x2_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x2_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x2_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x2_SET1(0)                          \
												\
	K_LOOP_2x2_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_2x2_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_2x2_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_2x2_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x2_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x2_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x2_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x2_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x2_SET1(0)                          \
												\
	K_LOOP_2x2_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_1x2_L1                    \
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_1x2_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_1x2_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x2_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x2_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x2_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x2_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x2_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x2_SET2(0)                          \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_1x2_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_1x2_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_1x2_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x2_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x2_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x2_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x2_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x2_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x2_SET2(0)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_8MASKx2_L1                \
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8MASKx2_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8MASKx2_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx2_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx2_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx2_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx2_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx2_SET1(0)                      \
												\
	K_LOOP_8MASKx2_SET2(3)                      \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_8MASKx2_L2                \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8MASKx2_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11, r9, 1) )              \
	K_LOOP_8MASKx2_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx2_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx2_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx2_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx2_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx2_SET1(0)                      \
												\
	K_LOOP_8MASKx2_SET2(3)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

/********************END of UNROLL_K_LOOP 24x2**\*****************************/
#define UNROLL_K_LOOP_24x1_L1                   \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11) )                     \
    K_LOOP_24x1_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x1_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x1_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x1_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x1_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x1_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x1_SET1(0, 1, 2)                   \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x1_SET2(3, 4, 5)                   \
    lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_24x1_L2                   \
    prefetchw0( mem(rdx))                       \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
                                                \
												\
    K_LOOP_PREFETCH_A                           \
    prefetch( 0, mem(r11) )                     \
    K_LOOP_24x1_SET1(0, 1, 2)                   \
												\
    prefetchw0( mem(rdx, 64))                   \
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x1_SET2(3, 4, 5)                   \
												\
    prefetchw0( mem(rdx, 128))                  \
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x1_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x1_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x1_SET1(0, 1, 2)                   \
												\
    vmovupd( mem(rax), zmm0 )                   \
    vmovupd( 0x40(rax), zmm1 )                  \
    vmovupd( 0x80(rax),zmm2 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x1_SET2(3, 4, 5)                   \
												\
    vmovupd( mem(rax), zmm3 )                   \
    vmovupd( 0x40(rax), zmm4 )                  \
    vmovupd( 0x80(rax),zmm5 )                   \
                                                \
    add( r10, rax )                             \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x1_SET1(0, 1, 2)                   \
												\
    K_LOOP_PREFETCH_A                           \
    K_LOOP_24x1_SET2(3, 4, 5)                   \
    lea(mem(r11, r8, 8), r11)                   \
    lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_20x1_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_20x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x1_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_20x1_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_20x1_SET1(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x1_SET2(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), ymm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_20x1_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_18x1_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_18x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x1_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_18x1_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_18x1_SET1(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x1_SET2(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), xmm2 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_18x1_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_17x1_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_17x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x1_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_17x1_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_17x1_SET1(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x1_SET2(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovsd ( 0x80(rax),xmm2 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_17x1_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_16x1_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16x1_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x1_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x1_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x1_SET1(0, 1)                      \
	K_LOOP_16x1_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_16x1_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16x1_SET1(0, 1)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x1_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x1_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	add( r10, rax )                             \
												\
	K_LOOP_16x1_SET1(0, 1)                      \
	K_LOOP_16x1_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_24MASKx1_L1               \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_24MASKx1_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx1_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx1_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx1_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx1_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx1_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx1_SET1(0, 1, 2)               \
	K_LOOP_24MASKx1_SET2(3, 4, 5)               \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_24MASKx1_L2               \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_24MASKx1_SET1(0, 1, 2)               \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx1_SET2(3, 4, 5)               \
												\
	prefetchw0( mem(rdx, 128))                  \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx1_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx1_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx1_SET1(0, 1, 2)               \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax), zmm1 )                  \
	vmovupd( 0x80(rax), zmm2 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx1_SET2(3, 4, 5)               \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax), zmm4 )                  \
	vmovupd( 0x80(rax), zmm5 MASK_KZ(2) )       \
	add( r10, rax )                             \
												\
	K_LOOP_24MASKx1_SET1(0, 1, 2)               \
	K_LOOP_24MASKx1_SET2(3, 4, 5)               \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_12x1_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_12x1_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x1_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x1_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x1_SET1(0, 1)                      \
	K_LOOP_12x1_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_12x1_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_12x1_SET1(0, 1)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x1_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x1_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),ymm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_12x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),ymm4  )                  \
	add( r10, rax )                             \
												\
	K_LOOP_12x1_SET1(0, 1)                      \
	K_LOOP_12x1_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_11x1_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_11x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x1_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_11x1_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	prefetch( 0, mem(r11) )                     \
	K_LOOP_11x1_SET1(0, 1, 2)                   \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x1_SET2(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x1_SET1(0, 1, 2)                   \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	vmovsd( 0x50(rax),xmm2 )                    \
	add( r10, rax )                             \
	K_LOOP_11x1_SET2(0, 1, 2)                   \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_10x1_L1                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_10x1_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x1_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x1_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x1_SET1(0, 1)                      \
	K_LOOP_10x1_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_10x1_L2                   \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_10x1_SET1(0, 1)                      \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x1_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x1_SET1(0, 1)                      \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),xmm1 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x1_SET2(3, 4)                      \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),xmm4 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_10x1_SET1(0, 1)                      \
	K_LOOP_10x1_SET2(3, 4)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_9x1_L1                    \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_9x1_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x1_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x1_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x1_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x1_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x1_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x1_SET1(0, 1)                       \
												\
	K_LOOP_9x1_SET2(3, 4)                       \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_9x1_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_9x1_SET1(0, 1)                       \
												\
	prefetchw0( mem(rdx, 64))                   \
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x1_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x1_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x1_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x1_SET1(0, 1)                       \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovsd( 0x40(rax),xmm1 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x1_SET2(3, 4)                       \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovsd( 0x40(rax),xmm4 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_9x1_SET1(0, 1)                       \
												\
	K_LOOP_9x1_SET2(3, 4)                       \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_8x1_L1                    \
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8x1_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x1_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x1_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x1_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x1_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x1_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x1_SET1(0)                          \
												\
	K_LOOP_8x1_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_8x1_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8x1_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x1_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x1_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x1_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x1_SET1(0)                          \
												\
	vmovupd( mem(rax), zmm0 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x1_SET2(3)                          \
												\
	vmovupd( mem(rax), zmm3 )                   \
	add( r10, rax )                             \
												\
	K_LOOP_8x1_SET1(0)                          \
												\
	K_LOOP_8x1_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_16MASKx1_L1               \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16MASKx1_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx1_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx1_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx1_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx1_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx1_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx1_SET1(0, 1)                  \
												\
	K_LOOP_16MASKx1_SET2(3, 4)                  \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_16MASKx1_L2               \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_16MASKx1_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx1_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx1_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx1_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx1_SET1(0, 1)                  \
												\
	vmovupd( mem(rax), zmm0 )                   \
	vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx1_SET2(3, 4)                  \
												\
	vmovupd( mem(rax), zmm3 )                   \
	vmovupd( 0x40(rax),zmm4 MASK_KZ(2) )        \
	add( r10, rax )                             \
												\
	K_LOOP_16MASKx1_SET1(0, 1)                  \
												\
	K_LOOP_16MASKx1_SET2(3, 4)                  \
	lea(mem(rdx, rdi, 1), rdx)                  \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_4x1_L1                    \
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_4x1_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x1_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x1_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x1_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x1_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x1_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x1_SET1(0)                          \
												\
	K_LOOP_4x1_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_4x1_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_4x1_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x1_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x1_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x1_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x1_SET1(0)                          \
												\
	vmovupd( mem(rax),ymm0)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x1_SET2(3)                          \
												\
	vmovupd( mem(rax),ymm3)                     \
	add( r10, rax )                             \
												\
	K_LOOP_4x1_SET1(0)                          \
												\
	K_LOOP_4x1_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_2x1_L1                    \
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_2x1_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x1_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x1_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x1_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x1_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x1_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x1_SET1(0)                          \
												\
	K_LOOP_2x1_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_2x1_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_2x1_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x1_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x1_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x1_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x1_SET1(0)                          \
												\
	vmovupd( mem(rax),xmm0 )                    \
	add( r10, rax )                             \
	K_LOOP_2x1_SET2(3)                          \
												\
	vmovupd( mem(rax),xmm3 )                    \
	add( r10, rax )                             \
												\
	K_LOOP_2x1_SET1(0)                          \
												\
	K_LOOP_2x1_SET2(3)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_1x1_L1                    \
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_1x1_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x1_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x1_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x1_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x1_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x1_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x1_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x1_SET2(0)                          \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_1x1_L2                    \
	prefetchw0( mem(rdx))                       \
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_1x1_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x1_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x1_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x1_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x1_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x1_SET2(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x1_SET1(0)                          \
												\
	vmovsd( mem(rax),xmm0 )                     \
	add( r10, rax )                             \
												\
	K_LOOP_1x1_SET2(0)                          \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)

#define UNROLL_K_LOOP_8MASKx1_L1                \
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8MASKx1_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx1_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx1_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx1_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx1_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx1_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx1_SET1(0)                      \
												\
	K_LOOP_8MASKx1_SET2(3)                      \
	lea(mem(r11, r8, 8), r11)

#define UNROLL_K_LOOP_8MASKx1_L2                \
	prefetchw0( mem(rdx))                       \
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	prefetch( 0, mem(r11) )                     \
	K_LOOP_8MASKx1_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx1_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx1_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx1_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx1_SET1(0)                      \
												\
	vmovupd( mem(rax),zmm0 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx1_SET2(3)                      \
												\
	vmovupd( mem(rax),zmm3 MASK_KZ(2) )         \
	add( r10, rax )                             \
												\
	K_LOOP_8MASKx1_SET1(0)                      \
												\
	K_LOOP_8MASKx1_SET2(3)                      \
	lea(mem(r11, r8, 8), r11)                   \
	lea(mem(rdx, rdi, 1), rdx)
/********************END of UNROLL_K_LOOP 24x1*******************************/

#define POST_K_LOOP         \
    mov(var(a), r15)        \
    mov(var(ps_a8), rcx)    \
    add(rcx, r15)           \
    mov(r15, var(a))        \
    mov(var(c), rcx)        \
    imul(imm(0x18), rsi)    \
    add(rsi, rcx)           \
    mov(rcx, var(c))        \
    mov(var(m_iter), r11)   \
    dec(r11)                \
    cmp(imm(0x0), r11)      \
    jnz(.M_ITER_LOOP)       \
                            \
    label(.MLEFT)           \
    mov(var(m_left), rsi)   \
    cmp(imm(0x0), rsi)      \
    JZ(.CONCLUDE)           \
                            \
    cmp(imm(0x14), rsi)     \
    JZ(.EDGE20XN)           \
                            \
    cmp(imm(0x12), rsi)     \
    JZ(.EDGE18XN)           \
                            \
    cmp(imm(0x11), rsi)     \
    JZ(.EDGE17XN)           \
                            \
    cmp(imm(0x10), rsi)     \
    JZ(.EDGE16XN)           \
                            \
    JG(.EDGE24MASKXN)       \
                            \
    cmp(imm(0xC), rsi)      \
    JZ(.EDGE12XN)           \
                            \
    cmp(imm(0xB), rsi)      \
    JZ(.EDGE11XN)           \
                            \
    cmp(imm(0xA), rsi)      \
    JZ(.EDGE10XN)           \
                            \
    cmp(imm(0x9), rsi)      \
    JZ(.EDGE9XN)            \
                            \
    cmp(imm(0x8), rsi)      \
    JZ(.EDGE8XN)            \
    JG(.EDGE16MASKXN)       \
                            \
    cmp(imm(0x4), rsi)      \
    JZ(.EDGE4XN)            \
                            \
    cmp(imm(0x2), rsi)      \
    JZ(.EDGE2XN)            \
                            \
    cmp(imm(0x1), rsi)      \
    JZ(.EDGE1XN)            \
                            \
    cmp(imm(0x0), rsi)      \
    jg(.EDGE8MASKXN)


#define POST_ACCUMULATION               \
    /* load address of alpha */         \
	mov(var(alpha), rdx)                \
    /* broadcast alpha */               \
	vbroadcastsd(mem(rdx), zmm30)       \
    /* load address of beta */          \
	mov(var(beta), rbx)                 \
    /* broadcast beta */                \
	vbroadcastsd(mem(rbx), zmm31)       \
                                        \
    /* load rs_c */                     \
	mov(var(rs_c0), rsi)                \
    /* rsi = rs_c * sizeof(double)*/    \
	lea(mem(, rsi, 8), rsi)             \
    /* rdx = rcx + 4 * cs_c*/           \
	lea(mem(rcx, rdi, 4), rdx)          \
    /* r13 = 3*cs_c*/                   \
	lea(mem(rdi, rdi, 2), r13)          \
                                        \
	vxorpd(ymm2, ymm2, ymm2)            \
    /* set ZF if beta == 0 */           \
	vucomisd(xmm2, xmm31)


#define M_LEFT_24x8                                             \
																\
    /* m_left 20 edge kernel begins */                          \
    label(.EDGE20XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB20PREFETCHLOOP)                                     \
    label(.SUB20LOOP1)                                          \
																\
        UNROLL_K_LOOP_20x8_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP1)                                            \
																\
    label(.SUB20PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB20TAILITER)                                         \
																\
    label(.SUB20LOOP2)                                          \
        UNROLL_K_LOOP_20x8_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP2)                                            \
    label(.SUB20TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB20TAIL)                                             \
    label(.SUB20LOOP3)                                          \
        UNROLL_K_LOOP_20x8_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP3)                                            \
																\
    label(.SUB20TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB20DPOSTACCUM)                                        \
    label(.SUB20DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax), ymm2 )                              \
        add( r10,rax )                                          \
        K_LOOP_20x8(0, 1, 2)                                    \
        dec(rsi)                                                \
    jne(.SUB20DLOOPKLEFT)                                       \
																\
    label(.SUB20DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB20DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB20DROWSTORED)                                        \
																\
    label(.SUB20DCOLSTORED)                                     \
    STORE_COLSTORED_C_20x8_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DROWSTORED)                                     \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30, zmm6, zmm6 )                                 \
    vmulpd( zmm30, zmm7, zmm7 )                                 \
    vmulpd( zmm30, zmm28, zmm28 )                               \
    vmulpd( zmm30, zmm8, zmm8 )                                 \
    vmulpd( zmm30, zmm9, zmm9 )                                 \
    vmulpd( zmm30, zmm29, zmm29 )                               \
    vmulpd( zmm30, zmm10, zmm10 )                               \
    vmulpd( zmm30, zmm11, zmm11 )                               \
    vmulpd( zmm30, zmm26, zmm26 )                               \
    vmulpd( zmm30, zmm12, zmm12 )                               \
    vmulpd( zmm30, zmm13, zmm13 )                               \
    vmulpd( zmm30, zmm27, zmm27 )                               \
    vmulpd( zmm30, zmm14, zmm14 )                               \
    vmulpd( zmm30, zmm15, zmm15 )                               \
    vmulpd( zmm30, zmm24, zmm24 )                               \
    vmulpd( zmm30, zmm16, zmm16 )                               \
    vmulpd( zmm30, zmm17, zmm17 )                               \
    vmulpd( zmm30, zmm25, zmm25 )                               \
    vmulpd( zmm30, zmm18, zmm18 )                               \
    vmulpd( zmm30, zmm19, zmm19 )                               \
    vmulpd( zmm30, zmm22, zmm22 )                               \
    vmulpd( zmm30, zmm20, zmm20 )                               \
    vmulpd( zmm30, zmm21, zmm21 )                               \
    vmulpd( zmm30, zmm23, zmm23 )                               \
    STORE_ROWSTORED_C_20x8_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB20DROWSTORBZ)                                        \
																\
    label(.SUB20DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_20x8_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DROWSTORBZ)                                     \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30, zmm6, zmm6 )                                 \
    vmulpd( zmm30, zmm7, zmm7 )                                 \
    vmulpd( zmm30, zmm28, zmm28 )                               \
    vmulpd( zmm30, zmm8, zmm8 )                                 \
    vmulpd( zmm30, zmm9, zmm9 )                                 \
    vmulpd( zmm30, zmm29, zmm29 )                               \
    vmulpd( zmm30, zmm10, zmm10 )                               \
    vmulpd( zmm30, zmm11, zmm11 )                               \
    vmulpd( zmm30, zmm26, zmm26 )                               \
    vmulpd( zmm30, zmm12, zmm12 )                               \
    vmulpd( zmm30, zmm13, zmm13 )                               \
    vmulpd( zmm30, zmm27, zmm27 )                               \
    vmulpd( zmm30, zmm14, zmm14 )                               \
    vmulpd( zmm30, zmm15, zmm15 )                               \
    vmulpd( zmm30, zmm24, zmm24 )                               \
    vmulpd( zmm30, zmm16, zmm16 )                               \
    vmulpd( zmm30, zmm17, zmm17 )                               \
    vmulpd( zmm30, zmm25, zmm25 )                               \
    vmulpd( zmm30, zmm18, zmm18 )                               \
    vmulpd( zmm30, zmm19, zmm19 )                               \
    vmulpd( zmm30, zmm22, zmm22 )                               \
    vmulpd( zmm30, zmm20, zmm20 )                               \
    vmulpd( zmm30, zmm21, zmm21 )                               \
    vmulpd( zmm30, zmm23, zmm23 )                               \
    STORE_ROWSTORED_C_20x8_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 20 edge kernel ends */                            \
																\
    /* m_left 18 edge kernel begins */                          \
    label(.EDGE18XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB18PREFETCHLOOP)                                     \
    label(.SUB18LOOP1)                                          \
																\
        UNROLL_K_LOOP_18x8_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP1)                                            \
																\
    label(.SUB18PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB18TAILITER)                                         \
																\
    label(.SUB18LOOP2)                                          \
        UNROLL_K_LOOP_18x8_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP2)                                            \
    label(.SUB18TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB18TAIL)                                             \
    label(.SUB18LOOP3)                                          \
        UNROLL_K_LOOP_18x8_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP3)                                            \
																\
    label(.SUB18TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB18DPOSTACCUM)                                        \
																\
    label(.SUB18DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax), xmm2 )                              \
        add( r10,rax )                                          \
        K_LOOP_18x8(0, 1, 2)                                    \
        dec(rsi)                                                \
    jne(.SUB18DLOOPKLEFT)                                       \
																\
    label(.SUB18DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB18DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB18DROWSTORED)                                        \
																\
    label(.SUB18DCOLSTORED)                                     \
    STORE_COLSTORED_C_18x8_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DROWSTORED)                                     \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30, zmm6, zmm6 )                                 \
    vmulpd( zmm30, zmm7, zmm7 )                                 \
    vmulpd( zmm30, zmm28, zmm28 )                               \
    vmulpd( zmm30, zmm8, zmm8 )                                 \
    vmulpd( zmm30, zmm9, zmm9 )                                 \
    vmulpd( zmm30, zmm29, zmm29 )                               \
    vmulpd( zmm30, zmm10, zmm10 )                               \
    vmulpd( zmm30, zmm11, zmm11 )                               \
    vmulpd( zmm30, zmm26, zmm26 )                               \
    vmulpd( zmm30, zmm12, zmm12 )                               \
    vmulpd( zmm30, zmm13, zmm13 )                               \
    vmulpd( zmm30, zmm27, zmm27 )                               \
    vmulpd( zmm30, zmm14, zmm14 )                               \
    vmulpd( zmm30, zmm15, zmm15 )                               \
    vmulpd( zmm30, zmm24, zmm24 )                               \
    vmulpd( zmm30, zmm16, zmm16 )                               \
    vmulpd( zmm30, zmm17, zmm17 )                               \
    vmulpd( zmm30, zmm25, zmm25 )                               \
    vmulpd( zmm30, zmm18, zmm18 )                               \
    vmulpd( zmm30, zmm19, zmm19 )                               \
    vmulpd( zmm30, zmm22, zmm22 )                               \
    vmulpd( zmm30, zmm20, zmm20 )                               \
    vmulpd( zmm30, zmm21, zmm21 )                               \
    vmulpd( zmm30, zmm23, zmm23 )                               \
    STORE_ROWSTORED_C_18x8_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB18DROWSTORBZ)                                        \
																\
    label(.SUB18DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_18x8_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DROWSTORBZ)                                     \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30, zmm6, zmm6 )                                 \
    vmulpd( zmm30, zmm7, zmm7 )                                 \
    vmulpd( zmm30, zmm28, zmm28 )                               \
    vmulpd( zmm30, zmm8, zmm8 )                                 \
    vmulpd( zmm30, zmm9, zmm9 )                                 \
    vmulpd( zmm30, zmm29, zmm29 )                               \
    vmulpd( zmm30, zmm10, zmm10 )                               \
    vmulpd( zmm30, zmm11, zmm11 )                               \
    vmulpd( zmm30, zmm26, zmm26 )                               \
    vmulpd( zmm30, zmm12, zmm12 )                               \
    vmulpd( zmm30, zmm13, zmm13 )                               \
    vmulpd( zmm30, zmm27, zmm27 )                               \
    vmulpd( zmm30, zmm14, zmm14 )                               \
    vmulpd( zmm30, zmm15, zmm15 )                               \
    vmulpd( zmm30, zmm24, zmm24 )                               \
    vmulpd( zmm30, zmm16, zmm16 )                               \
    vmulpd( zmm30, zmm17, zmm17 )                               \
    vmulpd( zmm30, zmm25, zmm25 )                               \
    vmulpd( zmm30, zmm18, zmm18 )                               \
    vmulpd( zmm30, zmm19, zmm19 )                               \
    vmulpd( zmm30, zmm22, zmm22 )                               \
    vmulpd( zmm30, zmm20, zmm20 )                               \
    vmulpd( zmm30, zmm21, zmm21 )                               \
    vmulpd( zmm30, zmm23, zmm23 )                               \
    STORE_ROWSTORED_C_18x8_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 18 edge kernel ends */                            \
    /* m_left 17 edge kernel begins */                          \
    label(.EDGE17XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB17PREFETCHLOOP)                                     \
    label(.SUB17LOOP1)                                          \
																\
        UNROLL_K_LOOP_17x8_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP1)                                            \
																\
    label(.SUB17PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB17TAILITER)                                         \
																\
    label(.SUB17LOOP2)                                          \
        UNROLL_K_LOOP_17x8_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP2)                                            \
    label(.SUB17TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB17TAIL)                                             \
    label(.SUB17LOOP3)                                          \
        UNROLL_K_LOOP_17x8_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP3)                                            \
    label(.SUB17TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB17DPOSTACCUM)                                        \
    label(.SUB17DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovsd ( 0x80(rax),xmm2 )                               \
        add( r10,rax )                                          \
        K_LOOP_17x8(0, 1, 2)                                    \
        dec(rsi)                                                \
    jne(.SUB17DLOOPKLEFT)                                       \
																\
    label(.SUB17DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB17DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB17DROWSTORED)                                        \
																\
    label(.SUB17DCOLSTORED)                                     \
    STORE_COLSTORED_C_17x8_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DROWSTORED)                                     \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30, zmm6, zmm6 )                                 \
    vmulpd( zmm30, zmm7, zmm7 )                                 \
    vmulpd( zmm30, zmm28, zmm28 )                               \
    vmulpd( zmm30, zmm8, zmm8 )                                 \
    vmulpd( zmm30, zmm9, zmm9 )                                 \
    vmulpd( zmm30, zmm29, zmm29 )                               \
    vmulpd( zmm30, zmm10, zmm10 )                               \
    vmulpd( zmm30, zmm11, zmm11 )                               \
    vmulpd( zmm30, zmm26, zmm26 )                               \
    vmulpd( zmm30, zmm12, zmm12 )                               \
    vmulpd( zmm30, zmm13, zmm13 )                               \
    vmulpd( zmm30, zmm27, zmm27 )                               \
    vmulpd( zmm30, zmm14, zmm14 )                               \
    vmulpd( zmm30, zmm15, zmm15 )                               \
    vmulpd( zmm30, zmm24, zmm24 )                               \
    vmulpd( zmm30, zmm16, zmm16 )                               \
    vmulpd( zmm30, zmm17, zmm17 )                               \
    vmulpd( zmm30, zmm25, zmm25 )                               \
    vmulpd( zmm30, zmm18, zmm18 )                               \
    vmulpd( zmm30, zmm19, zmm19 )                               \
    vmulpd( zmm30, zmm22, zmm22 )                               \
    vmulpd( zmm30, zmm20, zmm20 )                               \
    vmulpd( zmm30, zmm21, zmm21 )                               \
    vmulpd( zmm30, zmm23, zmm23 )                               \
    STORE_ROWSTORED_C_17x8_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB17DROWSTORBZ)                                        \
																\
    label(.SUB17DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_17x8_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DROWSTORBZ)                                     \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30, zmm6, zmm6 )                                 \
    vmulpd( zmm30, zmm7, zmm7 )                                 \
    vmulpd( zmm30, zmm28, zmm28 )                               \
    vmulpd( zmm30, zmm8, zmm8 )                                 \
    vmulpd( zmm30, zmm9, zmm9 )                                 \
    vmulpd( zmm30, zmm29, zmm29 )                               \
    vmulpd( zmm30, zmm10, zmm10 )                               \
    vmulpd( zmm30, zmm11, zmm11 )                               \
    vmulpd( zmm30, zmm26, zmm26 )                               \
    vmulpd( zmm30, zmm12, zmm12 )                               \
    vmulpd( zmm30, zmm13, zmm13 )                               \
    vmulpd( zmm30, zmm27, zmm27 )                               \
    vmulpd( zmm30, zmm14, zmm14 )                               \
    vmulpd( zmm30, zmm15, zmm15 )                               \
    vmulpd( zmm30, zmm24, zmm24 )                               \
    vmulpd( zmm30, zmm16, zmm16 )                               \
    vmulpd( zmm30, zmm17, zmm17 )                               \
    vmulpd( zmm30, zmm25, zmm25 )                               \
    vmulpd( zmm30, zmm18, zmm18 )                               \
    vmulpd( zmm30, zmm19, zmm19 )                               \
    vmulpd( zmm30, zmm22, zmm22 )                               \
    vmulpd( zmm30, zmm20, zmm20 )                               \
    vmulpd( zmm30, zmm21, zmm21 )                               \
    vmulpd( zmm30, zmm23, zmm23 )                               \
    STORE_ROWSTORED_C_17x8_B0                                   \
    jmp(.CONCLUDE)                                              \
    /* m_left 17 edge kernel ends */                            \
    /* m_left 16 edge kernel begins */                          \
    label(.EDGE16XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB16DLOOPKITER)                                     \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB16PREFETCHLOOP)                                     \
    label(.SUB16LOOP1)                                          \
																\
        UNROLL_K_LOOP_16x8_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP1)                                            \
																\
    label(.SUB16PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB16TAILITER)                                         \
																\
    label(.SUB16LOOP2)                                          \
        UNROLL_K_LOOP_16x8_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP2)                                            \
    label(.SUB16TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB16TAIL)                                             \
																\
    label(.SUB16LOOP3)                                          \
																\
        UNROLL_K_LOOP_16x8_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP3)                                            \
																\
    label(.SUB16TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB16DPOSTACCUM)                                        \
																\
    label(.SUB16DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        add( r10,rax )                                          \
																\
        K_LOOP_16x8(0, 1)                                       \
        dec(rsi)                                                \
    jne(.SUB16DLOOPKLEFT)                                       \
																\
    label(.SUB16DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB16DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB16DROWSTORED)                                        \
																\
    label(.SUB16DCOLSTORED)                                     \
    STORE_COLSTORED_C_16x8_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DROWSTORED)                                     \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30, zmm6, zmm6 )                                 \
    vmulpd( zmm30, zmm7, zmm7 )                                 \
    vmulpd( zmm30, zmm8, zmm8 )                                 \
    vmulpd( zmm30, zmm9, zmm9 )                                 \
    vmulpd( zmm30, zmm10, zmm10 )                               \
    vmulpd( zmm30, zmm11, zmm11 )                               \
    vmulpd( zmm30, zmm12, zmm12 )                               \
    vmulpd( zmm30, zmm13, zmm13 )                               \
    vmulpd( zmm30, zmm14, zmm14 )                               \
    vmulpd( zmm30, zmm15, zmm15 )                               \
    vmulpd( zmm30, zmm16, zmm16 )                               \
    vmulpd( zmm30, zmm17, zmm17 )                               \
    vmulpd( zmm30, zmm18, zmm18 )                               \
    vmulpd( zmm30, zmm19, zmm19 )                               \
    vmulpd( zmm30, zmm22, zmm22 )                               \
    vmulpd( zmm30, zmm20, zmm20 )                               \
    vmulpd( zmm30, zmm21, zmm21 )                               \
    vmulpd( zmm30, zmm23, zmm23 )                               \
    STORE_ROWSTORED_C_16x8_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16DROWSTORBZ)                                        \
																\
    label(.SUB16DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_16x8_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DROWSTORBZ)                                     \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30, zmm6, zmm6 )                                 \
    vmulpd( zmm30, zmm7, zmm7 )                                 \
    vmulpd( zmm30, zmm8, zmm8 )                                 \
    vmulpd( zmm30, zmm9, zmm9 )                                 \
    vmulpd( zmm30, zmm10, zmm10 )                               \
    vmulpd( zmm30, zmm11, zmm11 )                               \
    vmulpd( zmm30, zmm12, zmm12 )                               \
    vmulpd( zmm30, zmm13, zmm13 )                               \
    vmulpd( zmm30, zmm14, zmm14 )                               \
    vmulpd( zmm30, zmm15, zmm15 )                               \
    vmulpd( zmm30, zmm16, zmm16 )                               \
    vmulpd( zmm30, zmm17, zmm17 )                               \
    vmulpd( zmm30, zmm18, zmm18 )                               \
    vmulpd( zmm30, zmm19, zmm19 )                               \
    vmulpd( zmm30, zmm22, zmm22 )                               \
    vmulpd( zmm30, zmm20, zmm20 )                               \
    vmulpd( zmm30, zmm21, zmm21 )                               \
    vmulpd( zmm30, zmm23, zmm23 )                               \
    STORE_ROWSTORED_C_16x8_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /**********  M_Left 24 masked kernel begins**************/  \
    label(.EDGE24MASKXN)                                        \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    sub(imm(16), rsi)                                           \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB24MASKPREFETCHLOOP)                                 \
    label(.SUB24MASKLOOP1)                                      \
																\
        UNROLL_K_LOOP_24MASKx8_L1                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP1)                                        \
																\
    label(.SUB24MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.SUB24MASKTAILITER)                                     \
																\
    label(.SUB24MASKLOOP2)                                      \
        UNROLL_K_LOOP_24MASKx8_L2                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP2)                                        \
    label(.SUB24MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB24MASKTAIL)                                         \
    label(.SUB24MASKLOOP3)                                      \
        UNROLL_K_LOOP_24MASKx8_L1                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP3)                                        \
    label(.SUB24MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB24MASKDPOSTACCUM)                                    \
    label(.SUB24MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax),zmm2 MASK_KZ(2) )                    \
        add( r10,rax )                                          \
        K_LOOP_24MASKx8(0, 1, 2)                                \
        dec(rsi)                                                \
    jne(.SUB24MASKDLOOPKLEFT)                                   \
    label(.SUB24MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.SUB24MASKDBETAZERO)                                     \
    cmp(imm(8), rdi)                                            \
    jz(.SUB24MASKDROWSTORED)                                    \
																\
    label(.SUB24MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_24MASKx8_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDROWSTORED)                                 \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30, zmm6, zmm6 )                                 \
    vmulpd( zmm30, zmm7, zmm7 )                                 \
    vmulpd( zmm30, zmm28, zmm28 )                               \
    vmulpd( zmm30, zmm8, zmm8 )                                 \
    vmulpd( zmm30, zmm9, zmm9 )                                 \
    vmulpd( zmm30, zmm29, zmm29 )                               \
    vmulpd( zmm30, zmm10, zmm10 )                               \
    vmulpd( zmm30, zmm11, zmm11 )                               \
    vmulpd( zmm30, zmm26, zmm26 )                               \
    vmulpd( zmm30, zmm12, zmm12 )                               \
    vmulpd( zmm30, zmm13, zmm13 )                               \
    vmulpd( zmm30, zmm27, zmm27 )                               \
    vmulpd( zmm30, zmm14, zmm14 )                               \
    vmulpd( zmm30, zmm15, zmm15 )                               \
    vmulpd( zmm30, zmm24, zmm24 )                               \
    vmulpd( zmm30, zmm16, zmm16 )                               \
    vmulpd( zmm30, zmm17, zmm17 )                               \
    vmulpd( zmm30, zmm25, zmm25 )                               \
    vmulpd( zmm30, zmm18, zmm18 )                               \
    vmulpd( zmm30, zmm19, zmm19 )                               \
    vmulpd( zmm30, zmm22, zmm22 )                               \
    vmulpd( zmm30, zmm20, zmm20 )                               \
    vmulpd( zmm30, zmm21, zmm21 )                               \
    vmulpd( zmm30, zmm23, zmm23 )                               \
    STORE_ROWSTORED_C_24MASKx8_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.SUB24MASKDROWSTORBZ)                                    \
																\
    label(.SUB24MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_24MASKx8_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDROWSTORBZ)                                 \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30, zmm6, zmm6 )                                 \
    vmulpd( zmm30, zmm7, zmm7 )                                 \
    vmulpd( zmm30, zmm28, zmm28 )                               \
    vmulpd( zmm30, zmm8, zmm8 )                                 \
    vmulpd( zmm30, zmm9, zmm9 )                                 \
    vmulpd( zmm30, zmm29, zmm29 )                               \
    vmulpd( zmm30, zmm10, zmm10 )                               \
    vmulpd( zmm30, zmm11, zmm11 )                               \
    vmulpd( zmm30, zmm26, zmm26 )                               \
    vmulpd( zmm30, zmm12, zmm12 )                               \
    vmulpd( zmm30, zmm13, zmm13 )                               \
    vmulpd( zmm30, zmm27, zmm27 )                               \
    vmulpd( zmm30, zmm14, zmm14 )                               \
    vmulpd( zmm30, zmm15, zmm15 )                               \
    vmulpd( zmm30, zmm24, zmm24 )                               \
    vmulpd( zmm30, zmm16, zmm16 )                               \
    vmulpd( zmm30, zmm17, zmm17 )                               \
    vmulpd( zmm30, zmm25, zmm25 )                               \
    vmulpd( zmm30, zmm18, zmm18 )                               \
    vmulpd( zmm30, zmm19, zmm19 )                               \
    vmulpd( zmm30, zmm22, zmm22 )                               \
    vmulpd( zmm30, zmm20, zmm20 )                               \
    vmulpd( zmm30, zmm21, zmm21 )                               \
    vmulpd( zmm30, zmm23, zmm23 )                               \
    STORE_ROWSTORED_C_24MASKx8_B0                               \
    jmp(.CONCLUDE)                                              \
    /* m_left 24masked edge kernel begins */                    \
    /* m_left 12 edge kernel begins */                          \
    label(.EDGE12XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB12DLOOPKITER)                                     \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB12PREFETCHLOOP)                                     \
																\
    label(.SUB12LOOP1)                                          \
																\
        UNROLL_K_LOOP_12x8_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB12LOOP1)                                            \
																\
    label(.SUB12PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB12TAILITER)                                         \
																\
    label(.SUB12LOOP2)                                          \
        UNROLL_K_LOOP_12x8_L2                                   \
        sub(imm(1), rsi)                                        \
    jnz(.SUB12LOOP2)                                            \
    label(.SUB12TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB12TAIL)                                             \
																\
    label(.SUB12LOOP3)                                          \
        UNROLL_K_LOOP_12x8_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB12LOOP3)                                            \
																\
    label(.SUB12TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB12DPOSTACCUM)                                        \
																\
    label(.SUB12DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),ymm1 )                               \
        add( r10,rax )                                          \
        K_LOOP_12x8(0, 1)                                       \
        dec(rsi)                                                \
    jne(.SUB12DLOOPKLEFT)                                       \
																\
    label(.SUB12DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB12DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
    jz(.SUB12DROWSTORED)                                        \
																\
    label(.SUB12DCOLSTORED)                                     \
    STORE_COLSTORED_C_12x8_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DROWSTORED)                                     \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30, zmm6, zmm6 )                                 \
    vmulpd( zmm30, zmm7, zmm7 )                                 \
    vmulpd( zmm30, zmm8, zmm8 )                                 \
    vmulpd( zmm30, zmm9, zmm9 )                                 \
    vmulpd( zmm30, zmm10, zmm10 )                               \
    vmulpd( zmm30, zmm11, zmm11 )                               \
    vmulpd( zmm30, zmm12, zmm12 )                               \
    vmulpd( zmm30, zmm13, zmm13 )                               \
    vmulpd( zmm30, zmm14, zmm14 )                               \
    vmulpd( zmm30, zmm15, zmm15 )                               \
    vmulpd( zmm30, zmm16, zmm16 )                               \
    vmulpd( zmm30, zmm17, zmm17 )                               \
    vmulpd( zmm30, zmm18, zmm18 )                               \
    vmulpd( zmm30, zmm19, zmm19 )                               \
    vmulpd( zmm30, zmm22, zmm22 )                               \
    vmulpd( zmm30, zmm20, zmm20 )                               \
    vmulpd( zmm30, zmm21, zmm21 )                               \
    vmulpd( zmm30, zmm23, zmm23 )                               \
    STORE_ROWSTORED_C_12x8_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB12DROWSTORBZ)                                        \
																\
    label(.SUB12DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_12x8_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DROWSTORBZ)                                     \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30, zmm6, zmm6 )                                 \
    vmulpd( zmm30, zmm7, zmm7 )                                 \
    vmulpd( zmm30, zmm8, zmm8 )                                 \
    vmulpd( zmm30, zmm9, zmm9 )                                 \
    vmulpd( zmm30, zmm10, zmm10 )                               \
    vmulpd( zmm30, zmm11, zmm11 )                               \
    vmulpd( zmm30, zmm12, zmm12 )                               \
    vmulpd( zmm30, zmm13, zmm13 )                               \
    vmulpd( zmm30, zmm14, zmm14 )                               \
    vmulpd( zmm30, zmm15, zmm15 )                               \
    vmulpd( zmm30, zmm16, zmm16 )                               \
    vmulpd( zmm30, zmm17, zmm17 )                               \
    vmulpd( zmm30, zmm18, zmm18 )                               \
    vmulpd( zmm30, zmm19, zmm19 )                               \
    vmulpd( zmm30, zmm22, zmm22 )                               \
    vmulpd( zmm30, zmm20, zmm20 )                               \
    vmulpd( zmm30, zmm21, zmm21 )                               \
    vmulpd( zmm30, zmm23, zmm23 )                               \
    STORE_ROWSTORED_C_12x8_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 11 edge kernel begins */                          \
    label(.EDGE11XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB11PREFETCHLOOP)                                     \
    label(.SUB11LOOP1)                                          \
																\
        UNROLL_K_LOOP_11x8_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP1)                                            \
																\
    label(.SUB11PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB11TAILITER)                                         \
																\
    label(.SUB11LOOP2)                                          \
        UNROLL_K_LOOP_11x8_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP2)                                            \
    label(.SUB11TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB11TAIL)                                             \
    label(.SUB11LOOP3)                                          \
        UNROLL_K_LOOP_11x8_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP3)                                            \
																\
    label(.SUB11TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB11DPOSTACCUM)                                        \
																\
    label(.SUB11DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),xmm1 )                               \
        vmovsd( 0x50(rax),xmm2 )                                \
        add( r10,rax )                                          \
        K_LOOP_11x8(0, 1, 2)                                    \
        dec(rsi)                                                \
    jne(.SUB11DLOOPKLEFT)                                       \
																\
    label(.SUB11DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB11DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB11DROWSTORED)                                        \
																\
    label(.SUB11DCOLSTORED)                                     \
    STORE_COLSTORED_C_11x8_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DROWSTORED)                                     \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30, zmm6, zmm6 )                                 \
    vmulpd( zmm30, zmm7, zmm7 )                                 \
    vmulpd( zmm30, zmm28, zmm28 )                               \
    vmulpd( zmm30, zmm8, zmm8 )                                 \
    vmulpd( zmm30, zmm9, zmm9 )                                 \
    vmulpd( zmm30, zmm29, zmm29 )                               \
    vmulpd( zmm30, zmm10, zmm10 )                               \
    vmulpd( zmm30, zmm11, zmm11 )                               \
    vmulpd( zmm30, zmm26, zmm26 )                               \
    vmulpd( zmm30, zmm12, zmm12 )                               \
    vmulpd( zmm30, zmm13, zmm13 )                               \
    vmulpd( zmm30, zmm27, zmm27 )                               \
    vmulpd( zmm30, zmm14, zmm14 )                               \
    vmulpd( zmm30, zmm15, zmm15 )                               \
    vmulpd( zmm30, zmm24, zmm24 )                               \
    vmulpd( zmm30, zmm16, zmm16 )                               \
    vmulpd( zmm30, zmm17, zmm17 )                               \
    vmulpd( zmm30, zmm25, zmm25 )                               \
    vmulpd( zmm30, zmm18, zmm18 )                               \
    vmulpd( zmm30, zmm19, zmm19 )                               \
    vmulpd( zmm30, zmm22, zmm22 )                               \
    vmulpd( zmm30, zmm20, zmm20 )                               \
    vmulpd( zmm30, zmm21, zmm21 )                               \
    vmulpd( zmm30, zmm23, zmm23 )                               \
    STORE_ROWSTORED_C_11x8_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB11DROWSTORBZ)                                        \
    label(.SUB11DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_11x8_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DROWSTORBZ)                                     \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30, zmm6, zmm6 )                                 \
    vmulpd( zmm30, zmm7, zmm7 )                                 \
    vmulpd( zmm30, zmm28, zmm28 )                               \
    vmulpd( zmm30, zmm8, zmm8 )                                 \
    vmulpd( zmm30, zmm9, zmm9 )                                 \
    vmulpd( zmm30, zmm29, zmm29 )                               \
    vmulpd( zmm30, zmm10, zmm10 )                               \
    vmulpd( zmm30, zmm11, zmm11 )                               \
    vmulpd( zmm30, zmm26, zmm26 )                               \
    vmulpd( zmm30, zmm12, zmm12 )                               \
    vmulpd( zmm30, zmm13, zmm13 )                               \
    vmulpd( zmm30, zmm27, zmm27 )                               \
    vmulpd( zmm30, zmm14, zmm14 )                               \
    vmulpd( zmm30, zmm15, zmm15 )                               \
    vmulpd( zmm30, zmm24, zmm24 )                               \
    vmulpd( zmm30, zmm16, zmm16 )                               \
    vmulpd( zmm30, zmm17, zmm17 )                               \
    vmulpd( zmm30, zmm25, zmm25 )                               \
    vmulpd( zmm30, zmm18, zmm18 )                               \
    vmulpd( zmm30, zmm19, zmm19 )                               \
    vmulpd( zmm30, zmm22, zmm22 )                               \
    vmulpd( zmm30, zmm20, zmm20 )                               \
    vmulpd( zmm30, zmm21, zmm21 )                               \
    vmulpd( zmm30, zmm23, zmm23 )                               \
    STORE_ROWSTORED_C_11x8_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 10 edge kernel begins */                          \
    label(.EDGE10XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB10PREFETCHLOOP)                                     \
    label(.SUB10LOOP1)                                          \
																\
        UNROLL_K_LOOP_10x8_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP1)                                            \
																\
    label(.SUB10PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB10TAILITER)                                         \
																\
    label(.SUB10LOOP2)                                          \
        UNROLL_K_LOOP_10x8_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP2)                                            \
    label(.SUB10TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB10TAIL)                                             \
    label(.SUB10LOOP3)                                          \
        UNROLL_K_LOOP_10x8_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP3)                                            \
																\
    label(.SUB10TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB10DPOSTACCUM)                                        \
																\
    label(.SUB10DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),xmm1 )                               \
        add( r10,rax )                                          \
        K_LOOP_10x8(0, 1)                                       \
        dec(rsi)                                                \
    jne(.SUB10DLOOPKLEFT)                                       \
																\
    label(.SUB10DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB10DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB10DROWSTORED)                                        \
																\
    label(.SUB10DCOLSTORED)                                     \
    STORE_COLSTORED_C_10x8_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DROWSTORED)                                     \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30, zmm6, zmm6 )                                 \
    vmulpd( zmm30, zmm7, zmm7 )                                 \
    vmulpd( zmm30, zmm8, zmm8 )                                 \
    vmulpd( zmm30, zmm9, zmm9 )                                 \
    vmulpd( zmm30, zmm10, zmm10 )                               \
    vmulpd( zmm30, zmm11, zmm11 )                               \
    vmulpd( zmm30, zmm12, zmm12 )                               \
    vmulpd( zmm30, zmm13, zmm13 )                               \
    vmulpd( zmm30, zmm14, zmm14 )                               \
    vmulpd( zmm30, zmm15, zmm15 )                               \
    vmulpd( zmm30, zmm16, zmm16 )                               \
    vmulpd( zmm30, zmm17, zmm17 )                               \
    vmulpd( zmm30, zmm18, zmm18 )                               \
    vmulpd( zmm30, zmm19, zmm19 )                               \
    vmulpd( zmm30, zmm22, zmm22 )                               \
    vmulpd( zmm30, zmm20, zmm20 )                               \
    vmulpd( zmm30, zmm21, zmm21 )                               \
    vmulpd( zmm30, zmm23, zmm23 )                               \
    STORE_ROWSTORED_C_10x8_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB10DROWSTORBZ)                                        \
    label(.SUB10DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_10x8_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DROWSTORBZ)                                     \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30, zmm6, zmm6 )                                 \
    vmulpd( zmm30, zmm7, zmm7 )                                 \
    vmulpd( zmm30, zmm8, zmm8 )                                 \
    vmulpd( zmm30, zmm9, zmm9 )                                 \
    vmulpd( zmm30, zmm10, zmm10 )                               \
    vmulpd( zmm30, zmm11, zmm11 )                               \
    vmulpd( zmm30, zmm12, zmm12 )                               \
    vmulpd( zmm30, zmm13, zmm13 )                               \
    vmulpd( zmm30, zmm14, zmm14 )                               \
    vmulpd( zmm30, zmm15, zmm15 )                               \
    vmulpd( zmm30, zmm16, zmm16 )                               \
    vmulpd( zmm30, zmm17, zmm17 )                               \
    vmulpd( zmm30, zmm18, zmm18 )                               \
    vmulpd( zmm30, zmm19, zmm19 )                               \
    vmulpd( zmm30, zmm22, zmm22 )                               \
    vmulpd( zmm30, zmm20, zmm20 )                               \
    vmulpd( zmm30, zmm21, zmm21 )                               \
    vmulpd( zmm30, zmm23, zmm23 )                               \
    STORE_ROWSTORED_C_10x8_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 9 edge kernel begins */                           \
    label(.EDGE9XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB9DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB9PREFETCHLOOP)                                      \
																\
    label(.SUB9LOOP1)                                           \
																\
        UNROLL_K_LOOP_9x8_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB9LOOP1)                                             \
																\
    label(.SUB9PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB9TAILITER)                                          \
																\
    label(.SUB9LOOP2)                                           \
        UNROLL_K_LOOP_9x8_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB9LOOP2)                                             \
    label(.SUB9TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB9TAIL)                                              \
																\
    label(.SUB9LOOP3)                                           \
																\
        UNROLL_K_LOOP_9x8_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB9LOOP3)                                             \
																\
    label(.SUB9TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB9DPOSTACCUM)                                         \
																\
    label(.SUB9DLOOPKLEFT)                                      \
        vmovupd( mem(rax),zmm0 )                                \
        vmovsd( 0x40(rax),xmm1 )                                \
        add( r10,rax )                                          \
        K_LOOP_9x8(0, 1)                                        \
        dec(rsi)                                                \
    jne(.SUB9DLOOPKLEFT)                                        \
																\
    label(.SUB9DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB9DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB9DROWSTORED)                                         \
																\
    label(.SUB9DCOLSTORED)                                      \
    STORE_COLSTORED_C_9x8_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DROWSTORED)                                      \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    vmulpd( zmm30,zmm20,zmm20 )                                 \
    vmulpd( zmm30,zmm21,zmm21 )                                 \
    vmulpd( zmm30,zmm23,zmm23 )                                 \
    STORE_ROWSTORED_C_9x8_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB9DROWSTORBZ)                                         \
    label(.SUB9DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_9x8_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DROWSTORBZ)                                      \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    vmulpd( zmm30,zmm20,zmm20 )                                 \
    vmulpd( zmm30,zmm21,zmm21 )                                 \
    vmulpd( zmm30,zmm23,zmm23 )                                 \
    STORE_ROWSTORED_C_9x8_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 8 edge kernel begins */                           \
    label(.EDGE8XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB8DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB8PREFETCHLOOP)                                      \
    label(.SUB8LOOP1)                                           \
        UNROLL_K_LOOP_8x8_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB8LOOP1)                                             \
																\
    label(.SUB8PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB8TAILITER)                                          \
																\
    label(.SUB8LOOP2)                                           \
        UNROLL_K_LOOP_8x8_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB8LOOP2)                                             \
    label(.SUB8TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB8TAIL)                                              \
																\
    label(.SUB8LOOP3)                                           \
        UNROLL_K_LOOP_8x8_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB8LOOP3)                                             \
																\
    vaddpd(zmm7, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm8, zmm8)                                    \
    vaddpd(zmm11, zmm10, zmm10)                                 \
    vaddpd(zmm13, zmm12, zmm12)                                 \
    vaddpd(zmm15, zmm14, zmm14)                                 \
    vaddpd(zmm17, zmm16, zmm16)                                 \
    vaddpd(zmm19, zmm18, zmm18)                                 \
    vaddpd(zmm21, zmm20, zmm20)                                 \
																\
    label(.SUB8TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB8DPOSTACCUM)                                         \
																\
    label(.SUB8DLOOPKLEFT)                                      \
        vmovupd( mem(rax),zmm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_8x8_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB8DLOOPKLEFT)                                        \
																\
    label(.SUB8DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB8DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB8DROWSTORED)                                         \
																\
    label(.SUB8DCOLSTORED)                                      \
																\
    STORE_COLSTORED_C_8x8_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DROWSTORED)                                      \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm20,zmm20 )                                 \
    STORE_ROWSTORED_C_8x8_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB8DROWSTORBZ)                                         \
    label(.SUB8DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_8x8_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DROWSTORBZ)                                      \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm20,zmm20 )                                 \
    STORE_ROWSTORED_C_8x8_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 16 mask edge kernel begins */                     \
    label(.EDGE16MASKXN)                                        \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    sub(imm(8), rsi)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB16MASKDLOOPKITER)                                 \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB16MASKPREFETCHLOOP)                                 \
																\
    label(.SUB16MASKLOOP1)                                      \
																\
        UNROLL_K_LOOP_16MASKx8_L1                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP1)                                        \
																\
    label(.SUB16MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.SUB16MASKTAILITER)                                     \
																\
    label(.SUB16MASKLOOP2)                                      \
																\
        UNROLL_K_LOOP_16MASKx8_L2                               \
        sub(imm(1), rsi)                                        \
    jnz(.SUB16MASKLOOP2)                                        \
    label(.SUB16MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB16MASKTAIL)                                         \
																\
    label(.SUB16MASKLOOP3)                                      \
																\
        UNROLL_K_LOOP_16MASKx8_L1                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP3)                                        \
																\
    label(.SUB16MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB16MASKDPOSTACCUM)                                    \
																\
    label(.SUB16MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )                    \
        add( r10,rax )                                          \
        K_LOOP_16MASKx8(0, 1)                                   \
        dec(rsi)                                                \
    jne(.SUB16MASKDLOOPKLEFT)                                   \
																\
    label(.SUB16MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.SUB16MASKDBETAZERO)                                     \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB16MASKDROWSTORED)                                    \
																\
    label(.SUB16MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_16MASKx8_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDROWSTORED)                                 \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    vmulpd( zmm30,zmm20,zmm20 )                                 \
    vmulpd( zmm30,zmm21,zmm21 )                                 \
    vmulpd( zmm30,zmm23,zmm23 )                                 \
    STORE_ROWSTORED_C_16MASKx8_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB16MASKDROWSTORBZ)                                    \
    label(.SUB16MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_16MASKx8_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDROWSTORBZ)                                 \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    vmulpd( zmm30,zmm20,zmm20 )                                 \
    vmulpd( zmm30,zmm21,zmm21 )                                 \
    vmulpd( zmm30,zmm23,zmm23 )                                 \
    STORE_ROWSTORED_C_16MASKx8_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 4 edge kernel begins */                           \
    label(.EDGE4XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB4DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB4PREFETCHLOOP)                                      \
    label(.SUB4LOOP1)                                           \
        UNROLL_K_LOOP_4x8_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB4LOOP1)                                             \
																\
    label(.SUB4PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB4TAILITER)                                          \
																\
    label(.SUB4LOOP2)                                           \
																\
        UNROLL_K_LOOP_4x8_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB4LOOP2)                                             \
    label(.SUB4TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB4TAIL)                                              \
																\
    label(.SUB4LOOP3)                                           \
																\
        UNROLL_K_LOOP_4x8_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB4LOOP3)                                             \
																\
    vaddpd(ymm7, ymm6, ymm6)                                    \
    vaddpd(ymm9, ymm8, ymm8)                                    \
    vaddpd(ymm11, ymm10, ymm10)                                 \
    vaddpd(ymm13, ymm12, ymm12)                                 \
    vaddpd(ymm15, ymm14, ymm14)                                 \
    vaddpd(ymm17, ymm16, ymm16)                                 \
    vaddpd(ymm19, ymm18, ymm18)                                 \
    vaddpd(ymm21, ymm20, ymm20)                                 \
																\
    label(.SUB4TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB4DPOSTACCUM)                                         \
																\
    label(.SUB4DLOOPKLEFT)                                      \
        vmovupd( mem(rax),ymm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_4x8_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB4DLOOPKLEFT)                                        \
																\
    label(.SUB4DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB4DBETAZERO)                                          \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB4DROWSTORED)                                         \
																\
    label(.SUB4DCOLSTORED)                                      \
    STORE_COLSTORED_C_4x8_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DROWSTORED)                                      \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm20,zmm20 )                                 \
    STORE_ROWSTORED_C_4x8_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB4DROWSTORBZ)                                         \
    label(.SUB4DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_4x8_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DROWSTORBZ)                                      \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm20,zmm20 )                                 \
    STORE_ROWSTORED_C_4x8_B0                                    \
    jmp(.CONCLUDE)                                              \
    /* m_left 2 edge kernel begins */                           \
    label(.EDGE2XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB2DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB2PREFETCHLOOP)                                      \
    label(.SUB2LOOP1)                                           \
        UNROLL_K_LOOP_2x8_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB2LOOP1)                                             \
																\
    label(.SUB2PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB2TAILITER)                                          \
																\
    label(.SUB2LOOP2)                                           \
																\
        UNROLL_K_LOOP_2x8_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB2LOOP2)                                             \
    label(.SUB2TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB2TAIL)                                              \
																\
    label(.SUB2LOOP3)                                           \
        UNROLL_K_LOOP_2x8_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB2LOOP3)                                             \
																\
    vaddpd(xmm7, xmm6, xmm6)                                    \
    vaddpd(xmm9, xmm8, xmm8)                                    \
    vaddpd(xmm11, xmm10, xmm10)                                 \
    vaddpd(xmm13, xmm12, xmm12)                                 \
    vaddpd(xmm15, xmm14, xmm14)                                 \
    vaddpd(xmm17, xmm16, xmm16)                                 \
    vaddpd(xmm19, xmm18, xmm18)                                 \
    vaddpd(xmm21, xmm20, xmm20)                                 \
																\
    label(.SUB2TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB2DPOSTACCUM)                                         \
																\
    label(.SUB2DLOOPKLEFT)                                      \
        vmovupd( mem(rax),xmm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_2x8_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB2DLOOPKLEFT)                                        \
																\
    label(.SUB2DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB2DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB2DROWSTORED)                                         \
																\
    label(.SUB2DCOLSTORED)                                      \
    STORE_COLSTORED_C_2x8_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DROWSTORED)                                      \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm20,zmm20 )                                 \
    STORE_ROWSTORED_C_2x8_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB2DROWSTORBZ)                                         \
    label(.SUB2DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_2x8_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DROWSTORBZ)                                      \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm20,zmm20 )                                 \
    STORE_ROWSTORED_C_2x8_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 1 edge kernel begins */                           \
    label(.EDGE1XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB1TAIL)                                               \
    label(.SUB1LOOP3)                                           \
        UNROLL_K_LOOP_1x8_L1                                    \
        dec(rsi)                                                \
        jnz(.SUB1LOOP3)                                         \
																\
    vaddpd(xmm7, xmm6, xmm6)                                    \
    vaddpd(xmm9, xmm8, xmm8)                                    \
    vaddpd(xmm11, xmm10, xmm10)                                 \
    vaddpd(xmm13, xmm12, xmm12)                                 \
    vaddpd(xmm15, xmm14, xmm14)                                 \
    vaddpd(xmm17, xmm16, xmm16)                                 \
    vaddpd(xmm19, xmm18, xmm18)                                 \
    vaddpd(xmm21, xmm20, xmm20)                                 \
    label(.SUB1TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB1DPOSTACCUM)                                         \
																\
    label(.SUB1DLOOPKLEFT)                                      \
        vmovsd( mem(rax),xmm0 )                                 \
        add( r10,rax )                                          \
        K_LOOP_1x8_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB1DLOOPKLEFT)                                        \
																\
    label(.SUB1DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB1DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB1DROWSTORED)                                         \
																\
    label(.SUB1DCOLSTORED)                                      \
    STORE_COLSTORED_C_1x8_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DROWSTORED)                                      \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm20,zmm20 )                                 \
    STORE_ROWSTORED_C_1x8_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB1DROWSTORBZ)                                         \
    label(.SUB1DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_1x8_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DROWSTORBZ)                                      \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm20,zmm20 )                                 \
    STORE_ROWSTORED_C_1x8_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 8 mask edge kernel begins */                      \
    label(.EDGE8MASKXN)                                         \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
																\
    mov(imm(1), rdx)                                            \
    mov(imm(4), rsi)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(3))                                            \
																\
    mov(imm(1), rdx)                                            \
    mov(imm(2), rsi)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(4))                                            \
																\
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.EDGE8MASKDLOOPKITER)                                 \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.EDGE8MASKPREFETCHLOOP)                                 \
    label(.EDGE8MASKLOOP1)                                      \
        UNROLL_K_LOOP_8MASKx8_L1                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP1)                                        \
																\
    label(.EDGE8MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.EDGE8MASKTAILITER)                                     \
																\
    label(.EDGE8MASKLOOP2)                                      \
																\
        UNROLL_K_LOOP_8MASKx8_L2                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP2)                                        \
    label(.EDGE8MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.EDGE8MASKTAIL)                                         \
																\
    label(.EDGE8MASKLOOP3)                                      \
																\
        UNROLL_K_LOOP_8MASKx8_L1                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP3)                                        \
																\
    vaddpd(zmm7, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm8, zmm8)                                    \
    vaddpd(zmm11, zmm10, zmm10)                                 \
    vaddpd(zmm13, zmm12, zmm12)                                 \
    vaddpd(zmm15, zmm14, zmm14)                                 \
    vaddpd(zmm17, zmm16, zmm16)                                 \
    vaddpd(zmm19, zmm18, zmm18)                                 \
    vaddpd(zmm21, zmm20, zmm20)                                 \
																\
    label(.EDGE8MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.EDGE8MASKDPOSTACCUM)                                    \
																\
    label(.EDGE8MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 MASK_KZ(2) )                     \
        add( r10,rax )                                          \
        K_LOOP_8MASKx8_SET1(0)                                  \
        dec(rsi)                                                \
    jne(.EDGE8MASKDLOOPKLEFT)                                   \
																\
    label(.EDGE8MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.EDGE8MASKDBETAZERO)                                     \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.EDGE8MASKDROWSTORED)                                    \
																\
    label(.EDGE8MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_8MASKx8_Bn                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDROWSTORED)                                 \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm20,zmm20 )                                 \
    STORE_ROWSTORED_C_8MASKx8_Bn                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.EDGE8MASKDROWSTORBZ)                                    \
																\
    label(.EDGE8MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_8MASKx8_B0                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDROWSTORBZ)                                 \
    mov(var(alpha), rdx)                                        \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm20,zmm20 )                                 \
    STORE_ROWSTORED_C_8MASKx8_B0                                \
    jmp(.CONCLUDE)


#define M_LEFT_24x7												\
																\
    /* m_left 20 edge kernel begins */                          \
    label(.EDGE20XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB20PREFETCHLOOP)                                     \
    label(.SUB20LOOP1)                                          \
																\
        UNROLL_K_LOOP_20x7_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP1)                                            \
																\
    label(.SUB20PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB20TAILITER)                                         \
																\
    label(.SUB20LOOP2)                                          \
        UNROLL_K_LOOP_20x7_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP2)                                            \
    label(.SUB20TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB20TAIL)                                             \
    label(.SUB20LOOP3)                                          \
        UNROLL_K_LOOP_20x7_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP3)                                            \
																\
    label(.SUB20TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB20DPOSTACCUM)                                        \
    label(.SUB20DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax), ymm2 )                              \
        add( r10,rax )                                          \
        K_LOOP_20x7(0, 1, 2)                                    \
        dec(rsi)                                                \
    jne(.SUB20DLOOPKLEFT)                                       \
																\
    label(.SUB20DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB20DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB20DROWSTORED)                                        \
																\
    label(.SUB20DCOLSTORED)                                     \
    STORE_COLSTORED_C_20x7_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_20xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB20DROWSTORBZ)                                        \
																\
    label(.SUB20DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_20x7_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_20xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 18 edge kernel begins */                          \
    label(.EDGE18XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB18PREFETCHLOOP)                                     \
    label(.SUB18LOOP1)                                          \
																\
        UNROLL_K_LOOP_18x7_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP1)                                            \
																\
    label(.SUB18PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB18TAILITER)                                         \
																\
    label(.SUB18LOOP2)                                          \
        UNROLL_K_LOOP_18x7_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP2)                                            \
    label(.SUB18TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB18TAIL)                                             \
    label(.SUB18LOOP3)                                          \
        UNROLL_K_LOOP_18x7_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP3)                                            \
																\
    label(.SUB18TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB18DPOSTACCUM)                                        \
																\
    label(.SUB18DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax), xmm2 )                              \
        add( r10,rax )                                          \
        K_LOOP_18x7(0, 1, 2)                                    \
        dec(rsi)                                                \
    jne(.SUB18DLOOPKLEFT)                                       \
																\
    label(.SUB18DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB18DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB18DROWSTORED)                                        \
																\
    label(.SUB18DCOLSTORED)                                     \
    STORE_COLSTORED_C_18x7_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_18xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB18DROWSTORBZ)                                        \
																\
    label(.SUB18DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_18x7_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_18xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 17 edge kernel begins */                          \
    label(.EDGE17XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB17PREFETCHLOOP)                                     \
    label(.SUB17LOOP1)                                          \
																\
        UNROLL_K_LOOP_17x7_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP1)                                            \
																\
    label(.SUB17PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB17TAILITER)                                         \
																\
    label(.SUB17LOOP2)                                          \
        UNROLL_K_LOOP_17x7_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP2)                                            \
    label(.SUB17TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB17TAIL)                                             \
    label(.SUB17LOOP3)                                          \
        UNROLL_K_LOOP_17x7_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP3)                                            \
    label(.SUB17TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB17DPOSTACCUM)                                        \
    label(.SUB17DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovsd ( 0x80(rax),xmm2 )                               \
        add( r10,rax )                                          \
        K_LOOP_17x7(0, 1, 2)                                    \
        dec(rsi)                                                \
    jne(.SUB17DLOOPKLEFT)                                       \
																\
    label(.SUB17DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB17DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB17DROWSTORED)                                        \
																\
    label(.SUB17DCOLSTORED)                                     \
    STORE_COLSTORED_C_17x7_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_17xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB17DROWSTORBZ)                                        \
																\
    label(.SUB17DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_17x7_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_17xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 16 edge kernel begins */                          \
    label(.EDGE16XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB16DLOOPKITER)                                     \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB16PREFETCHLOOP)                                     \
    label(.SUB16LOOP1)                                          \
																\
        UNROLL_K_LOOP_16x7_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP1)                                            \
																\
    label(.SUB16PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB16TAILITER)                                         \
																\
    label(.SUB16LOOP2)                                          \
        UNROLL_K_LOOP_16x7_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP2)                                            \
    label(.SUB16TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB16TAIL)                                             \
																\
    label(.SUB16LOOP3)                                          \
																\
        UNROLL_K_LOOP_16x7_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP3)                                            \
																\
    label(.SUB16TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB16DPOSTACCUM)                                        \
																\
    label(.SUB16DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        add( r10,rax )                                          \
																\
        K_LOOP_16x7(0, 1)                                       \
        dec(rsi)                                                \
    jne(.SUB16DLOOPKLEFT)                                       \
																\
    label(.SUB16DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB16DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB16DROWSTORED)                                        \
																\
    label(.SUB16DCOLSTORED)                                     \
    STORE_COLSTORED_C_16x7_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_16xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16DROWSTORBZ)                                        \
																\
    label(.SUB16DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_16x7_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_16xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /**********  M_Left 24 masked kernel **************/        \
    label(.EDGE24MASKXN)                                        \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    sub(imm(16), rsi)                                           \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB24MASKPREFETCHLOOP)                                 \
    label(.SUB24MASKLOOP1)                                      \
																\
        UNROLL_K_LOOP_24MASKx7_L1                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP1)                                        \
																\
    label(.SUB24MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.SUB24MASKTAILITER)                                     \
																\
    label(.SUB24MASKLOOP2)                                      \
        UNROLL_K_LOOP_24MASKx7_L2                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP2)                                        \
    label(.SUB24MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB24MASKTAIL)                                         \
    label(.SUB24MASKLOOP3)                                      \
        UNROLL_K_LOOP_24MASKx7_L1                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP3)                                        \
    label(.SUB24MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB24MASKDPOSTACCUM)                                    \
    label(.SUB24MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax),zmm2 MASK_KZ(2) )                    \
        add( r10,rax )                                          \
        K_LOOP_24MASKx7(0, 1, 2)                                \
        dec(rsi)                                                \
    jne(.SUB24MASKDLOOPKLEFT)                                   \
    label(.SUB24MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.SUB24MASKDBETAZERO)                                     \
    cmp(imm(8), rdi)                                            \
    jz(.SUB24MASKDROWSTORED)                                    \
																\
    label(.SUB24MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_24MASKx7_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_24MASKxN_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.SUB24MASKDROWSTORBZ)                                    \
																\
    label(.SUB24MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_24MASKx7_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_24MASKxN_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 12 edge kernel begins */                          \
    label(.EDGE12XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB12DLOOPKITER)                                     \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB12PREFETCHLOOP)                                     \
																\
    label(.SUB12LOOP1)                                          \
																\
        UNROLL_K_LOOP_12x7_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB12LOOP1)                                            \
																\
    label(.SUB12PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB12TAILITER)                                         \
																\
    label(.SUB12LOOP2)                                          \
        UNROLL_K_LOOP_12x7_L2                                   \
        sub(imm(1), rsi)                                        \
    jnz(.SUB12LOOP2)                                            \
    label(.SUB12TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB12TAIL)                                             \
																\
    label(.SUB12LOOP3)                                          \
        UNROLL_K_LOOP_12x7_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB12LOOP3)                                            \
																\
    label(.SUB12TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB12DPOSTACCUM)                                        \
																\
    label(.SUB12DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),ymm1 )                               \
        add( r10,rax )                                          \
        K_LOOP_12x7(0, 1)                                       \
        dec(rsi)                                                \
    jne(.SUB12DLOOPKLEFT)                                       \
																\
    label(.SUB12DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB12DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
    jz(.SUB12DROWSTORED)                                        \
																\
    label(.SUB12DCOLSTORED)                                     \
    STORE_COLSTORED_C_12x7_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    STORE_ROWSTORED_C_12xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB12DROWSTORBZ)                                        \
																\
    label(.SUB12DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_12x7_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    STORE_ROWSTORED_C_12xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 11 edge kernel begins */                          \
    label(.EDGE11XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB11PREFETCHLOOP)                                     \
    label(.SUB11LOOP1)                                          \
																\
        UNROLL_K_LOOP_11x7_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP1)                                            \
																\
    label(.SUB11PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB11TAILITER)                                         \
																\
    label(.SUB11LOOP2)                                          \
        UNROLL_K_LOOP_11x7_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP2)                                            \
    label(.SUB11TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB11TAIL)                                             \
    label(.SUB11LOOP3)                                          \
        UNROLL_K_LOOP_11x7_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP3)                                            \
																\
    label(.SUB11TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB11DPOSTACCUM)                                        \
																\
    label(.SUB11DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),xmm1 )                               \
        vmovsd( 0x50(rax),xmm2 )                                \
        add( r10,rax )                                          \
        K_LOOP_11x7(0, 1, 2)                                    \
        dec(rsi)                                                \
    jne(.SUB11DLOOPKLEFT)                                       \
																\
    label(.SUB11DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB11DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB11DROWSTORED)                                        \
																\
    label(.SUB11DCOLSTORED)                                     \
    STORE_COLSTORED_C_11x7_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_11xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB11DROWSTORBZ)                                        \
    label(.SUB11DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_11x7_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_11xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 10 edge kernel begins */                          \
    label(.EDGE10XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB10PREFETCHLOOP)                                     \
    label(.SUB10LOOP1)                                          \
																\
        UNROLL_K_LOOP_10x7_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP1)                                            \
																\
    label(.SUB10PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB10TAILITER)                                         \
																\
    label(.SUB10LOOP2)                                          \
        UNROLL_K_LOOP_10x7_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP2)                                            \
    label(.SUB10TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB10TAIL)                                             \
    label(.SUB10LOOP3)                                          \
        UNROLL_K_LOOP_10x7_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP3)                                            \
																\
    label(.SUB10TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB10DPOSTACCUM)                                        \
																\
    label(.SUB10DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),xmm1 )                               \
        add( r10,rax )                                          \
        K_LOOP_10x7(0, 1)                                       \
        dec(rsi)                                                \
    jne(.SUB10DLOOPKLEFT)                                       \
																\
    label(.SUB10DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB10DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB10DROWSTORED)                                        \
																\
    label(.SUB10DCOLSTORED)                                     \
    STORE_COLSTORED_C_10x7_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_10xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB10DROWSTORBZ)                                        \
    label(.SUB10DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_10x7_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_10xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 9 edge kernel begins */                           \
    label(.EDGE9XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB9DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB9PREFETCHLOOP)                                      \
																\
    label(.SUB9LOOP1)                                           \
																\
        UNROLL_K_LOOP_9x7_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB9LOOP1)                                             \
																\
    label(.SUB9PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB9TAILITER)                                          \
																\
    label(.SUB9LOOP2)                                           \
        UNROLL_K_LOOP_9x7_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB9LOOP2)                                             \
    label(.SUB9TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB9TAIL)                                              \
																\
    label(.SUB9LOOP3)                                           \
																\
        UNROLL_K_LOOP_9x7_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB9LOOP3)                                             \
																\
    label(.SUB9TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB9DPOSTACCUM)                                         \
																\
    label(.SUB9DLOOPKLEFT)                                      \
        vmovupd( mem(rax),zmm0 )                                \
        vmovsd( 0x40(rax),xmm1 )                                \
        add( r10,rax )                                          \
        K_LOOP_9x7(0, 1)                                        \
        dec(rsi)                                                \
    jne(.SUB9DLOOPKLEFT)                                        \
																\
    label(.SUB9DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB9DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB9DROWSTORED)                                         \
																\
    label(.SUB9DCOLSTORED)                                      \
    STORE_COLSTORED_C_9x7_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_9xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB9DROWSTORBZ)                                         \
    label(.SUB9DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_9x7_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_9xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 8 edge kernel begins */                           \
    label(.EDGE8XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB8DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB8PREFETCHLOOP)                                      \
    label(.SUB8LOOP1)                                           \
        UNROLL_K_LOOP_8x7_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB8LOOP1)                                             \
																\
    label(.SUB8PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB8TAILITER)                                          \
																\
    label(.SUB8LOOP2)                                           \
        UNROLL_K_LOOP_8x7_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB8LOOP2)                                             \
    label(.SUB8TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB8TAIL)                                              \
																\
    label(.SUB8LOOP3)                                           \
        UNROLL_K_LOOP_8x7_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB8LOOP3)                                             \
																\
    vaddpd(zmm7, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm8, zmm8)                                    \
    vaddpd(zmm11, zmm10, zmm10)                                 \
    vaddpd(zmm13, zmm12, zmm12)                                 \
    vaddpd(zmm15, zmm14, zmm14)                                 \
    vaddpd(zmm17, zmm16, zmm16)                                 \
    vaddpd(zmm19, zmm18, zmm18)                                 \
																\
    label(.SUB8TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB8DPOSTACCUM)                                         \
																\
    label(.SUB8DLOOPKLEFT)                                      \
        vmovupd( mem(rax),zmm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_8x7_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB8DLOOPKLEFT)                                        \
																\
    label(.SUB8DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB8DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB8DROWSTORED)                                         \
																\
    label(.SUB8DCOLSTORED)                                      \
    STORE_COLSTORED_C_8x7_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    STORE_ROWSTORED_C_8xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB8DROWSTORBZ)                                         \
    label(.SUB8DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_8x7_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    STORE_ROWSTORED_C_8xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 16 mask edge kernel begins */                     \
    label(.EDGE16MASKXN)                                        \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    sub(imm(8), rsi)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
    lea(mem(r11, r9, 4), r15)                                   \
    PREPARE_SCRATCHPAD                                          \
    label(.SUB16MASKDLOOPKITER)                                 \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB16MASKPREFETCHLOOP)                                 \
																\
    label(.SUB16MASKLOOP1)                                      \
																\
        UNROLL_K_LOOP_16MASKx7_L1                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP1)                                        \
																\
    label(.SUB16MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.SUB16MASKTAILITER)                                     \
																\
    label(.SUB16MASKLOOP2)                                      \
																\
        UNROLL_K_LOOP_16MASKx7_L2                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP2)                                        \
    label(.SUB16MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB16MASKTAIL)                                         \
																\
    label(.SUB16MASKLOOP3)                                      \
																\
        UNROLL_K_LOOP_16MASKx7_L1                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP3)                                        \
																\
    label(.SUB16MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB16MASKDPOSTACCUM)                                    \
																\
    label(.SUB16MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )                    \
        add( r10,rax )                                          \
        K_LOOP_16MASKx7(0, 1)                                   \
        dec(rsi)                                                \
    jne(.SUB16MASKDLOOPKLEFT)                                   \
																\
    label(.SUB16MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.SUB16MASKDBETAZERO)                                     \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16MASKDROWSTORED)                                    \
																\
    label(.SUB16MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_16MASKx7_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_16MASKxN_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16MASKDROWSTORBZ)                                    \
																\
    label(.SUB16MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_16MASKx7_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    vmulpd( zmm30,zmm19,zmm19 )                                 \
    vmulpd( zmm30,zmm22,zmm22 )                                 \
    STORE_ROWSTORED_C_16MASKxN_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 4 edge kernel begins */                           \
    label(.EDGE4XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB4DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB4PREFETCHLOOP)                                      \
    label(.SUB4LOOP1)                                           \
        UNROLL_K_LOOP_4x7_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB4LOOP1)                                             \
																\
    label(.SUB4PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB4TAILITER)                                          \
																\
    label(.SUB4LOOP2)                                           \
																\
        UNROLL_K_LOOP_4x7_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB4LOOP2)                                             \
    label(.SUB4TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB4TAIL)                                              \
																\
    label(.SUB4LOOP3)                                           \
																\
        UNROLL_K_LOOP_4x7_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB4LOOP3)                                             \
																\
    vaddpd(ymm7, ymm6, ymm6)                                    \
    vaddpd(ymm9, ymm8, ymm8)                                    \
    vaddpd(ymm11, ymm10, ymm10)                                 \
    vaddpd(ymm13, ymm12, ymm12)                                 \
    vaddpd(ymm15, ymm14, ymm14)                                 \
    vaddpd(ymm17, ymm16, ymm16)                                 \
    vaddpd(ymm19, ymm18, ymm18)                                 \
																\
    label(.SUB4TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB4DPOSTACCUM)                                         \
																\
    label(.SUB4DLOOPKLEFT)                                      \
        vmovupd( mem(rax),ymm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_4x7_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB4DLOOPKLEFT)                                        \
																\
    label(.SUB4DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB4DBETAZERO)                                          \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB4DROWSTORED)                                         \
																\
    label(.SUB4DCOLSTORED)                                      \
    STORE_COLSTORED_C_4x7_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    STORE_ROWSTORED_C_4xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB4DROWSTORBZ)                                         \
    label(.SUB4DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_4x7_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    STORE_ROWSTORED_C_4xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 2 edge kernel begins */                           \
    label(.EDGE2XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB2DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB2PREFETCHLOOP)                                      \
    label(.SUB2LOOP1)                                           \
        UNROLL_K_LOOP_2x7_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB2LOOP1)                                             \
																\
    label(.SUB2PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB2TAILITER)                                          \
																\
    label(.SUB2LOOP2)                                           \
																\
        UNROLL_K_LOOP_2x7_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB2LOOP2)                                             \
    label(.SUB2TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB2TAIL)                                              \
																\
    label(.SUB2LOOP3)                                           \
        UNROLL_K_LOOP_2x7_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB2LOOP3)                                             \
																\
    vaddpd(xmm7, xmm6, xmm6)                                    \
    vaddpd(xmm9, xmm8, xmm8)                                    \
    vaddpd(xmm11, xmm10, xmm10)                                 \
    vaddpd(xmm13, xmm12, xmm12)                                 \
    vaddpd(xmm15, xmm14, xmm14)                                 \
    vaddpd(xmm17, xmm16, xmm16)                                 \
    vaddpd(xmm19, xmm18, xmm18)                                 \
																\
    label(.SUB2TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB2DPOSTACCUM)                                         \
																\
    label(.SUB2DLOOPKLEFT)                                      \
        vmovupd( mem(rax),xmm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_2x7_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB2DLOOPKLEFT)                                        \
																\
    label(.SUB2DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB2DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB2DROWSTORED)                                         \
																\
    label(.SUB2DCOLSTORED)                                      \
    STORE_COLSTORED_C_2x7_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    STORE_ROWSTORED_C_2xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB2DROWSTORBZ)                                         \
    label(.SUB2DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_2x7_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    STORE_ROWSTORED_C_2xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 1 edge kernel begins */                           \
    label(.EDGE1XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB1TAIL)                                               \
    label(.SUB1LOOP3)                                           \
        UNROLL_K_LOOP_1x7_L1                                    \
        dec(rsi)                                                \
        jnz(.SUB1LOOP3)                                         \
																\
																\
    vaddpd(xmm7, xmm6, xmm6)                                    \
    vaddpd(xmm9, xmm8, xmm8)                                    \
    vaddpd(xmm11, xmm10, xmm10)                                 \
    vaddpd(xmm13, xmm12, xmm12)                                 \
    vaddpd(xmm15, xmm14, xmm14)                                 \
    vaddpd(xmm17, xmm16, xmm16)                                 \
    vaddpd(xmm19, xmm18, xmm18)                                 \
																\
    label(.SUB1TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB1DPOSTACCUM)                                         \
																\
    label(.SUB1DLOOPKLEFT)                                      \
        vmovsd( mem(rax),xmm0 )                                 \
        add( r10,rax )                                          \
        K_LOOP_1x7_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB1DLOOPKLEFT)                                        \
																\
    label(.SUB1DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB1DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB1DROWSTORED)                                         \
																\
    label(.SUB1DCOLSTORED)                                      \
    STORE_COLSTORED_C_1x7_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    STORE_ROWSTORED_C_1xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB1DROWSTORBZ)                                         \
    label(.SUB1DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_1x7_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    STORE_ROWSTORED_C_1xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 8 mask edge kernel begins */                      \
    label(.EDGE8MASKXN)                                         \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.EDGE8MASKDLOOPKITER)                                 \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.EDGE8MASKPREFETCHLOOP)                                 \
    label(.EDGE8MASKLOOP1)                                      \
        UNROLL_K_LOOP_8MASKx7_L1                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP1)                                        \
																\
    label(.EDGE8MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.EDGE8MASKTAILITER)                                     \
																\
    label(.EDGE8MASKLOOP2)                                      \
																\
        UNROLL_K_LOOP_8MASKx7_L2                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP2)                                        \
    label(.EDGE8MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.EDGE8MASKTAIL)                                         \
																\
    label(.EDGE8MASKLOOP3)                                      \
																\
        UNROLL_K_LOOP_8MASKx7_L1                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP3)                                        \
																\
    vaddpd(zmm7, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm8, zmm8)                                    \
    vaddpd(zmm11, zmm10, zmm10)                                 \
    vaddpd(zmm13, zmm12, zmm12)                                 \
    vaddpd(zmm15, zmm14, zmm14)                                 \
    vaddpd(zmm17, zmm16, zmm16)                                 \
    vaddpd(zmm19, zmm18, zmm18)                                 \
																\
    label(.EDGE8MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.EDGE8MASKDPOSTACCUM)                                    \
																\
    label(.EDGE8MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 MASK_KZ(2) )                     \
        add( r10,rax )                                          \
        K_LOOP_8MASKx7_SET1(0)                                  \
        dec(rsi)                                                \
    jne(.EDGE8MASKDLOOPKLEFT)                                   \
																\
    label(.EDGE8MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.EDGE8MASKDBETAZERO)                                     \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.EDGE8MASKDROWSTORED)                                    \
																\
    label(.EDGE8MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_8MASKx7_Bn                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    STORE_ROWSTORED_C_8MASKxN_Bn                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.EDGE8MASKDROWSTORBZ)                                    \
																\
    label(.EDGE8MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_8MASKx7_B0                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(7), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm18,zmm18 )                                 \
    STORE_ROWSTORED_C_8MASKxN_B0                                \
    jmp(.CONCLUDE)

#define M_LEFT_24x6												\
																\
    /* m_left 20 edge kernel begins */                          \
    label(.EDGE20XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB20PREFETCHLOOP)                                     \
    label(.SUB20LOOP1)                                          \
																\
        UNROLL_K_LOOP_20x6_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP1)                                            \
																\
    label(.SUB20PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB20TAILITER)                                         \
																\
    label(.SUB20LOOP2)                                          \
        UNROLL_K_LOOP_20x6_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP2)                                            \
    label(.SUB20TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB20TAIL)                                             \
    label(.SUB20LOOP3)                                          \
        UNROLL_K_LOOP_20x6_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP3)                                            \
																\
    label(.SUB20TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB20DPOSTACCUM)                                        \
    label(.SUB20DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax), ymm2 )                              \
        add( r10,rax )                                          \
        K_LOOP_20x6(0, 1, 2)                                    \
        dec(rsi)                                                \
    jne(.SUB20DLOOPKLEFT)                                       \
																\
    label(.SUB20DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB20DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB20DROWSTORED)                                        \
																\
    label(.SUB20DCOLSTORED)                                     \
    STORE_COLSTORED_C_20x6_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    STORE_ROWSTORED_C_20xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB20DROWSTORBZ)                                        \
																\
    label(.SUB20DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_20x6_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    STORE_ROWSTORED_C_20xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 18 edge kernel begins */                          \
    label(.EDGE18XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB18PREFETCHLOOP)                                     \
    label(.SUB18LOOP1)                                          \
																\
        UNROLL_K_LOOP_18x6_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP1)                                            \
																\
    label(.SUB18PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB18TAILITER)                                         \
																\
    label(.SUB18LOOP2)                                          \
        UNROLL_K_LOOP_18x6_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP2)                                            \
    label(.SUB18TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB18TAIL)                                             \
    label(.SUB18LOOP3)                                          \
        UNROLL_K_LOOP_18x6_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP3)                                            \
																\
    label(.SUB18TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB18DPOSTACCUM)                                        \
																\
    label(.SUB18DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax), xmm2 )                              \
        add( r10,rax )                                          \
        K_LOOP_18x6(0, 1, 2)                                    \
        dec(rsi)                                                \
    jne(.SUB18DLOOPKLEFT)                                       \
																\
    label(.SUB18DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB18DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB18DROWSTORED)                                        \
																\
    label(.SUB18DCOLSTORED)                                     \
    STORE_COLSTORED_C_18x6_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    STORE_ROWSTORED_C_18xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB18DROWSTORBZ)                                        \
																\
    label(.SUB18DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_18x6_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    STORE_ROWSTORED_C_18xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 17 edge kernel begins */                          \
    label(.EDGE17XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB17PREFETCHLOOP)                                     \
    label(.SUB17LOOP1)                                          \
																\
        UNROLL_K_LOOP_17x6_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP1)                                            \
																\
    label(.SUB17PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB17TAILITER)                                         \
																\
    label(.SUB17LOOP2)                                          \
        UNROLL_K_LOOP_17x6_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP2)                                            \
    label(.SUB17TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB17TAIL)                                             \
    label(.SUB17LOOP3)                                          \
        UNROLL_K_LOOP_17x6_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP3)                                            \
    label(.SUB17TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB17DPOSTACCUM)                                        \
    label(.SUB17DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovsd ( 0x80(rax),xmm2 )                               \
        add( r10,rax )                                          \
        K_LOOP_17x6(0, 1, 2)                                    \
        dec(rsi)                                                \
    jne(.SUB17DLOOPKLEFT)                                       \
																\
    label(.SUB17DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB17DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB17DROWSTORED)                                        \
																\
    label(.SUB17DCOLSTORED)                                     \
    STORE_COLSTORED_C_17x6_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    STORE_ROWSTORED_C_17xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB17DROWSTORBZ)                                        \
																\
    label(.SUB17DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_17x6_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    STORE_ROWSTORED_C_17xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 16 edge kernel begins */                          \
    label(.EDGE16XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB16DLOOPKITER)                                     \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB16PREFETCHLOOP)                                     \
    label(.SUB16LOOP1)                                          \
																\
        UNROLL_K_LOOP_16x6_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP1)                                            \
																\
    label(.SUB16PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB16TAILITER)                                         \
																\
    label(.SUB16LOOP2)                                          \
        UNROLL_K_LOOP_16x6_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP2)                                            \
    label(.SUB16TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB16TAIL)                                             \
																\
    label(.SUB16LOOP3)                                          \
																\
        UNROLL_K_LOOP_16x6_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP3)                                            \
																\
    vaddpd(zmm18, zmm6, zmm6)                                   \
    vaddpd(zmm19, zmm7, zmm7)                                   \
    vaddpd(zmm20, zmm8, zmm8)                                   \
    vaddpd(zmm21, zmm9, zmm9)                                   \
    vaddpd(zmm22, zmm10, zmm10)                                 \
    vaddpd(zmm23, zmm11, zmm11)                                 \
    vaddpd(zmm24, zmm12, zmm12)                                 \
    vaddpd(zmm25, zmm13, zmm13)                                 \
    vaddpd(zmm26, zmm14, zmm14)                                 \
    vaddpd(zmm27, zmm15, zmm15)                                 \
    vaddpd(zmm28, zmm16, zmm16)                                 \
    vaddpd(zmm29, zmm17, zmm17)                                 \
																\
    label(.SUB16TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB16DPOSTACCUM)                                        \
																\
    label(.SUB16DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        add( r10,rax )                                          \
																\
        K_LOOP_16x6_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB16DLOOPKLEFT)                                       \
																\
    label(.SUB16DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB16DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB16DROWSTORED)                                        \
																\
    label(.SUB16DCOLSTORED)                                     \
    STORE_COLSTORED_C_16x6_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    STORE_ROWSTORED_C_16xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16DROWSTORBZ)                                        \
																\
    label(.SUB16DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_16x6_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    STORE_ROWSTORED_C_16xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /**********  M_Left 24 masked kernel **************/        \
    label(.EDGE24MASKXN)                                        \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    sub(imm(16), rsi)                                           \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB24MASKPREFETCHLOOP)                                 \
    label(.SUB24MASKLOOP1)                                      \
																\
        UNROLL_K_LOOP_24MASKx6_L1                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP1)                                        \
																\
    label(.SUB24MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.SUB24MASKTAILITER)                                     \
																\
    label(.SUB24MASKLOOP2)                                      \
        UNROLL_K_LOOP_24MASKx6_L2                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP2)                                        \
    label(.SUB24MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB24MASKTAIL)                                         \
    label(.SUB24MASKLOOP3)                                      \
        UNROLL_K_LOOP_24MASKx6_L1                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP3)                                        \
    label(.SUB24MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB24MASKDPOSTACCUM)                                    \
    label(.SUB24MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax),zmm2 MASK_KZ(2) )                    \
        add( r10,rax )                                          \
        K_LOOP_24MASKx6(0, 1, 2)                                \
        dec(rsi)                                                \
    jne(.SUB24MASKDLOOPKLEFT)                                   \
    label(.SUB24MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.SUB24MASKDBETAZERO)                                     \
    cmp(imm(8), rdi)                                            \
    jz(.SUB24MASKDROWSTORED)                                    \
																\
    label(.SUB24MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_24MASKx6_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    STORE_ROWSTORED_C_24MASKxN_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.SUB24MASKDROWSTORBZ)                                    \
																\
    label(.SUB24MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_24MASKx6_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    STORE_ROWSTORED_C_24MASKxN_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 12 edge kernel begins */                          \
    label(.EDGE12XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB12DLOOPKITER)                                     \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB12PREFETCHLOOP)                                     \
																\
    label(.SUB12LOOP1)                                          \
																\
        UNROLL_K_LOOP_12x6_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB12LOOP1)                                            \
																\
    label(.SUB12PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB12TAILITER)                                         \
																\
    label(.SUB12LOOP2)                                          \
        UNROLL_K_LOOP_12x6_L2                                   \
        sub(imm(1), rsi)                                        \
    jnz(.SUB12LOOP2)                                            \
    label(.SUB12TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB12TAIL)                                             \
																\
    label(.SUB12LOOP3)                                          \
        UNROLL_K_LOOP_12x6_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB12LOOP3)                                            \
																\
    vaddpd(zmm18, zmm6, zmm6)                                   \
    vaddpd(zmm19, zmm7, zmm7)                                   \
    vaddpd(zmm20, zmm8, zmm8)                                   \
    vaddpd(zmm21, zmm9, zmm9)                                   \
    vaddpd(zmm22, zmm10, zmm10)                                 \
    vaddpd(zmm23, zmm11, zmm11)                                 \
    vaddpd(zmm24, zmm12, zmm12)                                 \
    vaddpd(zmm25, zmm13, zmm13)                                 \
    vaddpd(zmm26, zmm14, zmm14)                                 \
    vaddpd(zmm27, zmm15, zmm15)                                 \
    vaddpd(zmm28, zmm16, zmm16)                                 \
    vaddpd(zmm29, zmm17, zmm17)                                 \
																\
    label(.SUB12TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB12DPOSTACCUM)                                        \
																\
    label(.SUB12DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),ymm1 )                               \
        add( r10,rax )                                          \
        K_LOOP_12x6_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB12DLOOPKLEFT)                                       \
																\
    label(.SUB12DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB12DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
    jz(.SUB12DROWSTORED)                                        \
																\
    label(.SUB12DCOLSTORED)                                     \
    STORE_COLSTORED_C_12x6_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    STORE_ROWSTORED_C_12xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB12DROWSTORBZ)                                        \
																\
    label(.SUB12DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_12x6_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    STORE_ROWSTORED_C_12xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 11 edge kernel begins */                          \
    label(.EDGE11XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB11PREFETCHLOOP)                                     \
    label(.SUB11LOOP1)                                          \
																\
        UNROLL_K_LOOP_11x6_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP1)                                            \
																\
    label(.SUB11PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB11TAILITER)                                         \
																\
    label(.SUB11LOOP2)                                          \
        UNROLL_K_LOOP_11x6_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP2)                                            \
    label(.SUB11TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB11TAIL)                                             \
    label(.SUB11LOOP3)                                          \
        UNROLL_K_LOOP_11x6_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP3)                                            \
																\
    label(.SUB11TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB11DPOSTACCUM)                                        \
																\
    label(.SUB11DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),xmm1 )                               \
        vmovsd( 0x50(rax),xmm2 )                                \
        add( r10,rax )                                          \
        K_LOOP_11x6(0, 1, 2)                                    \
        dec(rsi)                                                \
    jne(.SUB11DLOOPKLEFT)                                       \
																\
    label(.SUB11DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB11DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB11DROWSTORED)                                        \
																\
    label(.SUB11DCOLSTORED)                                     \
    STORE_COLSTORED_C_11x6_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    STORE_ROWSTORED_C_11xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB11DROWSTORBZ)                                        \
    label(.SUB11DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_11x6_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    vmulpd( zmm30,zmm25,zmm25 )                                 \
    STORE_ROWSTORED_C_11xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 10 edge kernel begins */                          \
    label(.EDGE10XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB10PREFETCHLOOP)                                     \
    label(.SUB10LOOP1)                                          \
																\
        UNROLL_K_LOOP_10x6_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP1)                                            \
																\
    label(.SUB10PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB10TAILITER)                                         \
																\
    label(.SUB10LOOP2)                                          \
        UNROLL_K_LOOP_10x6_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP2)                                            \
    label(.SUB10TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB10TAIL)                                             \
    label(.SUB10LOOP3)                                          \
        UNROLL_K_LOOP_10x6_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP3)                                            \
																\
    vaddpd(zmm18, zmm6, zmm6)                                   \
    vaddpd(zmm19, zmm7, zmm7)                                   \
    vaddpd(zmm20, zmm8, zmm8)                                   \
    vaddpd(zmm21, zmm9, zmm9)                                   \
    vaddpd(zmm22, zmm10, zmm10)                                 \
    vaddpd(zmm23, zmm11, zmm11)                                 \
    vaddpd(zmm24, zmm12, zmm12)                                 \
    vaddpd(zmm25, zmm13, zmm13)                                 \
    vaddpd(zmm26, zmm14, zmm14)                                 \
    vaddpd(zmm27, zmm15, zmm15)                                 \
    vaddpd(zmm28, zmm16, zmm16)                                 \
    vaddpd(zmm29, zmm17, zmm17)                                 \
																\
    label(.SUB10TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB10DPOSTACCUM)                                        \
																\
    label(.SUB10DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),xmm1 )                               \
        add( r10,rax )                                          \
        K_LOOP_10x6_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB10DLOOPKLEFT)                                       \
																\
    label(.SUB10DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB10DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB10DROWSTORED)                                        \
																\
    label(.SUB10DCOLSTORED)                                     \
    STORE_COLSTORED_C_10x6_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    STORE_ROWSTORED_C_10xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB10DROWSTORBZ)                                        \
    label(.SUB10DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_10x6_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    STORE_ROWSTORED_C_10xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 9 edge kernel begins */                           \
    label(.EDGE9XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB9DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB9PREFETCHLOOP)                                      \
																\
    label(.SUB9LOOP1)                                           \
																\
        UNROLL_K_LOOP_9x6_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB9LOOP1)                                             \
																\
    label(.SUB9PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB9TAILITER)                                          \
																\
    label(.SUB9LOOP2)                                           \
        UNROLL_K_LOOP_9x6_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB9LOOP2)                                             \
    label(.SUB9TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB9TAIL)                                              \
																\
    label(.SUB9LOOP3)                                           \
																\
        UNROLL_K_LOOP_9x6_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB9LOOP3)                                             \
																\
    vaddpd(zmm18, zmm6, zmm6)                                   \
    vaddpd(zmm19, zmm7, zmm7)                                   \
    vaddpd(zmm20, zmm8, zmm8)                                   \
    vaddpd(zmm21, zmm9, zmm9)                                   \
    vaddpd(zmm22, zmm10, zmm10)                                 \
    vaddpd(zmm23, zmm11, zmm11)                                 \
    vaddpd(zmm24, zmm12, zmm12)                                 \
    vaddpd(zmm25, zmm13, zmm13)                                 \
    vaddpd(zmm26, zmm14, zmm14)                                 \
    vaddpd(zmm27, zmm15, zmm15)                                 \
    vaddpd(zmm28, zmm16, zmm16)                                 \
    vaddpd(zmm29, zmm17, zmm17)                                 \
																\
    label(.SUB9TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB9DPOSTACCUM)                                         \
																\
    label(.SUB9DLOOPKLEFT)                                      \
        vmovupd( mem(rax),zmm0 )                                \
        vmovsd( 0x40(rax),xmm1 )                                \
        add( r10,rax )                                          \
        K_LOOP_9x6_SET1(0, 1)                                   \
        dec(rsi)                                                \
    jne(.SUB9DLOOPKLEFT)                                        \
																\
    label(.SUB9DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB9DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB9DROWSTORED)                                         \
																\
    label(.SUB9DCOLSTORED)                                      \
    STORE_COLSTORED_C_9x6_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    STORE_ROWSTORED_C_9xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB9DROWSTORBZ)                                         \
    label(.SUB9DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_9x6_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    STORE_ROWSTORED_C_9xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 8 edge kernel begins */                           \
    label(.EDGE8XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB8DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB8PREFETCHLOOP)                                      \
    label(.SUB8LOOP1)                                           \
        UNROLL_K_LOOP_8x6_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB8LOOP1)                                             \
																\
    label(.SUB8PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB8TAILITER)                                          \
																\
    label(.SUB8LOOP2)                                           \
        UNROLL_K_LOOP_8x6_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB8LOOP2)                                             \
    label(.SUB8TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB8TAIL)                                              \
																\
    label(.SUB8LOOP3)                                           \
        UNROLL_K_LOOP_8x6_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB8LOOP3)                                             \
																\
    vaddpd(zmm7, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm8, zmm8)                                    \
    vaddpd(zmm11, zmm10, zmm10)                                 \
    vaddpd(zmm13, zmm12, zmm12)                                 \
    vaddpd(zmm15, zmm14, zmm14)                                 \
    vaddpd(zmm17, zmm16, zmm16)                                 \
																\
    label(.SUB8TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB8DPOSTACCUM)                                         \
																\
    label(.SUB8DLOOPKLEFT)                                      \
        vmovupd( mem(rax),zmm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_8x6_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB8DLOOPKLEFT)                                        \
																\
    label(.SUB8DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB8DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB8DROWSTORED)                                         \
																\
    label(.SUB8DCOLSTORED)                                      \
    STORE_COLSTORED_C_8x6_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    STORE_ROWSTORED_C_8xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB8DROWSTORBZ)                                         \
    label(.SUB8DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_8x6_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    STORE_ROWSTORED_C_8xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /**********  M_Left 16 mask kernel **************/          \
    label(.EDGE16MASKXN)                                        \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    sub(imm(8), rsi)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
    lea(mem(r11, r9, 4), r15)                                   \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB16MASKDLOOPKITER)                                 \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB16MASKPREFETCHLOOP)                                 \
																\
    label(.SUB16MASKLOOP1)                                      \
																\
        UNROLL_K_LOOP_16MASKx6_L1                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP1)                                        \
																\
    label(.SUB16MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.SUB16MASKTAILITER)                                     \
																\
    label(.SUB16MASKLOOP2)                                      \
																\
        UNROLL_K_LOOP_16MASKx6_L2                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP2)                                        \
    label(.SUB16MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB16MASKTAIL)                                         \
																\
    label(.SUB16MASKLOOP3)                                      \
																\
        UNROLL_K_LOOP_16MASKx6_L1                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP3)                                        \
																\
    vaddpd(zmm18, zmm6, zmm6)                                   \
    vaddpd(zmm19, zmm7, zmm7)                                   \
    vaddpd(zmm20, zmm8, zmm8)                                   \
    vaddpd(zmm21, zmm9, zmm9)                                   \
    vaddpd(zmm22, zmm10, zmm10)                                 \
    vaddpd(zmm23, zmm11, zmm11)                                 \
    vaddpd(zmm24, zmm12, zmm12)                                 \
    vaddpd(zmm25, zmm13, zmm13)                                 \
    vaddpd(zmm26, zmm14, zmm14)                                 \
    vaddpd(zmm27, zmm15, zmm15)                                 \
    vaddpd(zmm28, zmm16, zmm16)                                 \
    vaddpd(zmm29, zmm17, zmm17)                                 \
																\
    label(.SUB16MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB16MASKDPOSTACCUM)                                    \
																\
    label(.SUB16MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )                    \
        add( r10,rax )                                          \
        K_LOOP_16MASKx6_SET1(0, 1)                              \
        dec(rsi)                                                \
    jne(.SUB16MASKDLOOPKLEFT)                                   \
																\
    label(.SUB16MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.SUB16MASKDBETAZERO)                                     \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16MASKDROWSTORED)                                    \
																\
    label(.SUB16MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_16MASKx6_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    STORE_ROWSTORED_C_16MASKxN_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16MASKDROWSTORBZ)                                    \
																\
    label(.SUB16MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_16MASKx6_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    vmulpd( zmm30,zmm17,zmm17 )                                 \
    STORE_ROWSTORED_C_16MASKxN_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 4 edge kernel begins */                           \
    label(.EDGE4XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB4DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB4PREFETCHLOOP)                                      \
    label(.SUB4LOOP1)                                           \
        UNROLL_K_LOOP_4x6_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB4LOOP1)                                             \
																\
    label(.SUB4PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB4TAILITER)                                          \
																\
    label(.SUB4LOOP2)                                           \
																\
        UNROLL_K_LOOP_4x6_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB4LOOP2)                                             \
    label(.SUB4TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB4TAIL)                                              \
																\
    label(.SUB4LOOP3)                                           \
																\
        UNROLL_K_LOOP_4x6_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB4LOOP3)                                             \
																\
    vaddpd(ymm7, ymm6, ymm6)                                    \
    vaddpd(ymm9, ymm8, ymm8)                                    \
    vaddpd(ymm11, ymm10, ymm10)                                 \
    vaddpd(ymm13, ymm12, ymm12)                                 \
    vaddpd(ymm15, ymm14, ymm14)                                 \
    vaddpd(ymm17, ymm16, ymm16)                                 \
																\
    label(.SUB4TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB4DPOSTACCUM)                                         \
																\
    label(.SUB4DLOOPKLEFT)                                      \
        vmovupd( mem(rax),ymm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_4x6_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB4DLOOPKLEFT)                                        \
																\
    label(.SUB4DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB4DBETAZERO)                                          \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB4DROWSTORED)                                         \
																\
    label(.SUB4DCOLSTORED)                                      \
    STORE_COLSTORED_C_4x6_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    STORE_ROWSTORED_C_4xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB4DROWSTORBZ)                                         \
    label(.SUB4DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_4x6_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    STORE_ROWSTORED_C_4xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 2 edge kernel begins */                           \
    label(.EDGE2XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB2DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB2PREFETCHLOOP)                                      \
    label(.SUB2LOOP1)                                           \
        UNROLL_K_LOOP_2x6_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB2LOOP1)                                             \
																\
    label(.SUB2PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB2TAILITER)                                          \
																\
    label(.SUB2LOOP2)                                           \
																\
        UNROLL_K_LOOP_2x6_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB2LOOP2)                                             \
    label(.SUB2TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB2TAIL)                                              \
																\
    label(.SUB2LOOP3)                                           \
        UNROLL_K_LOOP_2x6_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB2LOOP3)                                             \
																\
    vaddpd(xmm7, xmm6, xmm6)                                    \
    vaddpd(xmm9, xmm8, xmm8)                                    \
    vaddpd(xmm11, xmm10, xmm10)                                 \
    vaddpd(xmm13, xmm12, xmm12)                                 \
    vaddpd(xmm15, xmm14, xmm14)                                 \
    vaddpd(xmm17, xmm16, xmm16)                                 \
																\
    label(.SUB2TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB2DPOSTACCUM)                                         \
																\
    label(.SUB2DLOOPKLEFT)                                      \
        vmovupd( mem(rax),xmm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_2x6_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB2DLOOPKLEFT)                                        \
																\
    label(.SUB2DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB2DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB2DROWSTORED)                                         \
																\
    label(.SUB2DCOLSTORED)                                      \
    STORE_COLSTORED_C_2x6_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    STORE_ROWSTORED_C_2xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB2DROWSTORBZ)                                         \
    label(.SUB2DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_2x6_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    STORE_ROWSTORED_C_2xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 1 edge kernel begins */                           \
    label(.EDGE1XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB1TAIL)                                               \
    label(.SUB1LOOP3)                                           \
        UNROLL_K_LOOP_1x6_L1                                    \
        dec(rsi)                                                \
        jnz(.SUB1LOOP3)                                         \
																\
    vaddpd(xmm7, xmm6, xmm6)                                    \
    vaddpd(xmm9, xmm8, xmm8)                                    \
    vaddpd(xmm11, xmm10, xmm10)                                 \
    vaddpd(xmm13, xmm12, xmm12)                                 \
    vaddpd(xmm15, xmm14, xmm14)                                 \
    vaddpd(xmm17, xmm16, xmm16)                                 \
																\
    label(.SUB1TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB1DPOSTACCUM)                                         \
																\
    label(.SUB1DLOOPKLEFT)                                      \
        vmovsd( mem(rax),xmm0 )                                 \
        add( r10,rax )                                          \
        K_LOOP_1x6_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB1DLOOPKLEFT)                                        \
																\
    label(.SUB1DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB1DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB1DROWSTORED)                                         \
																\
    label(.SUB1DCOLSTORED)                                      \
    STORE_COLSTORED_C_1x6_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    STORE_ROWSTORED_C_1xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB1DROWSTORBZ)                                         \
    label(.SUB1DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_1x6_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    STORE_ROWSTORED_C_1xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 8 mask edge kernel begins */                      \
    label(.EDGE8MASKXN)                                         \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.EDGE8MASKDLOOPKITER)                                 \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.EDGE8MASKPREFETCHLOOP)                                 \
    label(.EDGE8MASKLOOP1)                                      \
        UNROLL_K_LOOP_8MASKx6_L1                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP1)                                        \
																\
    label(.EDGE8MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.EDGE8MASKTAILITER)                                     \
																\
    label(.EDGE8MASKLOOP2)                                      \
																\
        UNROLL_K_LOOP_8MASKx6_L2                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP2)                                        \
    label(.EDGE8MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.EDGE8MASKTAIL)                                         \
																\
    label(.EDGE8MASKLOOP3)                                      \
																\
        UNROLL_K_LOOP_8MASKx6_L1                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP3)                                        \
																\
    vaddpd(zmm7, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm8, zmm8)                                    \
    vaddpd(zmm11, zmm10, zmm10)                                 \
    vaddpd(zmm13, zmm12, zmm12)                                 \
    vaddpd(zmm15, zmm14, zmm14)                                 \
    vaddpd(zmm17, zmm16, zmm16)                                 \
    vaddpd(zmm19, zmm18, zmm18)                                 \
																\
    label(.EDGE8MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.EDGE8MASKDPOSTACCUM)                                    \
																\
    label(.EDGE8MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 MASK_KZ(2) )                     \
        add( r10,rax )                                          \
        K_LOOP_8MASKx6_SET1(0)                                  \
        dec(rsi)                                                \
    jne(.EDGE8MASKDLOOPKLEFT)                                   \
																\
    label(.EDGE8MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.EDGE8MASKDBETAZERO)                                     \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.EDGE8MASKDROWSTORED)                                    \
																\
    label(.EDGE8MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_8MASKx6_Bn                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    STORE_ROWSTORED_C_8MASKxN_Bn                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.EDGE8MASKDROWSTORBZ)                                    \
																\
    label(.EDGE8MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_8MASKx6_B0                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(6), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm16,zmm16 )                                 \
    STORE_ROWSTORED_C_8MASKxN_B0                                \
    jmp(.CONCLUDE)


#define M_LEFT_24x5												\
																\
    /* m_left 20 edge kernel begins */                          \
    label(.EDGE20XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB20PREFETCHLOOP)                                     \
    label(.SUB20LOOP1)                                          \
																\
        UNROLL_K_LOOP_20x5_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP1)                                            \
																\
    label(.SUB20PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB20TAILITER)                                         \
																\
    label(.SUB20LOOP2)                                          \
        UNROLL_K_LOOP_20x5_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP2)                                            \
    label(.SUB20TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB20TAIL)                                             \
    label(.SUB20LOOP3)                                          \
        UNROLL_K_LOOP_20x5_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP3)                                            \
																\
    label(.SUB20TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB20DPOSTACCUM)                                        \
    label(.SUB20DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax), ymm2 )                              \
        add( r10,rax )                                          \
        K_LOOP_20x5(0, 1, 2)                                    \
        dec(rsi)                                                \
    jne(.SUB20DLOOPKLEFT)                                       \
																\
    label(.SUB20DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB20DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB20DROWSTORED)                                        \
																\
    label(.SUB20DCOLSTORED)                                     \
    STORE_COLSTORED_C_20x5_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    STORE_ROWSTORED_C_20xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB20DROWSTORBZ)                                        \
																\
    label(.SUB20DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_20x5_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    STORE_ROWSTORED_C_20xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 18 edge kernel begins */                          \
    label(.EDGE18XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB18PREFETCHLOOP)                                     \
    label(.SUB18LOOP1)                                          \
																\
        UNROLL_K_LOOP_18x5_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP1)                                            \
																\
    label(.SUB18PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB18TAILITER)                                         \
																\
    label(.SUB18LOOP2)                                          \
        UNROLL_K_LOOP_18x5_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP2)                                            \
    label(.SUB18TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB18TAIL)                                             \
    label(.SUB18LOOP3)                                          \
        UNROLL_K_LOOP_18x5_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP3)                                            \
																\
    label(.SUB18TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB18DPOSTACCUM)                                        \
																\
    label(.SUB18DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax), xmm2 )                              \
        add( r10,rax )                                          \
        K_LOOP_18x5(0, 1, 2)                                    \
        dec(rsi)                                                \
    jne(.SUB18DLOOPKLEFT)                                       \
																\
    label(.SUB18DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB18DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB18DROWSTORED)                                        \
																\
    label(.SUB18DCOLSTORED)                                     \
    STORE_COLSTORED_C_18x5_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    STORE_ROWSTORED_C_18xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB18DROWSTORBZ)                                        \
																\
    label(.SUB18DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_18x5_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    STORE_ROWSTORED_C_18xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 17 edge kernel begins */                          \
    label(.EDGE17XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB17PREFETCHLOOP)                                     \
    label(.SUB17LOOP1)                                          \
																\
        UNROLL_K_LOOP_17x5_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP1)                                            \
																\
    label(.SUB17PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB17TAILITER)                                         \
																\
    label(.SUB17LOOP2)                                          \
        UNROLL_K_LOOP_17x5_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP2)                                            \
    label(.SUB17TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB17TAIL)                                             \
    label(.SUB17LOOP3)                                          \
        UNROLL_K_LOOP_17x5_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP3)                                            \
    label(.SUB17TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB17DPOSTACCUM)                                        \
    label(.SUB17DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovsd ( 0x80(rax),xmm2 )                               \
        add( r10,rax )                                          \
        K_LOOP_17x5(0, 1, 2)                                    \
        dec(rsi)                                                \
    jne(.SUB17DLOOPKLEFT)                                       \
																\
    label(.SUB17DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB17DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB17DROWSTORED)                                        \
																\
    label(.SUB17DCOLSTORED)                                     \
    STORE_COLSTORED_C_17x5_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    STORE_ROWSTORED_C_17xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB17DROWSTORBZ)                                        \
																\
    label(.SUB17DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_17x5_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    STORE_ROWSTORED_C_17xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 16 edge kernel begins */                          \
    label(.EDGE16XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB16DLOOPKITER)                                     \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB16PREFETCHLOOP)                                     \
    label(.SUB16LOOP1)                                          \
																\
        UNROLL_K_LOOP_16x5_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP1)                                            \
																\
    label(.SUB16PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB16TAILITER)                                         \
																\
    label(.SUB16LOOP2)                                          \
        UNROLL_K_LOOP_16x5_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP2)                                            \
    label(.SUB16TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB16TAIL)                                             \
																\
    label(.SUB16LOOP3)                                          \
																\
        UNROLL_K_LOOP_16x5_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP3)                                            \
																\
    vaddpd(zmm16, zmm6, zmm6)                                   \
    vaddpd(zmm17, zmm7, zmm7)                                   \
    vaddpd(zmm18, zmm8, zmm8)                                   \
    vaddpd(zmm19, zmm9, zmm9)                                   \
    vaddpd(zmm20, zmm10, zmm10)                                 \
    vaddpd(zmm21, zmm11, zmm11)                                 \
    vaddpd(zmm22, zmm12, zmm12)                                 \
    vaddpd(zmm23, zmm13, zmm13)                                 \
    vaddpd(zmm24, zmm14, zmm14)                                 \
    vaddpd(zmm25, zmm15, zmm15)                                 \
																\
    label(.SUB16TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB16DPOSTACCUM)                                        \
																\
    label(.SUB16DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        add( r10,rax )                                          \
																\
        K_LOOP_16x5_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB16DLOOPKLEFT)                                       \
																\
    label(.SUB16DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB16DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB16DROWSTORED)                                        \
																\
    label(.SUB16DCOLSTORED)                                     \
    STORE_COLSTORED_C_16x5_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    STORE_ROWSTORED_C_16xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16DROWSTORBZ)                                        \
																\
    label(.SUB16DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_16x5_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    STORE_ROWSTORED_C_16xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /**********  M_Left 24 masked kernel **************/        \
    label(.EDGE24MASKXN)                                        \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    sub(imm(16), rsi)                                           \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB24MASKPREFETCHLOOP)                                 \
    label(.SUB24MASKLOOP1)                                      \
																\
        UNROLL_K_LOOP_24MASKx5_L1                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP1)                                        \
																\
    label(.SUB24MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.SUB24MASKTAILITER)                                     \
																\
    label(.SUB24MASKLOOP2)                                      \
        UNROLL_K_LOOP_24MASKx5_L2                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP2)                                        \
    label(.SUB24MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB24MASKTAIL)                                         \
    label(.SUB24MASKLOOP3)                                      \
        UNROLL_K_LOOP_24MASKx5_L1                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP3)                                        \
    label(.SUB24MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB24MASKDPOSTACCUM)                                    \
    label(.SUB24MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax),zmm2 MASK_KZ(2) )                    \
        add( r10,rax )                                          \
        K_LOOP_24MASKx5(0, 1, 2)                                \
        dec(rsi)                                                \
    jne(.SUB24MASKDLOOPKLEFT)                                   \
    label(.SUB24MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.SUB24MASKDBETAZERO)                                     \
    cmp(imm(8), rdi)                                            \
    jz(.SUB24MASKDROWSTORED)                                    \
																\
    label(.SUB24MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_24MASKx5_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    STORE_ROWSTORED_C_24MASKxN_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.SUB24MASKDROWSTORBZ)                                    \
																\
    label(.SUB24MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_24MASKx5_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    STORE_ROWSTORED_C_24MASKxN_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 12 edge kernel begins */                          \
    label(.EDGE12XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB12DLOOPKITER)                                     \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB12PREFETCHLOOP)                                     \
																\
    label(.SUB12LOOP1)                                          \
																\
        UNROLL_K_LOOP_12x5_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB12LOOP1)                                            \
																\
    label(.SUB12PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB12TAILITER)                                         \
																\
    label(.SUB12LOOP2)                                          \
        UNROLL_K_LOOP_12x5_L2                                   \
        sub(imm(1), rsi)                                        \
    jnz(.SUB12LOOP2)                                            \
    label(.SUB12TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB12TAIL)                                             \
																\
    label(.SUB12LOOP3)                                          \
        UNROLL_K_LOOP_12x5_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB12LOOP3)                                            \
																\
    vaddpd(zmm16, zmm6, zmm6)                                   \
    vaddpd(zmm17, zmm7, zmm7)                                   \
    vaddpd(zmm18, zmm8, zmm8)                                   \
    vaddpd(zmm19, zmm9, zmm9)                                   \
    vaddpd(zmm20, zmm10, zmm10)                                 \
    vaddpd(zmm21, zmm11, zmm11)                                 \
    vaddpd(zmm22, zmm12, zmm12)                                 \
    vaddpd(zmm23, zmm13, zmm13)                                 \
    vaddpd(zmm24, zmm14, zmm14)                                 \
    vaddpd(zmm25, zmm15, zmm15)                                 \
																\
    label(.SUB12TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB12DPOSTACCUM)                                        \
																\
    label(.SUB12DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),ymm1 )                               \
        add( r10,rax )                                          \
        K_LOOP_12x5_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB12DLOOPKLEFT)                                       \
																\
    label(.SUB12DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB12DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
    jz(.SUB12DROWSTORED)                                        \
																\
    label(.SUB12DCOLSTORED)                                     \
    STORE_COLSTORED_C_12x5_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    STORE_ROWSTORED_C_12xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB12DROWSTORBZ)                                        \
																\
    label(.SUB12DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_12x5_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    STORE_ROWSTORED_C_12xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 11 edge kernel begins */                          \
    label(.EDGE11XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB11PREFETCHLOOP)                                     \
    label(.SUB11LOOP1)                                          \
																\
        UNROLL_K_LOOP_11x5_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP1)                                            \
																\
    label(.SUB11PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB11TAILITER)                                         \
																\
    label(.SUB11LOOP2)                                          \
        UNROLL_K_LOOP_11x5_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP2)                                            \
    label(.SUB11TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB11TAIL)                                             \
    label(.SUB11LOOP3)                                          \
        UNROLL_K_LOOP_11x5_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP3)                                            \
																\
    label(.SUB11TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB11DPOSTACCUM)                                        \
																\
    label(.SUB11DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),xmm1 )                               \
        vmovsd( 0x50(rax),xmm2 )                                \
        add( r10,rax )                                          \
        K_LOOP_11x5(0, 1, 2)                                    \
        dec(rsi)                                                \
    jne(.SUB11DLOOPKLEFT)                                       \
																\
    label(.SUB11DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB11DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB11DROWSTORED)                                        \
																\
    label(.SUB11DCOLSTORED)                                     \
    STORE_COLSTORED_C_11x5_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    STORE_ROWSTORED_C_11xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB11DROWSTORBZ)                                        \
    label(.SUB11DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_11x5_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    vmulpd( zmm30,zmm24,zmm24 )                                 \
    STORE_ROWSTORED_C_11xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 10 edge kernel begins */                          \
    label(.EDGE10XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB10PREFETCHLOOP)                                     \
    label(.SUB10LOOP1)                                          \
																\
        UNROLL_K_LOOP_10x5_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP1)                                            \
																\
    label(.SUB10PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB10TAILITER)                                         \
																\
    label(.SUB10LOOP2)                                          \
        UNROLL_K_LOOP_10x5_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP2)                                            \
    label(.SUB10TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB10TAIL)                                             \
    label(.SUB10LOOP3)                                          \
        UNROLL_K_LOOP_10x5_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP3)                                            \
																\
    vaddpd(zmm16, zmm6, zmm6)                                   \
    vaddpd(zmm17, zmm7, zmm7)                                   \
    vaddpd(zmm18, zmm8, zmm8)                                   \
    vaddpd(zmm19, zmm9, zmm9)                                   \
    vaddpd(zmm20, zmm10, zmm10)                                 \
    vaddpd(zmm21, zmm11, zmm11)                                 \
    vaddpd(zmm22, zmm12, zmm12)                                 \
    vaddpd(zmm23, zmm13, zmm13)                                 \
    vaddpd(zmm24, zmm14, zmm14)                                 \
    vaddpd(zmm25, zmm15, zmm15)                                 \
																\
    label(.SUB10TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB10DPOSTACCUM)                                        \
																\
    label(.SUB10DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),xmm1 )                               \
        add( r10,rax )                                          \
        K_LOOP_10x5_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB10DLOOPKLEFT)                                       \
																\
    label(.SUB10DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB10DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB10DROWSTORED)                                        \
																\
    label(.SUB10DCOLSTORED)                                     \
    STORE_COLSTORED_C_10x5_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    STORE_ROWSTORED_C_10xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB10DROWSTORBZ)                                        \
    label(.SUB10DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_10x5_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    STORE_ROWSTORED_C_10xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 9 edge kernel begins */                           \
    label(.EDGE9XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB9DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB9PREFETCHLOOP)                                      \
																\
    label(.SUB9LOOP1)                                           \
																\
        UNROLL_K_LOOP_9x5_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB9LOOP1)                                             \
																\
    label(.SUB9PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB9TAILITER)                                          \
																\
    label(.SUB9LOOP2)                                           \
        UNROLL_K_LOOP_9x5_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB9LOOP2)                                             \
    label(.SUB9TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB9TAIL)                                              \
																\
    label(.SUB9LOOP3)                                           \
																\
        UNROLL_K_LOOP_9x5_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB9LOOP3)                                             \
																\
    vaddpd(zmm16, zmm6, zmm6)                                   \
    vaddpd(zmm17, zmm7, zmm7)                                   \
    vaddpd(zmm18, zmm8, zmm8)                                   \
    vaddpd(zmm19, zmm9, zmm9)                                   \
    vaddpd(zmm20, zmm10, zmm10)                                 \
    vaddpd(zmm21, zmm11, zmm11)                                 \
    vaddpd(zmm22, zmm12, zmm12)                                 \
    vaddpd(zmm23, zmm13, zmm13)                                 \
    vaddpd(zmm24, zmm14, zmm14)                                 \
    vaddpd(zmm25, zmm15, zmm15)                                 \
																\
    label(.SUB9TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB9DPOSTACCUM)                                         \
																\
    label(.SUB9DLOOPKLEFT)                                      \
        vmovupd( mem(rax),zmm0 )                                \
        vmovsd( 0x40(rax),xmm1 )                                \
        add( r10,rax )                                          \
        K_LOOP_9x5_SET1(0, 1)                                   \
        dec(rsi)                                                \
    jne(.SUB9DLOOPKLEFT)                                        \
																\
    label(.SUB9DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB9DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB9DROWSTORED)                                         \
																\
    label(.SUB9DCOLSTORED)                                      \
    STORE_COLSTORED_C_9x5_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    STORE_ROWSTORED_C_9xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB9DROWSTORBZ)                                         \
    label(.SUB9DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_9x5_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    STORE_ROWSTORED_C_9xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 8 edge kernel begins */                           \
    label(.EDGE8XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB8DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB8PREFETCHLOOP)                                      \
    label(.SUB8LOOP1)                                           \
        UNROLL_K_LOOP_8x5_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB8LOOP1)                                             \
																\
    label(.SUB8PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB8TAILITER)                                          \
																\
    label(.SUB8LOOP2)                                           \
        UNROLL_K_LOOP_8x5_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB8LOOP2)                                             \
    label(.SUB8TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB8TAIL)                                              \
																\
    label(.SUB8LOOP3)                                           \
        UNROLL_K_LOOP_8x5_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB8LOOP3)                                             \
																\
    vaddpd(zmm7, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm8, zmm8)                                    \
    vaddpd(zmm11, zmm10, zmm10)                                 \
    vaddpd(zmm13, zmm12, zmm12)                                 \
    vaddpd(zmm15, zmm14, zmm14)                                 \
																\
    label(.SUB8TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB8DPOSTACCUM)                                         \
																\
    label(.SUB8DLOOPKLEFT)                                      \
        vmovupd( mem(rax),zmm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_8x5_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB8DLOOPKLEFT)                                        \
																\
    label(.SUB8DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB8DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB8DROWSTORED)                                         \
																\
    label(.SUB8DCOLSTORED)                                      \
    STORE_COLSTORED_C_8x5_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    STORE_ROWSTORED_C_8xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB8DROWSTORBZ)                                         \
    label(.SUB8DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_8x5_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    STORE_ROWSTORED_C_8xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 16 mask edge kernel begins */                     \
    label(.EDGE16MASKXN)                                        \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    sub(imm(8), rsi)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
    lea(mem(r11, r9, 4), r15)                                   \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB16MASKDLOOPKITER)                                 \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB16MASKPREFETCHLOOP)                                 \
																\
    label(.SUB16MASKLOOP1)                                      \
																\
        UNROLL_K_LOOP_16MASKx5_L1                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP1)                                        \
																\
    label(.SUB16MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.SUB16MASKTAILITER)                                     \
																\
    label(.SUB16MASKLOOP2)                                      \
																\
        UNROLL_K_LOOP_16MASKx5_L2                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP2)                                        \
    label(.SUB16MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB16MASKTAIL)                                         \
																\
    label(.SUB16MASKLOOP3)                                      \
																\
        UNROLL_K_LOOP_16MASKx5_L1                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP3)                                        \
																\
    vaddpd(zmm16, zmm6, zmm6)                                   \
    vaddpd(zmm17, zmm7, zmm7)                                   \
    vaddpd(zmm18, zmm8, zmm8)                                   \
    vaddpd(zmm19, zmm9, zmm9)                                   \
    vaddpd(zmm20, zmm10, zmm10)                                 \
    vaddpd(zmm21, zmm11, zmm11)                                 \
    vaddpd(zmm22, zmm12, zmm12)                                 \
    vaddpd(zmm23, zmm13, zmm13)                                 \
    vaddpd(zmm24, zmm14, zmm14)                                 \
    vaddpd(zmm25, zmm15, zmm15)                                 \
																\
    label(.SUB16MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB16MASKDPOSTACCUM)                                    \
																\
    label(.SUB16MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )                    \
        add( r10,rax )                                          \
        K_LOOP_16MASKx5_SET1(0, 1)                              \
        dec(rsi)                                                \
    jne(.SUB16MASKDLOOPKLEFT)                                   \
																\
    label(.SUB16MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.SUB16MASKDBETAZERO)                                     \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16MASKDROWSTORED)                                    \
																\
    label(.SUB16MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_16MASKx5_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    STORE_ROWSTORED_C_16MASKxN_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16MASKDROWSTORBZ)                                    \
																\
    label(.SUB16MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_16MASKx5_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    vmulpd( zmm30,zmm15,zmm15 )                                 \
    STORE_ROWSTORED_C_16MASKxN_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 4 edge kernel begins */                           \
    label(.EDGE4XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB4DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB4PREFETCHLOOP)                                      \
    label(.SUB4LOOP1)                                           \
        UNROLL_K_LOOP_4x5_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB4LOOP1)                                             \
																\
    label(.SUB4PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB4TAILITER)                                          \
																\
    label(.SUB4LOOP2)                                           \
																\
        UNROLL_K_LOOP_4x5_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB4LOOP2)                                             \
    label(.SUB4TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB4TAIL)                                              \
																\
    label(.SUB4LOOP3)                                           \
																\
        UNROLL_K_LOOP_4x5_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB4LOOP3)                                             \
																\
    vaddpd(ymm7, ymm6, ymm6)                                    \
    vaddpd(ymm9, ymm8, ymm8)                                    \
    vaddpd(ymm11, ymm10, ymm10)                                 \
    vaddpd(ymm13, ymm12, ymm12)                                 \
    vaddpd(ymm15, ymm14, ymm14)                                 \
																\
    label(.SUB4TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB4DPOSTACCUM)                                         \
																\
    label(.SUB4DLOOPKLEFT)                                      \
        vmovupd( mem(rax),ymm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_4x5_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB4DLOOPKLEFT)                                        \
																\
    label(.SUB4DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB4DBETAZERO)                                          \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB4DROWSTORED)                                         \
																\
    label(.SUB4DCOLSTORED)                                      \
    STORE_COLSTORED_C_4x5_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    STORE_ROWSTORED_C_4xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB4DROWSTORBZ)                                         \
    label(.SUB4DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_4x5_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    STORE_ROWSTORED_C_4xN_B0                                    \
    jmp(.CONCLUDE)                                              \
    /* m_left 2 edge kernel begins */                           \
    label(.EDGE2XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB2DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB2PREFETCHLOOP)                                      \
    label(.SUB2LOOP1)                                           \
        UNROLL_K_LOOP_2x5_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB2LOOP1)                                             \
																\
    label(.SUB2PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB2TAILITER)                                          \
																\
    label(.SUB2LOOP2)                                           \
																\
        UNROLL_K_LOOP_2x5_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB2LOOP2)                                             \
    label(.SUB2TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB2TAIL)                                              \
																\
    label(.SUB2LOOP3)                                           \
        UNROLL_K_LOOP_2x5_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB2LOOP3)                                             \
																\
    vaddpd(xmm7, xmm6, xmm6)                                    \
    vaddpd(xmm9, xmm8, xmm8)                                    \
    vaddpd(xmm11, xmm10, xmm10)                                 \
    vaddpd(xmm13, xmm12, xmm12)                                 \
    vaddpd(xmm15, xmm14, xmm14)                                 \
																\
    label(.SUB2TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB2DPOSTACCUM)                                         \
																\
    label(.SUB2DLOOPKLEFT)                                      \
        vmovupd( mem(rax),xmm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_2x5_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB2DLOOPKLEFT)                                        \
																\
    label(.SUB2DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB2DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB2DROWSTORED)                                         \
																\
    label(.SUB2DCOLSTORED)                                      \
    STORE_COLSTORED_C_2x5_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    STORE_ROWSTORED_C_2xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB2DROWSTORBZ)                                         \
    label(.SUB2DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_2x5_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    STORE_ROWSTORED_C_2xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 1 edge kernel begins */                           \
    label(.EDGE1XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB1TAIL)                                               \
    label(.SUB1LOOP3)                                           \
        UNROLL_K_LOOP_1x5_L1                                    \
        dec(rsi)                                                \
        jnz(.SUB1LOOP3)                                         \
																\
    vaddpd(xmm7, xmm6, xmm6)                                    \
    vaddpd(xmm9, xmm8, xmm8)                                    \
    vaddpd(xmm11, xmm10, xmm10)                                 \
    vaddpd(xmm13, xmm12, xmm12)                                 \
    vaddpd(xmm15, xmm14, xmm14)                                 \
																\
    label(.SUB1TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB1DPOSTACCUM)                                         \
																\
    label(.SUB1DLOOPKLEFT)                                      \
        vmovsd( mem(rax),xmm0 )                                 \
        add( r10,rax )                                          \
        K_LOOP_1x5_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB1DLOOPKLEFT)                                        \
																\
    label(.SUB1DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB1DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB1DROWSTORED)                                         \
																\
    label(.SUB1DCOLSTORED)                                      \
    STORE_COLSTORED_C_1x5_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    STORE_ROWSTORED_C_1xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB1DROWSTORBZ)                                         \
    label(.SUB1DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_1x5_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    STORE_ROWSTORED_C_1xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 8 mask edge kernel begins */                      \
    label(.EDGE8MASKXN)                                         \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r15 = r11 + 4* cs_b(B for prefetching) */                \
    lea(mem(r11, r9, 4), r15)                                   \
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.EDGE8MASKDLOOPKITER)                                 \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.EDGE8MASKPREFETCHLOOP)                                 \
    label(.EDGE8MASKLOOP1)                                      \
        UNROLL_K_LOOP_8MASKx5_L1                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP1)                                        \
																\
    label(.EDGE8MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.EDGE8MASKTAILITER)                                     \
																\
    label(.EDGE8MASKLOOP2)                                      \
																\
        UNROLL_K_LOOP_8MASKx5_L2                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP2)                                        \
    label(.EDGE8MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.EDGE8MASKTAIL)                                         \
																\
    label(.EDGE8MASKLOOP3)                                      \
																\
        UNROLL_K_LOOP_8MASKx5_L1                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP3)                                        \
																\
    vaddpd(zmm7, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm8, zmm8)                                    \
    vaddpd(zmm11, zmm10, zmm10)                                 \
    vaddpd(zmm13, zmm12, zmm12)                                 \
    vaddpd(zmm15, zmm14, zmm14)                                 \
    vaddpd(zmm17, zmm16, zmm16)                                 \
    vaddpd(zmm19, zmm18, zmm18)                                 \
																\
    label(.EDGE8MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.EDGE8MASKDPOSTACCUM)                                    \
																\
    label(.EDGE8MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 MASK_KZ(2) )                     \
        add( r10,rax )                                          \
        K_LOOP_8MASKx5_SET1(0)                                  \
        dec(rsi)                                                \
    jne(.EDGE8MASKDLOOPKLEFT)                                   \
																\
    label(.EDGE8MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.EDGE8MASKDBETAZERO)                                     \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.EDGE8MASKDROWSTORED)                                    \
																\
    label(.EDGE8MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_8MASKx5_Bn                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    STORE_ROWSTORED_C_8MASKxN_Bn                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.EDGE8MASKDROWSTORBZ)                                    \
																\
    label(.EDGE8MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_8MASKx5_B0                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(5), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm14,zmm14 )                                 \
    STORE_ROWSTORED_C_8MASKxN_B0                                \
    jmp(.CONCLUDE)


#define M_LEFT_24x4												\
																\
    /* m_left 20 edge kernel begins */                          \
    label(.EDGE20XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB20PREFETCHLOOP)                                     \
    label(.SUB20LOOP1)                                          \
																\
        UNROLL_K_LOOP_20x4_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP1)                                            \
																\
    label(.SUB20PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB20TAILITER)                                         \
																\
    label(.SUB20LOOP2)                                          \
        UNROLL_K_LOOP_20x4_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP2)                                            \
    label(.SUB20TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB20TAIL)                                             \
    label(.SUB20LOOP3)                                          \
        UNROLL_K_LOOP_20x4_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP3)                                            \
																\
    vaddpd(zmm14, zmm6, zmm6)                                   \
    vaddpd(zmm15, zmm7, zmm7)                                   \
    vaddpd(zmm16, zmm28, zmm28)                                 \
    vaddpd(zmm17, zmm8, zmm8)                                   \
    vaddpd(zmm18, zmm9, zmm9)                                   \
    vaddpd(zmm19, zmm29, zmm29)                                 \
    vaddpd(zmm20, zmm10, zmm10)                                 \
    vaddpd(zmm21, zmm11, zmm11)                                 \
    vaddpd(zmm22, zmm26, zmm26)                                 \
    vaddpd(zmm23, zmm12, zmm12)                                 \
    vaddpd(zmm24, zmm13, zmm13)                                 \
    vaddpd(zmm25, zmm27, zmm27)                                 \
																\
    label(.SUB20TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB20DPOSTACCUM)                                        \
    label(.SUB20DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax), ymm2 )                              \
        add( r10,rax )                                          \
        K_LOOP_20x4_SET1(0, 1, 2)                               \
        dec(rsi)                                                \
    jne(.SUB20DLOOPKLEFT)                                       \
																\
    label(.SUB20DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB20DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB20DROWSTORED)                                        \
																\
    label(.SUB20DCOLSTORED)                                     \
    STORE_COLSTORED_C_20x4_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    STORE_ROWSTORED_C_20xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB20DROWSTORBZ)                                        \
																\
    label(.SUB20DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_20x4_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    STORE_ROWSTORED_C_20xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 18 edge kernel begins */                          \
    label(.EDGE18XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB18PREFETCHLOOP)                                     \
    label(.SUB18LOOP1)                                          \
																\
        UNROLL_K_LOOP_18x4_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP1)                                            \
																\
    label(.SUB18PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB18TAILITER)                                         \
																\
    label(.SUB18LOOP2)                                          \
        UNROLL_K_LOOP_18x4_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP2)                                            \
    label(.SUB18TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB18TAIL)                                             \
    label(.SUB18LOOP3)                                          \
        UNROLL_K_LOOP_18x4_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP3)                                            \
																\
    vaddpd(zmm14, zmm6, zmm6)                                   \
    vaddpd(zmm15, zmm7, zmm7)                                   \
    vaddpd(zmm16, zmm28, zmm28)                                 \
    vaddpd(zmm17, zmm8, zmm8)                                   \
    vaddpd(zmm18, zmm9, zmm9)                                   \
    vaddpd(zmm19, zmm29, zmm29)                                 \
    vaddpd(zmm20, zmm10, zmm10)                                 \
    vaddpd(zmm21, zmm11, zmm11)                                 \
    vaddpd(zmm22, zmm26, zmm26)                                 \
    vaddpd(zmm23, zmm12, zmm12)                                 \
    vaddpd(zmm24, zmm13, zmm13)                                 \
    vaddpd(zmm25, zmm27, zmm27)                                 \
																\
    label(.SUB18TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB18DPOSTACCUM)                                        \
																\
    label(.SUB18DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax), xmm2 )                              \
        add( r10,rax )                                          \
        K_LOOP_18x4_SET1(0, 1, 2)                               \
        dec(rsi)                                                \
    jne(.SUB18DLOOPKLEFT)                                       \
																\
    label(.SUB18DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB18DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB18DROWSTORED)                                        \
																\
    label(.SUB18DCOLSTORED)                                     \
    STORE_COLSTORED_C_18x4_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    STORE_ROWSTORED_C_18xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB18DROWSTORBZ)                                        \
																\
    label(.SUB18DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_18x4_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    STORE_ROWSTORED_C_18xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 17 edge kernel begins */                          \
    label(.EDGE17XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB17PREFETCHLOOP)                                     \
    label(.SUB17LOOP1)                                          \
																\
        UNROLL_K_LOOP_17x4_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP1)                                            \
																\
    label(.SUB17PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB17TAILITER)                                         \
																\
    label(.SUB17LOOP2)                                          \
        UNROLL_K_LOOP_17x4_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP2)                                            \
    label(.SUB17TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB17TAIL)                                             \
    label(.SUB17LOOP3)                                          \
        UNROLL_K_LOOP_17x4_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP3)                                            \
																\
    vaddpd(zmm14, zmm6, zmm6)                                   \
    vaddpd(zmm15, zmm7, zmm7)                                   \
    vaddpd(zmm16, zmm28, zmm28)                                 \
    vaddpd(zmm17, zmm8, zmm8)                                   \
    vaddpd(zmm18, zmm9, zmm9)                                   \
    vaddpd(zmm19, zmm29, zmm29)                                 \
    vaddpd(zmm20, zmm10, zmm10)                                 \
    vaddpd(zmm21, zmm11, zmm11)                                 \
    vaddpd(zmm22, zmm26, zmm26)                                 \
    vaddpd(zmm23, zmm12, zmm12)                                 \
    vaddpd(zmm24, zmm13, zmm13)                                 \
    vaddpd(zmm25, zmm27, zmm27)                                 \
																\
    label(.SUB17TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB17DPOSTACCUM)                                        \
    label(.SUB17DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovsd ( 0x80(rax),xmm2 )                               \
        add( r10,rax )                                          \
        K_LOOP_17x4_SET1(0, 1, 2)                               \
        dec(rsi)                                                \
    jne(.SUB17DLOOPKLEFT)                                       \
																\
    label(.SUB17DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB17DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB17DROWSTORED)                                        \
																\
    label(.SUB17DCOLSTORED)                                     \
    STORE_COLSTORED_C_17x4_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    STORE_ROWSTORED_C_17xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB17DROWSTORBZ)                                        \
																\
    label(.SUB17DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_17x4_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    STORE_ROWSTORED_C_17xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 16 edge kernel begins */                          \
    label(.EDGE16XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB16DLOOPKITER)                                     \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB16PREFETCHLOOP)                                     \
    label(.SUB16LOOP1)                                          \
																\
        UNROLL_K_LOOP_16x4_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP1)                                            \
																\
    label(.SUB16PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB16TAILITER)                                         \
																\
    label(.SUB16LOOP2)                                          \
        UNROLL_K_LOOP_16x4_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP2)                                            \
    label(.SUB16TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB16TAIL)                                             \
																\
    label(.SUB16LOOP3)                                          \
																\
        UNROLL_K_LOOP_16x4_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP3)                                            \
																\
    vaddpd(zmm14, zmm6, zmm6)                                   \
    vaddpd(zmm15, zmm7, zmm7)                                   \
    vaddpd(zmm16, zmm8, zmm8)                                   \
    vaddpd(zmm17, zmm9, zmm9)                                   \
    vaddpd(zmm18, zmm10, zmm10)                                 \
    vaddpd(zmm19, zmm11, zmm11)                                 \
    vaddpd(zmm20, zmm12, zmm12)                                 \
    vaddpd(zmm21, zmm13, zmm13)                                 \
																\
    label(.SUB16TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB16DPOSTACCUM)                                        \
																\
    label(.SUB16DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        add( r10,rax )                                          \
																\
        K_LOOP_16x4_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB16DLOOPKLEFT)                                       \
																\
    label(.SUB16DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB16DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB16DROWSTORED)                                        \
																\
    label(.SUB16DCOLSTORED)                                     \
    STORE_COLSTORED_C_16x4_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    STORE_ROWSTORED_C_16xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16DROWSTORBZ)                                        \
																\
    label(.SUB16DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_16x4_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    STORE_ROWSTORED_C_16xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /**********  M_Left 24 masked kernel **************/        \
    label(.EDGE24MASKXN)                                        \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    sub(imm(16), rsi)                                           \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB24MASKPREFETCHLOOP)                                 \
    label(.SUB24MASKLOOP1)                                      \
																\
        UNROLL_K_LOOP_24MASKx4_L1                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP1)                                        \
																\
    label(.SUB24MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.SUB24MASKTAILITER)                                     \
																\
    label(.SUB24MASKLOOP2)                                      \
        UNROLL_K_LOOP_24MASKx4_L2                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP2)                                        \
    label(.SUB24MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB24MASKTAIL)                                         \
    label(.SUB24MASKLOOP3)                                      \
        UNROLL_K_LOOP_24MASKx4_L1                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP3)                                        \
																\
    vaddpd(zmm14, zmm6, zmm6)                                   \
    vaddpd(zmm15, zmm7, zmm7)                                   \
    vaddpd(zmm16, zmm28, zmm28)                                 \
    vaddpd(zmm17, zmm8, zmm8)                                   \
    vaddpd(zmm18, zmm9, zmm9)                                   \
    vaddpd(zmm19, zmm29, zmm29)                                 \
    vaddpd(zmm20, zmm10, zmm10)                                 \
    vaddpd(zmm21, zmm11, zmm11)                                 \
    vaddpd(zmm22, zmm26, zmm26)                                 \
    vaddpd(zmm23, zmm12, zmm12)                                 \
    vaddpd(zmm24, zmm13, zmm13)                                 \
    vaddpd(zmm25, zmm27, zmm27)                                 \
																\
    label(.SUB24MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB24MASKDPOSTACCUM)                                    \
    label(.SUB24MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax),zmm2 MASK_KZ(2) )                    \
        add( r10,rax )                                          \
        K_LOOP_24MASKx4_SET1(0, 1, 2)                           \
        dec(rsi)                                                \
    jne(.SUB24MASKDLOOPKLEFT)                                   \
    label(.SUB24MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.SUB24MASKDBETAZERO)                                     \
    cmp(imm(8), rdi)                                            \
    jz(.SUB24MASKDROWSTORED)                                    \
																\
    label(.SUB24MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_24MASKx4_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    STORE_ROWSTORED_C_24MASKxN_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.SUB24MASKDROWSTORBZ)                                    \
																\
    label(.SUB24MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_24MASKx4_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    STORE_ROWSTORED_C_24MASKxN_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 12 edge kernel begins */                          \
    label(.EDGE12XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB12DLOOPKITER)                                     \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB12PREFETCHLOOP)                                     \
																\
    label(.SUB12LOOP1)                                          \
																\
        UNROLL_K_LOOP_12x4_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB12LOOP1)                                            \
																\
    label(.SUB12PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB12TAILITER)                                         \
																\
    label(.SUB12LOOP2)                                          \
        UNROLL_K_LOOP_12x4_L2                                   \
        sub(imm(1), rsi)                                        \
    jnz(.SUB12LOOP2)                                            \
    label(.SUB12TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB12TAIL)                                             \
																\
    label(.SUB12LOOP3)                                          \
        UNROLL_K_LOOP_12x4_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB12LOOP3)                                            \
																\
    vaddpd(zmm14, zmm6, zmm6)                                   \
    vaddpd(zmm15, zmm7, zmm7)                                   \
    vaddpd(zmm16, zmm8, zmm8)                                   \
    vaddpd(zmm17, zmm9, zmm9)                                   \
    vaddpd(zmm18, zmm10, zmm10)                                 \
    vaddpd(zmm19, zmm11, zmm11)                                 \
    vaddpd(zmm20, zmm12, zmm12)                                 \
    vaddpd(zmm21, zmm13, zmm13)                                 \
																\
    label(.SUB12TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB12DPOSTACCUM)                                        \
																\
    label(.SUB12DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),ymm1 )                               \
        add( r10,rax )                                          \
        K_LOOP_12x4_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB12DLOOPKLEFT)                                       \
																\
    label(.SUB12DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB12DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
    jz(.SUB12DROWSTORED)                                        \
																\
    label(.SUB12DCOLSTORED)                                     \
    STORE_COLSTORED_C_12x4_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    STORE_ROWSTORED_C_12xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB12DROWSTORBZ)                                        \
																\
    label(.SUB12DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_12x4_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    STORE_ROWSTORED_C_12xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 11 edge kernel begins */                          \
    label(.EDGE11XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB11PREFETCHLOOP)                                     \
    label(.SUB11LOOP1)                                          \
																\
        UNROLL_K_LOOP_11x4_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP1)                                            \
																\
    label(.SUB11PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB11TAILITER)                                         \
																\
    label(.SUB11LOOP2)                                          \
        UNROLL_K_LOOP_11x4_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP2)                                            \
    label(.SUB11TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB11TAIL)                                             \
    label(.SUB11LOOP3)                                          \
        UNROLL_K_LOOP_11x4_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP3)                                            \
																\
    vaddpd(zmm14, zmm6, zmm6)                                   \
    vaddpd(zmm15, zmm7, zmm7)                                   \
    vaddpd(zmm16, zmm28, zmm28)                                 \
    vaddpd(zmm17, zmm8, zmm8)                                   \
    vaddpd(zmm18, zmm9, zmm9)                                   \
    vaddpd(zmm19, zmm29, zmm29)                                 \
    vaddpd(zmm20, zmm10, zmm10)                                 \
    vaddpd(zmm21, zmm11, zmm11)                                 \
    vaddpd(zmm22, zmm26, zmm26)                                 \
    vaddpd(zmm23, zmm12, zmm12)                                 \
    vaddpd(zmm24, zmm13, zmm13)                                 \
    vaddpd(zmm25, zmm27, zmm27)                                 \
																\
    label(.SUB11TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB11DPOSTACCUM)                                        \
																\
    label(.SUB11DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),xmm1 )                               \
        vmovsd( 0x50(rax),xmm2 )                                \
        add( r10,rax )                                          \
        K_LOOP_11x4_SET1(0, 1, 2)                               \
        dec(rsi)                                                \
    jne(.SUB11DLOOPKLEFT)                                       \
																\
    label(.SUB11DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB11DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB11DROWSTORED)                                        \
																\
    label(.SUB11DCOLSTORED)                                     \
    STORE_COLSTORED_C_11x4_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    STORE_ROWSTORED_C_11xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB11DROWSTORBZ)                                        \
    label(.SUB11DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_11x4_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    vmulpd( zmm30,zmm27,zmm27 )                                 \
    STORE_ROWSTORED_C_11xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 10 edge kernel begins */                          \
    label(.EDGE10XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB10PREFETCHLOOP)                                     \
    label(.SUB10LOOP1)                                          \
																\
        UNROLL_K_LOOP_10x4_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP1)                                            \
																\
    label(.SUB10PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB10TAILITER)                                         \
																\
    label(.SUB10LOOP2)                                          \
        UNROLL_K_LOOP_10x4_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP2)                                            \
    label(.SUB10TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB10TAIL)                                             \
    label(.SUB10LOOP3)                                          \
        UNROLL_K_LOOP_10x4_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP3)                                            \
																\
    vaddpd(zmm14, zmm6, zmm6)                                   \
    vaddpd(zmm15, zmm7, zmm7)                                   \
    vaddpd(zmm16, zmm8, zmm8)                                   \
    vaddpd(zmm17, zmm9, zmm9)                                   \
    vaddpd(zmm18, zmm10, zmm10)                                 \
    vaddpd(zmm19, zmm11, zmm11)                                 \
    vaddpd(zmm20, zmm12, zmm12)                                 \
    vaddpd(zmm21, zmm13, zmm13)                                 \
																\
    label(.SUB10TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB10DPOSTACCUM)                                        \
																\
    label(.SUB10DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),xmm1 )                               \
        add( r10,rax )                                          \
        K_LOOP_10x4_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB10DLOOPKLEFT)                                       \
																\
    label(.SUB10DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB10DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB10DROWSTORED)                                        \
																\
    label(.SUB10DCOLSTORED)                                     \
    STORE_COLSTORED_C_10x4_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    STORE_ROWSTORED_C_10xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB10DROWSTORBZ)                                        \
    label(.SUB10DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_10x4_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    STORE_ROWSTORED_C_10xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 9 edge kernel begins */                           \
    label(.EDGE9XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB9DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB9PREFETCHLOOP)                                      \
																\
    label(.SUB9LOOP1)                                           \
																\
        UNROLL_K_LOOP_9x4_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB9LOOP1)                                             \
																\
    label(.SUB9PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB9TAILITER)                                          \
																\
    label(.SUB9LOOP2)                                           \
        UNROLL_K_LOOP_9x4_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB9LOOP2)                                             \
    label(.SUB9TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB9TAIL)                                              \
																\
    label(.SUB9LOOP3)                                           \
																\
        UNROLL_K_LOOP_9x4_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB9LOOP3)                                             \
																\
    vaddpd(zmm14, zmm6, zmm6)                                   \
    vaddpd(zmm15, zmm7, zmm7)                                   \
    vaddpd(zmm16, zmm8, zmm8)                                   \
    vaddpd(zmm17, zmm9, zmm9)                                   \
    vaddpd(zmm18, zmm10, zmm10)                                 \
    vaddpd(zmm19, zmm11, zmm11)                                 \
    vaddpd(zmm20, zmm12, zmm12)                                 \
    vaddpd(zmm21, zmm13, zmm13)                                 \
																\
    label(.SUB9TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB9DPOSTACCUM)                                         \
																\
    label(.SUB9DLOOPKLEFT)                                      \
        vmovupd( mem(rax),zmm0 )                                \
        vmovsd( 0x40(rax),xmm1 )                                \
        add( r10,rax )                                          \
        K_LOOP_9x4_SET1(0, 1)                                   \
        dec(rsi)                                                \
    jne(.SUB9DLOOPKLEFT)                                        \
																\
    label(.SUB9DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB9DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB9DROWSTORED)                                         \
																\
    label(.SUB9DCOLSTORED)                                      \
    STORE_COLSTORED_C_9x4_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    STORE_ROWSTORED_C_9xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB9DROWSTORBZ)                                         \
    label(.SUB9DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_9x4_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    STORE_ROWSTORED_C_9xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 8 edge kernel begins */                           \
    label(.EDGE8XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB8DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB8PREFETCHLOOP)                                      \
    label(.SUB8LOOP1)                                           \
        UNROLL_K_LOOP_8x4_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB8LOOP1)                                             \
																\
    label(.SUB8PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB8TAILITER)                                          \
																\
    label(.SUB8LOOP2)                                           \
        UNROLL_K_LOOP_8x4_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB8LOOP2)                                             \
    label(.SUB8TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB8TAIL)                                              \
																\
    label(.SUB8LOOP3)                                           \
        UNROLL_K_LOOP_8x4_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB8LOOP3)                                             \
																\
    vaddpd(zmm7, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm8, zmm8)                                    \
    vaddpd(zmm11, zmm10, zmm10)                                 \
    vaddpd(zmm13, zmm12, zmm12)                                 \
																\
    label(.SUB8TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB8DPOSTACCUM)                                         \
																\
    label(.SUB8DLOOPKLEFT)                                      \
        vmovupd( mem(rax),zmm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_8x4_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB8DLOOPKLEFT)                                        \
																\
    label(.SUB8DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB8DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB8DROWSTORED)                                         \
																\
    label(.SUB8DCOLSTORED)                                      \
																\
    STORE_COLSTORED_C_8x4_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    STORE_ROWSTORED_C_8xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB8DROWSTORBZ)                                         \
    label(.SUB8DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_8x4_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    STORE_ROWSTORED_C_8xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 16 mask edge kernel begins */                     \
    label(.EDGE16MASKXN)                                        \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    sub(imm(8), rsi)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
    lea(mem(r11, r9, 4), r15)                                   \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB16MASKDLOOPKITER)                                 \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB16MASKPREFETCHLOOP)                                 \
																\
    label(.SUB16MASKLOOP1)                                      \
																\
        UNROLL_K_LOOP_16MASKx4_L1                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP1)                                        \
																\
    label(.SUB16MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.SUB16MASKTAILITER)                                     \
																\
    label(.SUB16MASKLOOP2)                                      \
																\
        UNROLL_K_LOOP_16MASKx4_L2                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP2)                                        \
    label(.SUB16MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB16MASKTAIL)                                         \
																\
    label(.SUB16MASKLOOP3)                                      \
																\
        UNROLL_K_LOOP_16MASKx4_L1                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP3)                                        \
																\
    vaddpd(zmm14, zmm6, zmm6)                                   \
    vaddpd(zmm15, zmm7, zmm7)                                   \
    vaddpd(zmm16, zmm8, zmm8)                                   \
    vaddpd(zmm17, zmm9, zmm9)                                   \
    vaddpd(zmm18, zmm10, zmm10)                                 \
    vaddpd(zmm19, zmm11, zmm11)                                 \
    vaddpd(zmm20, zmm12, zmm12)                                 \
    vaddpd(zmm21, zmm13, zmm13)                                 \
																\
    label(.SUB16MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB16MASKDPOSTACCUM)                                    \
																\
    label(.SUB16MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )                    \
        add( r10,rax )                                          \
        K_LOOP_16MASKx4_SET1(0, 1)                              \
        dec(rsi)                                                \
    jne(.SUB16MASKDLOOPKLEFT)                                   \
																\
    label(.SUB16MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.SUB16MASKDBETAZERO)                                     \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16MASKDROWSTORED)                                    \
																\
    label(.SUB16MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_16MASKx4_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    STORE_ROWSTORED_C_16MASKxN_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16MASKDROWSTORBZ)                                    \
																\
    label(.SUB16MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_16MASKx4_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    vmulpd( zmm30,zmm13,zmm13 )                                 \
    STORE_ROWSTORED_C_16MASKxN_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 4 edge kernel begins */                           \
    label(.EDGE4XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB4DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB4PREFETCHLOOP)                                      \
    label(.SUB4LOOP1)                                           \
        UNROLL_K_LOOP_4x4_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB4LOOP1)                                             \
																\
    label(.SUB4PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB4TAILITER)                                          \
																\
    label(.SUB4LOOP2)                                           \
																\
        UNROLL_K_LOOP_4x4_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB4LOOP2)                                             \
    label(.SUB4TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB4TAIL)                                              \
																\
    label(.SUB4LOOP3)                                           \
																\
        UNROLL_K_LOOP_4x4_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB4LOOP3)                                             \
																\
    vaddpd(ymm7, ymm6, ymm6)                                    \
    vaddpd(ymm9, ymm8, ymm8)                                    \
    vaddpd(ymm11, ymm10, ymm10)                                 \
    vaddpd(ymm13, ymm12, ymm12)                                 \
																\
    label(.SUB4TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB4DPOSTACCUM)                                         \
																\
    label(.SUB4DLOOPKLEFT)                                      \
        vmovupd( mem(rax),ymm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_4x4_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB4DLOOPKLEFT)                                        \
																\
    label(.SUB4DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB4DBETAZERO)                                          \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB4DROWSTORED)                                         \
																\
    label(.SUB4DCOLSTORED)                                      \
    STORE_COLSTORED_C_4x4_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    STORE_ROWSTORED_C_4xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB4DROWSTORBZ)                                         \
    label(.SUB4DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_4x4_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    STORE_ROWSTORED_C_4xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 2 edge kernel begins */                           \
    label(.EDGE2XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB2DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB2PREFETCHLOOP)                                      \
    label(.SUB2LOOP1)                                           \
        UNROLL_K_LOOP_2x4_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB2LOOP1)                                             \
																\
    label(.SUB2PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB2TAILITER)                                          \
																\
    label(.SUB2LOOP2)                                           \
																\
        UNROLL_K_LOOP_2x4_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB2LOOP2)                                             \
    label(.SUB2TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB2TAIL)                                              \
																\
    label(.SUB2LOOP3)                                           \
        UNROLL_K_LOOP_2x4_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB2LOOP3)                                             \
																\
    vaddpd(xmm7, xmm6, xmm6)                                    \
    vaddpd(xmm9, xmm8, xmm8)                                    \
    vaddpd(xmm11, xmm10, xmm10)                                 \
    vaddpd(xmm13, xmm12, xmm12)                                 \
																\
    label(.SUB2TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB2DPOSTACCUM)                                         \
																\
    label(.SUB2DLOOPKLEFT)                                      \
        vmovupd( mem(rax),xmm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_2x4_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB2DLOOPKLEFT)                                        \
																\
    label(.SUB2DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB2DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB2DROWSTORED)                                         \
																\
    label(.SUB2DCOLSTORED)                                      \
    STORE_COLSTORED_C_2x4_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    STORE_ROWSTORED_C_2xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB2DROWSTORBZ)                                         \
    label(.SUB2DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_2x4_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    STORE_ROWSTORED_C_2xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 1 edge kernel begins */                           \
    label(.EDGE1XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB1TAIL)                                               \
    label(.SUB1LOOP3)                                           \
        UNROLL_K_LOOP_1x4_L1                                    \
        dec(rsi)                                                \
        jnz(.SUB1LOOP3)                                         \
																\
    vaddpd(xmm7, xmm6, xmm6)                                    \
    vaddpd(xmm9, xmm8, xmm8)                                    \
    vaddpd(xmm11, xmm10, xmm10)                                 \
    vaddpd(xmm13, xmm12, xmm12)                                 \
																\
    label(.SUB1TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB1DPOSTACCUM)                                         \
																\
    label(.SUB1DLOOPKLEFT)                                      \
        vmovsd( mem(rax),xmm0 )                                 \
        add( r10,rax )                                          \
        K_LOOP_1x4_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB1DLOOPKLEFT)                                        \
																\
    label(.SUB1DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB1DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB1DROWSTORED)                                         \
																\
    label(.SUB1DCOLSTORED)                                      \
    STORE_COLSTORED_C_1x4_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    STORE_ROWSTORED_C_1xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB1DROWSTORBZ)                                         \
    label(.SUB1DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_1x4_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    STORE_ROWSTORED_C_1xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 8 mask edge kernel begins */                      \
    label(.EDGE8MASKXN)                                         \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.EDGE8MASKDLOOPKITER)                                 \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.EDGE8MASKPREFETCHLOOP)                                 \
    label(.EDGE8MASKLOOP1)                                      \
        UNROLL_K_LOOP_8MASKx4_L1                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP1)                                        \
																\
    label(.EDGE8MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.EDGE8MASKTAILITER)                                     \
																\
    label(.EDGE8MASKLOOP2)                                      \
																\
        UNROLL_K_LOOP_8MASKx4_L2                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP2)                                        \
    label(.EDGE8MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.EDGE8MASKTAIL)                                         \
																\
    label(.EDGE8MASKLOOP3)                                      \
																\
        UNROLL_K_LOOP_8MASKx4_L1                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP3)                                        \
																\
    vaddpd(zmm7, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm8, zmm8)                                    \
    vaddpd(zmm11, zmm10, zmm10)                                 \
    vaddpd(zmm13, zmm12, zmm12)                                 \
																\
    label(.EDGE8MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.EDGE8MASKDPOSTACCUM)                                    \
																\
    label(.EDGE8MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 MASK_KZ(2) )                     \
        add( r10,rax )                                          \
        K_LOOP_8MASKx4_SET1(0)                                  \
        dec(rsi)                                                \
    jne(.EDGE8MASKDLOOPKLEFT)                                   \
																\
    label(.EDGE8MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.EDGE8MASKDBETAZERO)                                     \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.EDGE8MASKDROWSTORED)                                    \
																\
    label(.EDGE8MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_8MASKx4_Bn                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    STORE_ROWSTORED_C_8MASKxN_Bn                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.EDGE8MASKDROWSTORBZ)                                    \
																\
    label(.EDGE8MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_8MASKx4_B0                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(4), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm12,zmm12 )                                 \
    STORE_ROWSTORED_C_8MASKxN_B0                                \
    jmp(.CONCLUDE)


#define M_LEFT_24x3												\
																\
    /* m_left 20 edge kernel begins */                          \
    label(.EDGE20XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB20PREFETCHLOOP)                                     \
    label(.SUB20LOOP1)                                          \
																\
        UNROLL_K_LOOP_20x3_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP1)                                            \
																\
    label(.SUB20PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB20TAILITER)                                         \
																\
    label(.SUB20LOOP2)                                          \
        UNROLL_K_LOOP_20x3_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP2)                                            \
    label(.SUB20TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB20TAIL)                                             \
    label(.SUB20LOOP3)                                          \
        UNROLL_K_LOOP_20x3_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP3)                                            \
																\
    vaddpd(zmm12, zmm6, zmm6)                                   \
    vaddpd(zmm13, zmm7, zmm7)                                   \
    vaddpd(zmm14, zmm28, zmm28)                                 \
    vaddpd(zmm15, zmm8, zmm8)                                   \
    vaddpd(zmm16, zmm9, zmm9)                                   \
    vaddpd(zmm17, zmm29, zmm29)                                 \
    vaddpd(zmm18, zmm10, zmm10)                                 \
    vaddpd(zmm19, zmm11, zmm11)                                 \
    vaddpd(zmm20, zmm26, zmm26)                                 \
																\
    label(.SUB20TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB20DPOSTACCUM)                                        \
    label(.SUB20DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax), ymm2 )                              \
        add( r10,rax )                                          \
        K_LOOP_20x3_SET1(0, 1, 2)                               \
        dec(rsi)                                                \
    jne(.SUB20DLOOPKLEFT)                                       \
																\
    label(.SUB20DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB20DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB20DROWSTORED)                                        \
																\
    label(.SUB20DCOLSTORED)                                     \
    STORE_COLSTORED_C_20x3_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    STORE_ROWSTORED_C_20xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB20DROWSTORBZ)                                        \
																\
    label(.SUB20DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_20x3_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    STORE_ROWSTORED_C_20xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 18 edge kernel begins */                          \
    label(.EDGE18XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB18PREFETCHLOOP)                                     \
    label(.SUB18LOOP1)                                          \
																\
        UNROLL_K_LOOP_18x3_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP1)                                            \
																\
    label(.SUB18PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB18TAILITER)                                         \
																\
    label(.SUB18LOOP2)                                          \
        UNROLL_K_LOOP_18x3_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP2)                                            \
    label(.SUB18TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB18TAIL)                                             \
    label(.SUB18LOOP3)                                          \
        UNROLL_K_LOOP_18x3_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP3)                                            \
																\
    vaddpd(zmm12, zmm6, zmm6)                                   \
    vaddpd(zmm13, zmm7, zmm7)                                   \
    vaddpd(zmm14, zmm28, zmm28)                                 \
    vaddpd(zmm15, zmm8, zmm8)                                   \
    vaddpd(zmm16, zmm9, zmm9)                                   \
    vaddpd(zmm17, zmm29, zmm29)                                 \
    vaddpd(zmm18, zmm10, zmm10)                                 \
    vaddpd(zmm19, zmm11, zmm11)                                 \
    vaddpd(zmm20, zmm26, zmm26)                                 \
																\
    label(.SUB18TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB18DPOSTACCUM)                                        \
																\
    label(.SUB18DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax), xmm2 )                              \
        add( r10,rax )                                          \
        K_LOOP_18x3_SET1(0, 1, 2)                               \
        dec(rsi)                                                \
    jne(.SUB18DLOOPKLEFT)                                       \
																\
    label(.SUB18DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB18DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB18DROWSTORED)                                        \
    label(.SUB18DCOLSTORED)                                     \
    STORE_COLSTORED_C_18x3_Bn                                   \
    jmp(.CONCLUDE)                                              \
    label(.SUB18DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    STORE_ROWSTORED_C_18xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
    label(.SUB18DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB18DROWSTORBZ)                                        \
    label(.SUB18DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_18x3_B0                                   \
    jmp(.CONCLUDE)                                              \
    label(.SUB18DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    STORE_ROWSTORED_C_18xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
   /* m_left 17 edge kernel begins */                           \
    label(.EDGE17XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB17PREFETCHLOOP)                                     \
    label(.SUB17LOOP1)                                          \
																\
        UNROLL_K_LOOP_17x3_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP1)                                            \
																\
    label(.SUB17PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB17TAILITER)                                         \
																\
    label(.SUB17LOOP2)                                          \
        UNROLL_K_LOOP_17x3_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP2)                                            \
    label(.SUB17TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB17TAIL)                                             \
    label(.SUB17LOOP3)                                          \
        UNROLL_K_LOOP_17x3_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP3)                                            \
																\
    vaddpd(zmm12, zmm6, zmm6)                                   \
    vaddpd(zmm13, zmm7, zmm7)                                   \
    vaddpd(zmm14, zmm28, zmm28)                                 \
    vaddpd(zmm15, zmm8, zmm8)                                   \
    vaddpd(zmm16, zmm9, zmm9)                                   \
    vaddpd(zmm17, zmm29, zmm29)                                 \
    vaddpd(zmm18, zmm10, zmm10)                                 \
    vaddpd(zmm19, zmm11, zmm11)                                 \
    vaddpd(zmm20, zmm26, zmm26)                                 \
																\
    label(.SUB17TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB17DPOSTACCUM)                                        \
    label(.SUB17DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovsd ( 0x80(rax),xmm2 )                               \
        add( r10,rax )                                          \
        K_LOOP_17x3_SET1(0, 1, 2)                               \
        dec(rsi)                                                \
    jne(.SUB17DLOOPKLEFT)                                       \
																\
    label(.SUB17DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB17DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB17DROWSTORED)                                        \
																\
    label(.SUB17DCOLSTORED)                                     \
    STORE_COLSTORED_C_17x3_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    STORE_ROWSTORED_C_17xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB17DROWSTORBZ)                                        \
																\
    label(.SUB17DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_17x3_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    STORE_ROWSTORED_C_17xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 16 edge kernel begins */                          \
    label(.EDGE16XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB16DLOOPKITER)                                     \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB16PREFETCHLOOP)                                     \
    label(.SUB16LOOP1)                                          \
																\
        UNROLL_K_LOOP_16x3_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP1)                                            \
																\
    label(.SUB16PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB16TAILITER)                                         \
																\
    label(.SUB16LOOP2)                                          \
        UNROLL_K_LOOP_16x3_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP2)                                            \
    label(.SUB16TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB16TAIL)                                             \
																\
    label(.SUB16LOOP3)                                          \
																\
        UNROLL_K_LOOP_16x3_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP3)                                            \
																\
    vaddpd(zmm12, zmm6, zmm6)                                   \
    vaddpd(zmm13, zmm7, zmm7)                                   \
    vaddpd(zmm14, zmm8, zmm8)                                   \
    vaddpd(zmm15, zmm9, zmm9)                                   \
    vaddpd(zmm16, zmm10, zmm10)                                 \
    vaddpd(zmm17, zmm11, zmm11)                                 \
																\
    label(.SUB16TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB16DPOSTACCUM)                                        \
																\
    label(.SUB16DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        add( r10,rax )                                          \
																\
        K_LOOP_16x3_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB16DLOOPKLEFT)                                       \
																\
    label(.SUB16DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB16DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB16DROWSTORED)                                        \
																\
    label(.SUB16DCOLSTORED)                                     \
    STORE_COLSTORED_C_16x3_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    STORE_ROWSTORED_C_16xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16DROWSTORBZ)                                        \
																\
    label(.SUB16DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_16x3_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    STORE_ROWSTORED_C_16xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /**********  M_Left 24 masked kernel **************/        \
    label(.EDGE24MASKXN)                                        \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    sub(imm(16), rsi)                                           \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB24MASKPREFETCHLOOP)                                 \
    label(.SUB24MASKLOOP1)                                      \
																\
        UNROLL_K_LOOP_24MASKx3_L1                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP1)                                        \
																\
    label(.SUB24MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.SUB24MASKTAILITER)                                     \
																\
    label(.SUB24MASKLOOP2)                                      \
        UNROLL_K_LOOP_24MASKx3_L2                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP2)                                        \
    label(.SUB24MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB24MASKTAIL)                                         \
    label(.SUB24MASKLOOP3)                                      \
        UNROLL_K_LOOP_24MASKx3_L1                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP3)                                        \
																\
    vaddpd(zmm12, zmm6, zmm6)                                   \
    vaddpd(zmm13, zmm7, zmm7)                                   \
    vaddpd(zmm14, zmm28, zmm28)                                 \
    vaddpd(zmm15, zmm8, zmm8)                                   \
    vaddpd(zmm16, zmm9, zmm9)                                   \
    vaddpd(zmm17, zmm29, zmm29)                                 \
    vaddpd(zmm18, zmm10, zmm10)                                 \
    vaddpd(zmm19, zmm11, zmm11)                                 \
    vaddpd(zmm20, zmm26, zmm26)                                 \
																\
    label(.SUB24MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB24MASKDPOSTACCUM)                                    \
    label(.SUB24MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax),zmm2 MASK_KZ(2) )                    \
        add( r10,rax )                                          \
        K_LOOP_24MASKx3_SET1(0, 1, 2)                           \
        dec(rsi)                                                \
    jne(.SUB24MASKDLOOPKLEFT)                                   \
    label(.SUB24MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.SUB24MASKDBETAZERO)                                     \
    cmp(imm(8), rdi)                                            \
    jz(.SUB24MASKDROWSTORED)                                    \
																\
    label(.SUB24MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_24MASKx3_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    STORE_ROWSTORED_C_24MASKxN_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.SUB24MASKDROWSTORBZ)                                    \
																\
    label(.SUB24MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_24MASKx3_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    STORE_ROWSTORED_C_24MASKxN_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 12 edge kernel begins */                          \
    label(.EDGE12XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB12DLOOPKITER)                                     \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB12PREFETCHLOOP)                                     \
																\
    label(.SUB12LOOP1)                                          \
																\
        UNROLL_K_LOOP_12x3_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB12LOOP1)                                            \
																\
    label(.SUB12PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB12TAILITER)                                         \
																\
    label(.SUB12LOOP2)                                          \
        UNROLL_K_LOOP_12x3_L2                                   \
        sub(imm(1), rsi)                                        \
    jnz(.SUB12LOOP2)                                            \
    label(.SUB12TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB12TAIL)                                             \
																\
    label(.SUB12LOOP3)                                          \
        UNROLL_K_LOOP_12x3_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB12LOOP3)                                            \
																\
    vaddpd(zmm12, zmm6, zmm6)                                   \
    vaddpd(zmm13, zmm7, zmm7)                                   \
    vaddpd(zmm14, zmm8, zmm8)                                   \
    vaddpd(zmm15, zmm9, zmm9)                                   \
    vaddpd(zmm16, zmm10, zmm10)                                 \
    vaddpd(zmm17, zmm11, zmm11)                                 \
																\
    label(.SUB12TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB12DPOSTACCUM)                                        \
																\
    label(.SUB12DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),ymm1 )                               \
        add( r10,rax )                                          \
        K_LOOP_12x3_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB12DLOOPKLEFT)                                       \
																\
    label(.SUB12DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB12DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
    jz(.SUB12DROWSTORED)                                        \
																\
    label(.SUB12DCOLSTORED)                                     \
    STORE_COLSTORED_C_12x3_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    STORE_ROWSTORED_C_12xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB12DROWSTORBZ)                                        \
																\
    label(.SUB12DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_12x3_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    STORE_ROWSTORED_C_12xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 11 edge kernel begins */                          \
    label(.EDGE11XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB11PREFETCHLOOP)                                     \
    label(.SUB11LOOP1)                                          \
																\
        UNROLL_K_LOOP_11x3_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP1)                                            \
																\
    label(.SUB11PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB11TAILITER)                                         \
																\
    label(.SUB11LOOP2)                                          \
        UNROLL_K_LOOP_11x3_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP2)                                            \
    label(.SUB11TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB11TAIL)                                             \
    label(.SUB11LOOP3)                                          \
        UNROLL_K_LOOP_11x3_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP3)                                            \
																\
    vaddpd(zmm12, zmm6, zmm6)                                   \
    vaddpd(zmm13, zmm7, zmm7)                                   \
    vaddpd(zmm14, zmm28, zmm28)                                 \
    vaddpd(zmm15, zmm8, zmm8)                                   \
    vaddpd(zmm16, zmm9, zmm9)                                   \
    vaddpd(zmm17, zmm29, zmm29)                                 \
    vaddpd(zmm18, zmm10, zmm10)                                 \
    vaddpd(zmm19, zmm11, zmm11)                                 \
    vaddpd(zmm20, zmm26, zmm26)                                 \
																\
    label(.SUB11TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB11DPOSTACCUM)                                        \
																\
    label(.SUB11DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),xmm1 )                               \
        vmovsd( 0x50(rax),xmm2 )                                \
        add( r10,rax )                                          \
        K_LOOP_11x3_SET1(0, 1, 2)                               \
        dec(rsi)                                                \
    jne(.SUB11DLOOPKLEFT)                                       \
																\
    label(.SUB11DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB11DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB11DROWSTORED)                                        \
																\
    label(.SUB11DCOLSTORED)                                     \
    STORE_COLSTORED_C_11x3_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    STORE_ROWSTORED_C_11xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB11DROWSTORBZ)                                        \
    label(.SUB11DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_11x3_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    vmulpd( zmm30,zmm26,zmm26 )                                 \
    STORE_ROWSTORED_C_11xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 10 edge kernel begins */                          \
    label(.EDGE10XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB10PREFETCHLOOP)                                     \
    label(.SUB10LOOP1)                                          \
																\
        UNROLL_K_LOOP_10x3_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP1)                                            \
																\
    label(.SUB10PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB10TAILITER)                                         \
																\
    label(.SUB10LOOP2)                                          \
        UNROLL_K_LOOP_10x3_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP2)                                            \
    label(.SUB10TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB10TAIL)                                             \
    label(.SUB10LOOP3)                                          \
        UNROLL_K_LOOP_10x3_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP3)                                            \
																\
    vaddpd(zmm12, zmm6, zmm6)                                   \
    vaddpd(zmm13, zmm7, zmm7)                                   \
    vaddpd(zmm14, zmm8, zmm8)                                   \
    vaddpd(zmm15, zmm9, zmm9)                                   \
    vaddpd(zmm16, zmm10, zmm10)                                 \
    vaddpd(zmm17, zmm11, zmm11)                                 \
																\
    label(.SUB10TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB10DPOSTACCUM)                                        \
																\
    label(.SUB10DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),xmm1 )                               \
        add( r10,rax )                                          \
        K_LOOP_10x3_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB10DLOOPKLEFT)                                       \
																\
    label(.SUB10DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB10DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB10DROWSTORED)                                        \
																\
    label(.SUB10DCOLSTORED)                                     \
    STORE_COLSTORED_C_10x3_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    STORE_ROWSTORED_C_10xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB10DROWSTORBZ)                                        \
    label(.SUB10DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_10x3_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    STORE_ROWSTORED_C_10xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 9 edge kernel begins */                           \
    label(.EDGE9XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB9DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB9PREFETCHLOOP)                                      \
																\
    label(.SUB9LOOP1)                                           \
																\
        UNROLL_K_LOOP_9x3_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB9LOOP1)                                             \
																\
    label(.SUB9PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB9TAILITER)                                          \
																\
    label(.SUB9LOOP2)                                           \
        UNROLL_K_LOOP_9x3_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB9LOOP2)                                             \
    label(.SUB9TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB9TAIL)                                              \
																\
    label(.SUB9LOOP3)                                           \
																\
        UNROLL_K_LOOP_9x3_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB9LOOP3)                                             \
																\
    vaddpd(zmm12, zmm6, zmm6)                                   \
    vaddpd(zmm13, zmm7, zmm7)                                   \
    vaddpd(zmm14, zmm8, zmm8)                                   \
    vaddpd(zmm15, zmm9, zmm9)                                   \
    vaddpd(zmm16, zmm10, zmm10)                                 \
    vaddpd(zmm17, zmm11, zmm11)                                 \
																\
    label(.SUB9TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB9DPOSTACCUM)                                         \
																\
    label(.SUB9DLOOPKLEFT)                                      \
        vmovupd( mem(rax),zmm0 )                                \
        vmovsd( 0x40(rax),xmm1 )                                \
        add( r10,rax )                                          \
        K_LOOP_9x3_SET1(0, 1)                                   \
        dec(rsi)                                                \
    jne(.SUB9DLOOPKLEFT)                                        \
																\
    label(.SUB9DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB9DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB9DROWSTORED)                                         \
																\
    label(.SUB9DCOLSTORED)                                      \
    STORE_COLSTORED_C_9x3_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    STORE_ROWSTORED_C_9xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB9DROWSTORBZ)                                         \
    label(.SUB9DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_9x3_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    STORE_ROWSTORED_C_9xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 8 edge kernel begins */                           \
    label(.EDGE8XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB8DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB8PREFETCHLOOP)                                      \
    label(.SUB8LOOP1)                                           \
        UNROLL_K_LOOP_8x3_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB8LOOP1)                                             \
																\
    label(.SUB8PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB8TAILITER)                                          \
																\
    label(.SUB8LOOP2)                                           \
        UNROLL_K_LOOP_8x3_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB8LOOP2)                                             \
    label(.SUB8TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB8TAIL)                                              \
																\
    label(.SUB8LOOP3)                                           \
        UNROLL_K_LOOP_8x3_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB8LOOP3)                                             \
																\
    vaddpd(zmm7, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm8, zmm8)                                    \
    vaddpd(zmm11, zmm10, zmm10)                                 \
																\
    label(.SUB8TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB8DPOSTACCUM)                                         \
																\
    label(.SUB8DLOOPKLEFT)                                      \
        vmovupd( mem(rax),zmm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_8x3_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB8DLOOPKLEFT)                                        \
																\
    label(.SUB8DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB8DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB8DROWSTORED)                                         \
																\
    label(.SUB8DCOLSTORED)                                      \
																\
    STORE_COLSTORED_C_8x3_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    STORE_ROWSTORED_C_8xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB8DROWSTORBZ)                                         \
    label(.SUB8DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_8x3_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    STORE_ROWSTORED_C_8xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 16 mask edge kernel begins */                     \
    label(.EDGE16MASKXN)                                        \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    sub(imm(8), rsi)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
    lea(mem(r11, r9, 4), r15)                                   \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB16MASKDLOOPKITER)                                 \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB16MASKPREFETCHLOOP)                                 \
																\
    label(.SUB16MASKLOOP1)                                      \
																\
        UNROLL_K_LOOP_16MASKx3_L1                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP1)                                        \
																\
    label(.SUB16MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.SUB16MASKTAILITER)                                     \
																\
    label(.SUB16MASKLOOP2)                                      \
																\
        UNROLL_K_LOOP_16MASKx3_L2                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP2)                                        \
    label(.SUB16MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB16MASKTAIL)                                         \
																\
    label(.SUB16MASKLOOP3)                                      \
																\
        UNROLL_K_LOOP_16MASKx3_L1                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP3)                                        \
																\
    vaddpd(zmm12, zmm6, zmm6)                                   \
    vaddpd(zmm13, zmm7, zmm7)                                   \
    vaddpd(zmm14, zmm8, zmm8)                                   \
    vaddpd(zmm15, zmm9, zmm9)                                   \
    vaddpd(zmm16, zmm10, zmm10)                                 \
    vaddpd(zmm17, zmm11, zmm11)                                 \
																\
    label(.SUB16MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB16MASKDPOSTACCUM)                                    \
																\
    label(.SUB16MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )                    \
        add( r10,rax )                                          \
        K_LOOP_16MASKx3_SET1(0, 1)                              \
        dec(rsi)                                                \
    jne(.SUB16MASKDLOOPKLEFT)                                   \
																\
    label(.SUB16MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.SUB16MASKDBETAZERO)                                     \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16MASKDROWSTORED)                                    \
																\
    label(.SUB16MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_16MASKx3_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    STORE_ROWSTORED_C_16MASKxN_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16MASKDROWSTORBZ)                                    \
																\
    label(.SUB16MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_16MASKx3_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    vmulpd( zmm30,zmm11,zmm11 )                                 \
    STORE_ROWSTORED_C_16MASKxN_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 4 edge kernel begins */                           \
    label(.EDGE4XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB4DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB4PREFETCHLOOP)                                      \
    label(.SUB4LOOP1)                                           \
        UNROLL_K_LOOP_4x3_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB4LOOP1)                                             \
																\
    label(.SUB4PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB4TAILITER)                                          \
																\
    label(.SUB4LOOP2)                                           \
																\
        UNROLL_K_LOOP_4x3_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB4LOOP2)                                             \
    label(.SUB4TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB4TAIL)                                              \
																\
    label(.SUB4LOOP3)                                           \
																\
        UNROLL_K_LOOP_4x3_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB4LOOP3)                                             \
																\
    vaddpd(ymm7, ymm6, ymm6)                                    \
    vaddpd(ymm9, ymm8, ymm8)                                    \
    vaddpd(ymm11, ymm10, ymm10)                                 \
																\
    label(.SUB4TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB4DPOSTACCUM)                                         \
																\
    label(.SUB4DLOOPKLEFT)                                      \
        vmovupd( mem(rax),ymm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_4x3_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB4DLOOPKLEFT)                                        \
																\
    label(.SUB4DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB4DBETAZERO)                                          \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB4DROWSTORED)                                         \
																\
    label(.SUB4DCOLSTORED)                                      \
    STORE_COLSTORED_C_4x3_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    STORE_ROWSTORED_C_4xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB4DROWSTORBZ)                                         \
    label(.SUB4DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_4x3_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    STORE_ROWSTORED_C_4xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 2 edge kernel begins */                           \
    label(.EDGE2XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB2DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB2PREFETCHLOOP)                                      \
    label(.SUB2LOOP1)                                           \
        UNROLL_K_LOOP_2x3_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB2LOOP1)                                             \
																\
    label(.SUB2PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB2TAILITER)                                          \
																\
    label(.SUB2LOOP2)                                           \
																\
        UNROLL_K_LOOP_2x3_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB2LOOP2)                                             \
    label(.SUB2TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB2TAIL)                                              \
																\
    label(.SUB2LOOP3)                                           \
        UNROLL_K_LOOP_2x3_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB2LOOP3)                                             \
																\
    vaddpd(xmm7, xmm6, xmm6)                                    \
    vaddpd(xmm9, xmm8, xmm8)                                    \
    vaddpd(xmm11, xmm10, xmm10)                                 \
																\
    label(.SUB2TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB2DPOSTACCUM)                                         \
																\
    label(.SUB2DLOOPKLEFT)                                      \
        vmovupd( mem(rax),xmm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_2x3_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB2DLOOPKLEFT)                                        \
																\
    label(.SUB2DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB2DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB2DROWSTORED)                                         \
																\
    label(.SUB2DCOLSTORED)                                      \
    STORE_COLSTORED_C_2x3_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    STORE_ROWSTORED_C_2xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB2DROWSTORBZ)                                         \
    label(.SUB2DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_2x3_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    STORE_ROWSTORED_C_2xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 1 edge kernel begins */                           \
    label(.EDGE1XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB1TAIL)                                               \
    label(.SUB1LOOP3)                                           \
        UNROLL_K_LOOP_1x3_L1                                    \
        dec(rsi)                                                \
        jnz(.SUB1LOOP3)                                         \
																\
    vaddpd(xmm7, xmm6, xmm6)                                    \
    vaddpd(xmm9, xmm8, xmm8)                                    \
    vaddpd(xmm11, xmm10, xmm10)                                 \
																\
    label(.SUB1TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB1DPOSTACCUM)                                         \
																\
    label(.SUB1DLOOPKLEFT)                                      \
        vmovsd( mem(rax),xmm0 )                                 \
        add( r10,rax )                                          \
        K_LOOP_1x3_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB1DLOOPKLEFT)                                        \
																\
    label(.SUB1DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB1DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB1DROWSTORED)                                         \
																\
    label(.SUB1DCOLSTORED)                                      \
    STORE_COLSTORED_C_1x3_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    STORE_ROWSTORED_C_1xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB1DROWSTORBZ)                                         \
    label(.SUB1DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_1x3_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    STORE_ROWSTORED_C_1xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 8 mask edge kernel begins */                      \
    label(.EDGE8MASKXN)                                         \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.EDGE8MASKDLOOPKITER)                                 \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.EDGE8MASKPREFETCHLOOP)                                 \
    label(.EDGE8MASKLOOP1)                                      \
        UNROLL_K_LOOP_8MASKx3_L1                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP1)                                        \
																\
    label(.EDGE8MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.EDGE8MASKTAILITER)                                     \
																\
    label(.EDGE8MASKLOOP2)                                      \
																\
        UNROLL_K_LOOP_8MASKx3_L2                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP2)                                        \
    label(.EDGE8MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.EDGE8MASKTAIL)                                         \
																\
    label(.EDGE8MASKLOOP3)                                      \
																\
        UNROLL_K_LOOP_8MASKx3_L1                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP3)                                        \
																\
    vaddpd(zmm7, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm8, zmm8)                                    \
    vaddpd(zmm11, zmm10, zmm10)                                 \
																\
    label(.EDGE8MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.EDGE8MASKDPOSTACCUM)                                    \
																\
    label(.EDGE8MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 MASK_KZ(2) )                     \
        add( r10,rax )                                          \
        K_LOOP_8MASKx3_SET1(0)                                  \
        dec(rsi)                                                \
    jne(.EDGE8MASKDLOOPKLEFT)                                   \
																\
    label(.EDGE8MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.EDGE8MASKDBETAZERO)                                     \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.EDGE8MASKDROWSTORED)                                    \
																\
    label(.EDGE8MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_8MASKx3_Bn                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    STORE_ROWSTORED_C_8MASKxN_Bn                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.EDGE8MASKDROWSTORBZ)                                    \
																\
    label(.EDGE8MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_8MASKx3_B0                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(3), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm10,zmm10 )                                 \
    STORE_ROWSTORED_C_8MASKxN_B0                                \
    jmp(.CONCLUDE)


#define M_LEFT_24x2												\
																\
    /* m_left 20 edge kernel begins */                          \
    label(.EDGE20XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB20PREFETCHLOOP)                                     \
    label(.SUB20LOOP1)                                          \
																\
        UNROLL_K_LOOP_20x2_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP1)                                            \
																\
    label(.SUB20PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB20TAILITER)                                         \
																\
    label(.SUB20LOOP2)                                          \
        UNROLL_K_LOOP_20x2_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP2)                                            \
    label(.SUB20TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB20TAIL)                                             \
    label(.SUB20LOOP3)                                          \
        UNROLL_K_LOOP_20x2_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP3)                                            \
																\
    vaddpd(zmm10, zmm6, zmm6)                                   \
    vaddpd(zmm11, zmm7, zmm7)                                   \
    vaddpd(zmm12, zmm28, zmm28)                                 \
    vaddpd(zmm13, zmm8, zmm8)                                   \
    vaddpd(zmm14, zmm9, zmm9)                                   \
    vaddpd(zmm15, zmm29, zmm29)                                 \
																\
    label(.SUB20TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB20DPOSTACCUM)                                        \
    label(.SUB20DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax), ymm2 )                              \
        add( r10,rax )                                          \
        K_LOOP_20x2_SET1(0, 1, 2)                               \
        dec(rsi)                                                \
    jne(.SUB20DLOOPKLEFT)                                       \
																\
    label(.SUB20DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB20DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB20DROWSTORED)                                        \
																\
    label(.SUB20DCOLSTORED)                                     \
    STORE_COLSTORED_C_20x2_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    STORE_ROWSTORED_C_20xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB20DROWSTORBZ)                                        \
																\
    label(.SUB20DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_20x2_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    STORE_ROWSTORED_C_20xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 18 edge kernel begins */                          \
    label(.EDGE18XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB18PREFETCHLOOP)                                     \
    label(.SUB18LOOP1)                                          \
																\
        UNROLL_K_LOOP_18x2_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP1)                                            \
																\
    label(.SUB18PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB18TAILITER)                                         \
																\
    label(.SUB18LOOP2)                                          \
        UNROLL_K_LOOP_18x2_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP2)                                            \
    label(.SUB18TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB18TAIL)                                             \
    label(.SUB18LOOP3)                                          \
        UNROLL_K_LOOP_18x2_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP3)                                            \
																\
																\
    vaddpd(zmm10, zmm6, zmm6)                                   \
    vaddpd(zmm11, zmm7, zmm7)                                   \
    vaddpd(zmm12, zmm28, zmm28)                                 \
    vaddpd(zmm13, zmm8, zmm8)                                   \
    vaddpd(zmm14, zmm9, zmm9)                                   \
    vaddpd(zmm15, zmm29, zmm29)                                 \
																\
    label(.SUB18TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB18DPOSTACCUM)                                        \
																\
    label(.SUB18DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax), xmm2 )                              \
        add( r10,rax )                                          \
        K_LOOP_18x2_SET1(0, 1, 2)                               \
        dec(rsi)                                                \
    jne(.SUB18DLOOPKLEFT)                                       \
																\
    label(.SUB18DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB18DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB18DROWSTORED)                                        \
																\
    label(.SUB18DCOLSTORED)                                     \
    STORE_COLSTORED_C_18x2_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    STORE_ROWSTORED_C_18xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB18DROWSTORBZ)                                        \
																\
    label(.SUB18DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_18x2_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    STORE_ROWSTORED_C_18xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 17 edge kernel begins */                          \
    label(.EDGE17XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB17PREFETCHLOOP)                                     \
    label(.SUB17LOOP1)                                          \
																\
        UNROLL_K_LOOP_17x2_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP1)                                            \
																\
    label(.SUB17PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB17TAILITER)                                         \
																\
    label(.SUB17LOOP2)                                          \
        UNROLL_K_LOOP_17x2_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP2)                                            \
    label(.SUB17TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB17TAIL)                                             \
    label(.SUB17LOOP3)                                          \
        UNROLL_K_LOOP_17x2_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP3)                                            \
																\
    vaddpd(zmm10, zmm6, zmm6)                                   \
    vaddpd(zmm11, zmm7, zmm7)                                   \
    vaddpd(zmm12, zmm28, zmm28)                                 \
    vaddpd(zmm13, zmm8, zmm8)                                   \
    vaddpd(zmm14, zmm9, zmm9)                                   \
    vaddpd(zmm15, zmm29, zmm29)                                 \
																\
    label(.SUB17TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB17DPOSTACCUM)                                        \
    label(.SUB17DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovsd ( 0x80(rax),xmm2 )                               \
        add( r10,rax )                                          \
        K_LOOP_17x2_SET1(0, 1, 2)                               \
        dec(rsi)                                                \
    jne(.SUB17DLOOPKLEFT)                                       \
																\
    label(.SUB17DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB17DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB17DROWSTORED)                                        \
																\
    label(.SUB17DCOLSTORED)                                     \
    STORE_COLSTORED_C_17x2_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    STORE_ROWSTORED_C_17xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB17DROWSTORBZ)                                        \
																\
    label(.SUB17DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_17x2_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    STORE_ROWSTORED_C_17xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 16 edge kernel begins */                          \
    label(.EDGE16XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB16DLOOPKITER)                                     \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB16PREFETCHLOOP)                                     \
    label(.SUB16LOOP1)                                          \
																\
        UNROLL_K_LOOP_16x2_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP1)                                            \
																\
    label(.SUB16PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB16TAILITER)                                         \
																\
    label(.SUB16LOOP2)                                          \
        UNROLL_K_LOOP_16x2_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP2)                                            \
    label(.SUB16TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB16TAIL)                                             \
																\
    label(.SUB16LOOP3)                                          \
																\
        UNROLL_K_LOOP_16x2_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP3)                                            \
																\
    vaddpd(zmm10, zmm6, zmm6)                                   \
    vaddpd(zmm11, zmm7, zmm7)                                   \
    vaddpd(zmm12, zmm8, zmm8)                                   \
    vaddpd(zmm13, zmm9, zmm9)                                   \
																\
    label(.SUB16TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB16DPOSTACCUM)                                        \
																\
    label(.SUB16DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        add( r10,rax )                                          \
																\
        K_LOOP_16x2_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB16DLOOPKLEFT)                                       \
																\
    label(.SUB16DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB16DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB16DROWSTORED)                                        \
																\
    label(.SUB16DCOLSTORED)                                     \
    STORE_COLSTORED_C_16x2_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    STORE_ROWSTORED_C_16xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16DROWSTORBZ)                                        \
																\
    label(.SUB16DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_16x2_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    STORE_ROWSTORED_C_16xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /**********  M_Left 24 masked kernel **************/        \
    label(.EDGE24MASKXN)                                        \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    sub(imm(16), rsi)                                           \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB24MASKPREFETCHLOOP)                                 \
    label(.SUB24MASKLOOP1)                                      \
																\
        UNROLL_K_LOOP_24MASKx2_L1                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP1)                                        \
																\
    label(.SUB24MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.SUB24MASKTAILITER)                                     \
																\
    label(.SUB24MASKLOOP2)                                      \
        UNROLL_K_LOOP_24MASKx2_L2                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP2)                                        \
    label(.SUB24MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB24MASKTAIL)                                         \
    label(.SUB24MASKLOOP3)                                      \
        UNROLL_K_LOOP_24MASKx2_L1                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP3)                                        \
																\
    vaddpd(zmm10, zmm6, zmm6)                                   \
    vaddpd(zmm11, zmm7, zmm7)                                   \
    vaddpd(zmm12, zmm28, zmm28)                                 \
    vaddpd(zmm13, zmm8, zmm8)                                   \
    vaddpd(zmm14, zmm9, zmm9)                                   \
    vaddpd(zmm15, zmm29, zmm29)                                 \
																\
    label(.SUB24MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB24MASKDPOSTACCUM)                                    \
    label(.SUB24MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax),zmm2 MASK_KZ(2) )                    \
        add( r10,rax )                                          \
        K_LOOP_24MASKx2_SET1(0, 1, 2)                           \
        dec(rsi)                                                \
    jne(.SUB24MASKDLOOPKLEFT)                                   \
    label(.SUB24MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.SUB24MASKDBETAZERO)                                     \
    cmp(imm(8), rdi)                                            \
    jz(.SUB24MASKDROWSTORED)                                    \
																\
    label(.SUB24MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_24MASKx2_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    STORE_ROWSTORED_C_24MASKxN_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.SUB24MASKDROWSTORBZ)                                    \
																\
    label(.SUB24MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_24MASKx2_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    STORE_ROWSTORED_C_24MASKxN_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 12 edge kernel begins */                          \
    label(.EDGE12XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB12DLOOPKITER)                                     \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB12PREFETCHLOOP)                                     \
																\
    label(.SUB12LOOP1)                                          \
																\
        UNROLL_K_LOOP_12x2_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB12LOOP1)                                            \
																\
    label(.SUB12PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB12TAILITER)                                         \
																\
    label(.SUB12LOOP2)                                          \
        UNROLL_K_LOOP_12x2_L2                                   \
        sub(imm(1), rsi)                                        \
    jnz(.SUB12LOOP2)                                            \
    label(.SUB12TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB12TAIL)                                             \
																\
    label(.SUB12LOOP3)                                          \
        UNROLL_K_LOOP_12x2_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB12LOOP3)                                            \
																\
    vaddpd(zmm10, zmm6, zmm6)                                   \
    vaddpd(zmm11, zmm7, zmm7)                                   \
    vaddpd(zmm12, zmm8, zmm8)                                   \
    vaddpd(zmm13, zmm9, zmm9)                                   \
																\
    label(.SUB12TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB12DPOSTACCUM)                                        \
																\
    label(.SUB12DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),ymm1 )                               \
        add( r10,rax )                                          \
        K_LOOP_12x2_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB12DLOOPKLEFT)                                       \
																\
    label(.SUB12DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB12DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
    jz(.SUB12DROWSTORED)                                        \
																\
    label(.SUB12DCOLSTORED)                                     \
    STORE_COLSTORED_C_12x2_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    STORE_ROWSTORED_C_12xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
    label(.SUB12DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB12DROWSTORBZ)                                        \
																\
    label(.SUB12DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_12x2_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    STORE_ROWSTORED_C_12xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 11 edge kernel begins */                          \
    label(.EDGE11XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB11PREFETCHLOOP)                                     \
    label(.SUB11LOOP1)                                          \
																\
        UNROLL_K_LOOP_11x2_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP1)                                            \
																\
    label(.SUB11PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB11TAILITER)                                         \
																\
    label(.SUB11LOOP2)                                          \
        UNROLL_K_LOOP_11x2_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP2)                                            \
    label(.SUB11TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB11TAIL)                                             \
    label(.SUB11LOOP3)                                          \
        UNROLL_K_LOOP_11x2_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP3)                                            \
																\
    vaddpd(zmm10, zmm6, zmm6)                                   \
    vaddpd(zmm11, zmm7, zmm7)                                   \
    vaddpd(zmm12, zmm28, zmm28)                                 \
    vaddpd(zmm13, zmm8, zmm8)                                   \
    vaddpd(zmm14, zmm9, zmm9)                                   \
    vaddpd(zmm15, zmm29, zmm29)                                 \
																\
    label(.SUB11TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB11DPOSTACCUM)                                        \
																\
    label(.SUB11DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),xmm1 )                               \
        vmovsd( 0x50(rax),xmm2 )                                \
        add( r10,rax )                                          \
        K_LOOP_11x2_SET1(0, 1, 2)                               \
        dec(rsi)                                                \
    jne(.SUB11DLOOPKLEFT)                                       \
																\
    label(.SUB11DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB11DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB11DROWSTORED)                                        \
																\
    label(.SUB11DCOLSTORED)                                     \
    STORE_COLSTORED_C_11x2_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    STORE_ROWSTORED_C_11xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB11DROWSTORBZ)                                        \
    label(.SUB11DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_11x2_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    vmulpd( zmm30,zmm29,zmm29 )                                 \
    STORE_ROWSTORED_C_11xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 10 edge kernel begins */                          \
    label(.EDGE10XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB10PREFETCHLOOP)                                     \
    label(.SUB10LOOP1)                                          \
																\
        UNROLL_K_LOOP_10x2_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP1)                                            \
																\
    label(.SUB10PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB10TAILITER)                                         \
																\
    label(.SUB10LOOP2)                                          \
        UNROLL_K_LOOP_10x2_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP2)                                            \
    label(.SUB10TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB10TAIL)                                             \
    label(.SUB10LOOP3)                                          \
        UNROLL_K_LOOP_10x2_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP3)                                            \
																\
																\
    vaddpd(zmm10, zmm6, zmm6)                                   \
    vaddpd(zmm11, zmm7, zmm7)                                   \
    vaddpd(zmm12, zmm8, zmm8)                                   \
    vaddpd(zmm13, zmm9, zmm9)                                   \
																\
    label(.SUB10TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB10DPOSTACCUM)                                        \
																\
    label(.SUB10DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),xmm1 )                               \
        add( r10,rax )                                          \
        K_LOOP_10x2_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB10DLOOPKLEFT)                                       \
																\
    label(.SUB10DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB10DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB10DROWSTORED)                                        \
																\
    label(.SUB10DCOLSTORED)                                     \
    STORE_COLSTORED_C_10x2_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    STORE_ROWSTORED_C_10xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB10DROWSTORBZ)                                        \
    label(.SUB10DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_10x2_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    STORE_ROWSTORED_C_10xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 9 edge kernel begins */                           \
    label(.EDGE9XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB9DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB9PREFETCHLOOP)                                      \
																\
    label(.SUB9LOOP1)                                           \
																\
        UNROLL_K_LOOP_9x2_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB9LOOP1)                                             \
																\
    label(.SUB9PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB9TAILITER)                                          \
																\
    label(.SUB9LOOP2)                                           \
        UNROLL_K_LOOP_9x2_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB9LOOP2)                                             \
    label(.SUB9TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB9TAIL)                                              \
																\
    label(.SUB9LOOP3)                                           \
																\
        UNROLL_K_LOOP_9x2_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB9LOOP3)                                             \
																\
    vaddpd(zmm10, zmm6, zmm6)                                   \
    vaddpd(zmm11, zmm7, zmm7)                                   \
    vaddpd(zmm12, zmm8, zmm8)                                   \
    vaddpd(zmm13, zmm9, zmm9)                                   \
																\
    label(.SUB9TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB9DPOSTACCUM)                                         \
																\
    label(.SUB9DLOOPKLEFT)                                      \
        vmovupd( mem(rax),zmm0 )                                \
        vmovsd( 0x40(rax),xmm1 )                                \
        add( r10,rax )                                          \
        K_LOOP_9x2_SET1(0, 1)                                   \
        dec(rsi)                                                \
    jne(.SUB9DLOOPKLEFT)                                        \
																\
    label(.SUB9DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB9DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB9DROWSTORED)                                         \
																\
    label(.SUB9DCOLSTORED)                                      \
    STORE_COLSTORED_C_9x2_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    STORE_ROWSTORED_C_9xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB9DROWSTORBZ)                                         \
    label(.SUB9DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_9x2_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    STORE_ROWSTORED_C_9xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 8 edge kernel begins */                           \
    label(.EDGE8XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB8DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB8PREFETCHLOOP)                                      \
    label(.SUB8LOOP1)                                           \
        UNROLL_K_LOOP_8x2_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB8LOOP1)                                             \
																\
    label(.SUB8PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB8TAILITER)                                          \
																\
    label(.SUB8LOOP2)                                           \
        UNROLL_K_LOOP_8x2_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB8LOOP2)                                             \
    label(.SUB8TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB8TAIL)                                              \
																\
    label(.SUB8LOOP3)                                           \
        UNROLL_K_LOOP_8x2_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB8LOOP3)                                             \
																\
    vaddpd(zmm7, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm8, zmm8)                                    \
																\
    label(.SUB8TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB8DPOSTACCUM)                                         \
																\
    label(.SUB8DLOOPKLEFT)                                      \
        vmovupd( mem(rax),zmm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_8x2_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB8DLOOPKLEFT)                                        \
																\
    label(.SUB8DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB8DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB8DROWSTORED)                                         \
																\
    label(.SUB8DCOLSTORED)                                      \
    STORE_COLSTORED_C_8x2_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    STORE_ROWSTORED_C_8xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB8DROWSTORBZ)                                         \
    label(.SUB8DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_8x2_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    STORE_ROWSTORED_C_8xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 16 mask edge kernel begins */                     \
    label(.EDGE16MASKXN)                                        \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    sub(imm(8), rsi)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
    lea(mem(r11, r9, 4), r15)                                   \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB16MASKDLOOPKITER)                                 \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB16MASKPREFETCHLOOP)                                 \
																\
    label(.SUB16MASKLOOP1)                                      \
																\
        UNROLL_K_LOOP_16MASKx2_L1                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP1)                                        \
																\
    label(.SUB16MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.SUB16MASKTAILITER)                                     \
																\
    label(.SUB16MASKLOOP2)                                      \
																\
        UNROLL_K_LOOP_16MASKx2_L2                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP2)                                        \
    label(.SUB16MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB16MASKTAIL)                                         \
																\
    label(.SUB16MASKLOOP3)                                      \
																\
        UNROLL_K_LOOP_16MASKx2_L1                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP3)                                        \
																\
    vaddpd(zmm10, zmm6, zmm6)                                   \
    vaddpd(zmm11, zmm7, zmm7)                                   \
    vaddpd(zmm12, zmm8, zmm8)                                   \
    vaddpd(zmm13, zmm9, zmm9)                                   \
																\
    label(.SUB16MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB16MASKDPOSTACCUM)                                    \
																\
    label(.SUB16MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )                    \
        add( r10,rax )                                          \
        K_LOOP_16MASKx2_SET1(0, 1)                              \
        dec(rsi)                                                \
    jne(.SUB16MASKDLOOPKLEFT)                                   \
																\
    label(.SUB16MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.SUB16MASKDBETAZERO)                                     \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16MASKDROWSTORED)                                    \
																\
    label(.SUB16MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_16MASKx2_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    STORE_ROWSTORED_C_16MASKxN_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16MASKDROWSTORBZ)                                    \
																\
    label(.SUB16MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_16MASKx2_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    vmulpd( zmm30,zmm9,zmm9 )                                   \
    STORE_ROWSTORED_C_16MASKxN_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 4 edge kernel begins */                           \
    label(.EDGE4XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB4DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB4PREFETCHLOOP)                                      \
    label(.SUB4LOOP1)                                           \
        UNROLL_K_LOOP_4x2_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB4LOOP1)                                             \
																\
    label(.SUB4PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB4TAILITER)                                          \
																\
    label(.SUB4LOOP2)                                           \
																\
        UNROLL_K_LOOP_4x2_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB4LOOP2)                                             \
    label(.SUB4TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB4TAIL)                                              \
																\
    label(.SUB4LOOP3)                                           \
																\
        UNROLL_K_LOOP_4x2_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB4LOOP3)                                             \
																\
    vaddpd(ymm7, ymm6, ymm6)                                    \
    vaddpd(ymm9, ymm8, ymm8)                                    \
																\
    label(.SUB4TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB4DPOSTACCUM)                                         \
																\
    label(.SUB4DLOOPKLEFT)                                      \
        vmovupd( mem(rax),ymm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_4x2_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB4DLOOPKLEFT)                                        \
																\
    label(.SUB4DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB4DBETAZERO)                                          \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB4DROWSTORED)                                         \
																\
    label(.SUB4DCOLSTORED)                                      \
    STORE_COLSTORED_C_4x2_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    STORE_ROWSTORED_C_4xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB4DROWSTORBZ)                                         \
    label(.SUB4DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_4x2_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    STORE_ROWSTORED_C_4xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 2 edge kernel begins */                           \
    label(.EDGE2XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB2DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB2PREFETCHLOOP)                                      \
    label(.SUB2LOOP1)                                           \
        UNROLL_K_LOOP_2x2_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB2LOOP1)                                             \
																\
    label(.SUB2PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB2TAILITER)                                          \
																\
    label(.SUB2LOOP2)                                           \
																\
        UNROLL_K_LOOP_2x2_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB2LOOP2)                                             \
    label(.SUB2TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB2TAIL)                                              \
																\
    label(.SUB2LOOP3)                                           \
        UNROLL_K_LOOP_2x2_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB2LOOP3)                                             \
																\
    vaddpd(xmm7, xmm6, xmm6)                                    \
    vaddpd(xmm9, xmm8, xmm8)                                    \
																\
    label(.SUB2TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB2DPOSTACCUM)                                         \
																\
    label(.SUB2DLOOPKLEFT)                                      \
        vmovupd( mem(rax),xmm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_2x2_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB2DLOOPKLEFT)                                        \
																\
    label(.SUB2DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB2DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB2DROWSTORED)                                         \
																\
    label(.SUB2DCOLSTORED)                                      \
    STORE_COLSTORED_C_2x2_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    STORE_ROWSTORED_C_2xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB2DROWSTORBZ)                                         \
    label(.SUB2DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_2x2_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    STORE_ROWSTORED_C_2xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 1 edge kernel begins */                           \
    label(.EDGE1XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB1TAIL)                                               \
    label(.SUB1LOOP3)                                           \
        UNROLL_K_LOOP_1x2_L1                                    \
        dec(rsi)                                                \
        jnz(.SUB1LOOP3)                                         \
																\
    vaddpd(xmm7, xmm6, xmm6)                                    \
    vaddpd(xmm9, xmm8, xmm8)                                    \
																\
    label(.SUB1TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB1DPOSTACCUM)                                         \
																\
    label(.SUB1DLOOPKLEFT)                                      \
        vmovsd( mem(rax),xmm0 )                                 \
        add( r10,rax )                                          \
        K_LOOP_1x2_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB1DLOOPKLEFT)                                        \
																\
    label(.SUB1DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB1DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB1DROWSTORED)                                         \
																\
    label(.SUB1DCOLSTORED)                                      \
    STORE_COLSTORED_C_1x2_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    STORE_ROWSTORED_C_1xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB1DROWSTORBZ)                                         \
    label(.SUB1DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_1x2_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    STORE_ROWSTORED_C_1xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 8 mask edge kernel begins */                      \
    label(.EDGE8MASKXN)                                         \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.EDGE8MASKDLOOPKITER)                                 \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.EDGE8MASKPREFETCHLOOP)                                 \
    label(.EDGE8MASKLOOP1)                                      \
        UNROLL_K_LOOP_8MASKx2_L1                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP1)                                        \
																\
    label(.EDGE8MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.EDGE8MASKTAILITER)                                     \
																\
    label(.EDGE8MASKLOOP2)                                      \
																\
        UNROLL_K_LOOP_8MASKx2_L2                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP2)                                        \
    label(.EDGE8MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.EDGE8MASKTAIL)                                         \
																\
    label(.EDGE8MASKLOOP3)                                      \
																\
        UNROLL_K_LOOP_8MASKx2_L1                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP3)                                        \
																\
    vaddpd(zmm7, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm8, zmm8)                                    \
																\
    label(.EDGE8MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.EDGE8MASKDPOSTACCUM)                                    \
																\
    label(.EDGE8MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 MASK_KZ(2) )                     \
        add( r10,rax )                                          \
        K_LOOP_8MASKx2_SET1(0)                                  \
        dec(rsi)                                                \
    jne(.EDGE8MASKDLOOPKLEFT)                                   \
																\
    label(.EDGE8MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.EDGE8MASKDBETAZERO)                                     \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.EDGE8MASKDROWSTORED)                                    \
																\
    label(.EDGE8MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_8MASKx2_Bn                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    STORE_ROWSTORED_C_8MASKxN_Bn                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.EDGE8MASKDROWSTORBZ)                                    \
																\
    label(.EDGE8MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_8MASKx2_B0                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(2), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm8,zmm8 )                                   \
    STORE_ROWSTORED_C_8MASKxN_B0                                \
    jmp(.CONCLUDE)


#define M_LEFT_24x1												\
																\
    /* m_left 20 edge kernel begins */                          \
    label(.EDGE20XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB20PREFETCHLOOP)                                     \
    label(.SUB20LOOP1)                                          \
																\
        UNROLL_K_LOOP_20x1_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP1)                                            \
																\
    label(.SUB20PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB20TAILITER)                                         \
																\
    label(.SUB20LOOP2)                                          \
        UNROLL_K_LOOP_20x1_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP2)                                            \
    label(.SUB20TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB20TAIL)                                             \
    label(.SUB20LOOP3)                                          \
        UNROLL_K_LOOP_20x1_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB20LOOP3)                                            \
																\
    vaddpd(zmm8, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm7, zmm7)                                    \
    vaddpd(zmm29, zmm28, zmm28)                                 \
																\
    label(.SUB20TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB20DPOSTACCUM)                                        \
    label(.SUB20DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax), ymm2 )                              \
        add( r10,rax )                                          \
        K_LOOP_20x1_SET1(0, 1, 2)                               \
        dec(rsi)                                                \
    jne(.SUB20DLOOPKLEFT)                                       \
																\
    label(.SUB20DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB20DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB20DROWSTORED)                                        \
																\
    label(.SUB20DCOLSTORED)                                     \
    STORE_COLSTORED_C_20x1_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    STORE_ROWSTORED_C_20xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB20DROWSTORBZ)                                        \
																\
    label(.SUB20DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_20x1_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB20DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    STORE_ROWSTORED_C_20xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 18 edge kernel begins */                          \
    label(.EDGE18XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB18PREFETCHLOOP)                                     \
    label(.SUB18LOOP1)                                          \
																\
        UNROLL_K_LOOP_18x1_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP1)                                            \
																\
    label(.SUB18PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB18TAILITER)                                         \
																\
    label(.SUB18LOOP2)                                          \
        UNROLL_K_LOOP_18x1_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP2)                                            \
    label(.SUB18TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB18TAIL)                                             \
    label(.SUB18LOOP3)                                          \
        UNROLL_K_LOOP_18x1_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB18LOOP3)                                            \
																\
																\
    vaddpd(zmm8, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm7, zmm7)                                    \
    vaddpd(zmm29, zmm28, zmm28)                                 \
																\
    label(.SUB18TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB18DPOSTACCUM)                                        \
																\
    label(.SUB18DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax), xmm2 )                              \
        add( r10,rax )                                          \
        K_LOOP_18x1_SET1(0, 1, 2)                               \
        dec(rsi)                                                \
    jne(.SUB18DLOOPKLEFT)                                       \
																\
    label(.SUB18DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB18DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB18DROWSTORED)                                        \
																\
    label(.SUB18DCOLSTORED)                                     \
    STORE_COLSTORED_C_18x1_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    STORE_ROWSTORED_C_18xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB18DROWSTORBZ)                                        \
																\
    label(.SUB18DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_18x1_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB18DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    STORE_ROWSTORED_C_18xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 17 edge kernel begins */                          \
    label(.EDGE17XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB17PREFETCHLOOP)                                     \
    label(.SUB17LOOP1)                                          \
																\
        UNROLL_K_LOOP_17x1_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP1)                                            \
																\
    label(.SUB17PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB17TAILITER)                                         \
																\
    label(.SUB17LOOP2)                                          \
        UNROLL_K_LOOP_17x1_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP2)                                            \
    label(.SUB17TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB17TAIL)                                             \
    label(.SUB17LOOP3)                                          \
        UNROLL_K_LOOP_17x1_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB17LOOP3)                                            \
																\
    vaddpd(zmm8, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm7, zmm7)                                    \
    vaddpd(zmm29, zmm28, zmm28)                                 \
																\
    label(.SUB17TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB17DPOSTACCUM)                                        \
    label(.SUB17DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovsd ( 0x80(rax),xmm2 )                               \
        add( r10,rax )                                          \
        K_LOOP_17x1_SET1(0, 1, 2)                               \
        dec(rsi)                                                \
    jne(.SUB17DLOOPKLEFT)                                       \
																\
    label(.SUB17DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB17DBETAZERO)                                         \
    cmp(imm(8), rdi)                                            \
    jz(.SUB17DROWSTORED)                                        \
																\
    label(.SUB17DCOLSTORED)                                     \
    STORE_COLSTORED_C_17x1_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    STORE_ROWSTORED_C_17xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB17DROWSTORBZ)                                        \
																\
    label(.SUB17DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_17x1_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB17DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    STORE_ROWSTORED_C_17xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 16 edge kernel begins */                          \
    label(.EDGE16XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB16DLOOPKITER)                                     \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB16PREFETCHLOOP)                                     \
    label(.SUB16LOOP1)                                          \
																\
        UNROLL_K_LOOP_16x1_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP1)                                            \
																\
    label(.SUB16PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB16TAILITER)                                         \
																\
    label(.SUB16LOOP2)                                          \
        UNROLL_K_LOOP_16x1_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP2)                                            \
    label(.SUB16TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB16TAIL)                                             \
																\
    label(.SUB16LOOP3)                                          \
																\
        UNROLL_K_LOOP_16x1_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB16LOOP3)                                            \
																\
																\
    vaddpd(zmm8, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm7, zmm7)                                    \
																\
    label(.SUB16TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB16DPOSTACCUM)                                        \
																\
    label(.SUB16DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        add( r10,rax )                                          \
																\
        K_LOOP_16x1_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB16DLOOPKLEFT)                                       \
																\
    label(.SUB16DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB16DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB16DROWSTORED)                                        \
																\
    label(.SUB16DCOLSTORED)                                     \
    STORE_COLSTORED_C_16x1_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    STORE_ROWSTORED_C_16xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16DROWSTORBZ)                                        \
																\
    label(.SUB16DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_16x1_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    STORE_ROWSTORED_C_16xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /**********  M_Left 24 masked kernel **************/        \
    label(.EDGE24MASKXN)                                        \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    sub(imm(16), rsi)                                           \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB24MASKPREFETCHLOOP)                                 \
    label(.SUB24MASKLOOP1)                                      \
																\
        UNROLL_K_LOOP_24MASKx1_L1                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP1)                                        \
																\
    label(.SUB24MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.SUB24MASKTAILITER)                                     \
																\
    label(.SUB24MASKLOOP2)                                      \
        UNROLL_K_LOOP_24MASKx1_L2                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP2)                                        \
    label(.SUB24MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB24MASKTAIL)                                         \
    label(.SUB24MASKLOOP3)                                      \
        UNROLL_K_LOOP_24MASKx1_L1                               \
        dec(rsi)                                                \
    jnz(.SUB24MASKLOOP3)                                        \
																\
    vaddpd(zmm8, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm7, zmm7)                                    \
    vaddpd(zmm29, zmm28, zmm28)                                 \
																\
    label(.SUB24MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB24MASKDPOSTACCUM)                                    \
    label(.SUB24MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 )                               \
        vmovupd( 0x80(rax),zmm2 MASK_KZ(2) )                    \
        add( r10,rax )                                          \
        K_LOOP_24MASKx1_SET1(0, 1, 2)                           \
        dec(rsi)                                                \
    jne(.SUB24MASKDLOOPKLEFT)                                   \
    label(.SUB24MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.SUB24MASKDBETAZERO)                                     \
    cmp(imm(8), rdi)                                            \
    jz(.SUB24MASKDROWSTORED)                                    \
																\
    label(.SUB24MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_24MASKx1_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    STORE_ROWSTORED_C_24MASKxN_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.SUB24MASKDROWSTORBZ)                                    \
																\
    label(.SUB24MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_24MASKx1_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB24MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    STORE_ROWSTORED_C_24MASKxN_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 12 edge kernel begins */                          \
    label(.EDGE12XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB12DLOOPKITER)                                     \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB12PREFETCHLOOP)                                     \
																\
    label(.SUB12LOOP1)                                          \
																\
        UNROLL_K_LOOP_12x1_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB12LOOP1)                                            \
																\
    label(.SUB12PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB12TAILITER)                                         \
																\
    label(.SUB12LOOP2)                                          \
        UNROLL_K_LOOP_12x1_L2                                   \
        sub(imm(1), rsi)                                        \
    jnz(.SUB12LOOP2)                                            \
    label(.SUB12TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB12TAIL)                                             \
																\
    label(.SUB12LOOP3)                                          \
        UNROLL_K_LOOP_12x1_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB12LOOP3)                                            \
																\
    vaddpd(zmm8, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm7, zmm7)                                    \
																\
    label(.SUB12TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB12DPOSTACCUM)                                        \
																\
    label(.SUB12DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),ymm1 )                               \
        add( r10,rax )                                          \
        K_LOOP_12x1_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB12DLOOPKLEFT)                                       \
																\
    label(.SUB12DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB12DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
    jz(.SUB12DROWSTORED)                                        \
																\
    label(.SUB12DCOLSTORED)                                     \
    STORE_COLSTORED_C_12x1_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    STORE_ROWSTORED_C_12xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB12DROWSTORBZ)                                        \
																\
    label(.SUB12DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_12x1_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB12DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    STORE_ROWSTORED_C_12xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 11 edge kernel begins */                          \
    label(.EDGE11XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB11PREFETCHLOOP)                                     \
    label(.SUB11LOOP1)                                          \
																\
        UNROLL_K_LOOP_11x1_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP1)                                            \
																\
    label(.SUB11PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB11TAILITER)                                         \
																\
    label(.SUB11LOOP2)                                          \
        UNROLL_K_LOOP_11x1_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP2)                                            \
    label(.SUB11TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB11TAIL)                                             \
    label(.SUB11LOOP3)                                          \
        UNROLL_K_LOOP_11x1_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB11LOOP3)                                            \
																\
    vaddpd(zmm8, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm7, zmm7)                                    \
    vaddpd(zmm29, zmm28, zmm28)                                 \
																\
    label(.SUB11TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB11DPOSTACCUM)                                        \
																\
    label(.SUB11DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),xmm1 )                               \
        vmovsd( 0x50(rax),xmm2 )                                \
        add( r10,rax )                                          \
        K_LOOP_11x1_SET1(0, 1, 2)                               \
        dec(rsi)                                                \
    jne(.SUB11DLOOPKLEFT)                                       \
																\
    label(.SUB11DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB11DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB11DROWSTORED)                                        \
																\
    label(.SUB11DCOLSTORED)                                     \
    STORE_COLSTORED_C_11x1_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    STORE_ROWSTORED_C_11xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB11DROWSTORBZ)                                        \
																\
    label(.SUB11DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_11x1_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB11DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    vmulpd( zmm30,zmm28,zmm28 )                                 \
    STORE_ROWSTORED_C_11xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 10 edge kernel begins */                          \
    label(.EDGE10XN)                                            \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB10PREFETCHLOOP)                                     \
    label(.SUB10LOOP1)                                          \
																\
        UNROLL_K_LOOP_10x1_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP1)                                            \
																\
    label(.SUB10PREFETCHLOOP)                                   \
    add(imm(8), rsi)                                            \
    jle(.SUB10TAILITER)                                         \
																\
    label(.SUB10LOOP2)                                          \
        UNROLL_K_LOOP_10x1_L2                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP2)                                            \
    label(.SUB10TAILITER)                                       \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB10TAIL)                                             \
    label(.SUB10LOOP3)                                          \
        UNROLL_K_LOOP_10x1_L1                                   \
        dec(rsi)                                                \
    jnz(.SUB10LOOP3)                                            \
																\
    vaddpd(zmm8, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm7, zmm7)                                    \
																\
    label(.SUB10TAIL)                                           \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB10DPOSTACCUM)                                        \
																\
    label(.SUB10DLOOPKLEFT)                                     \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),xmm1 )                               \
        add( r10,rax )                                          \
        K_LOOP_10x1_SET1(0, 1)                                  \
        dec(rsi)                                                \
    jne(.SUB10DLOOPKLEFT)                                       \
																\
    label(.SUB10DPOSTACCUM)                                     \
    POST_ACCUMULATION                                           \
    je(.SUB10DBETAZERO)                                         \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB10DROWSTORED)                                        \
																\
    label(.SUB10DCOLSTORED)                                     \
    STORE_COLSTORED_C_10x1_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DROWSTORED)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    STORE_ROWSTORED_C_10xN_Bn                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DBETAZERO)                                      \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB10DROWSTORBZ)                                        \
																\
    label(.SUB10DCOLSTORBZ)                                     \
    STORE_COLSTORED_C_10x1_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB10DROWSTORBZ)                                     \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    STORE_ROWSTORED_C_10xN_B0                                   \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 9 edge kernel begins */                           \
    label(.EDGE9XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB9DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB9PREFETCHLOOP)                                      \
																\
    label(.SUB9LOOP1)                                           \
																\
        UNROLL_K_LOOP_9x1_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB9LOOP1)                                             \
																\
    label(.SUB9PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB9TAILITER)                                          \
																\
    label(.SUB9LOOP2)                                           \
        UNROLL_K_LOOP_9x1_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB9LOOP2)                                             \
    label(.SUB9TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB9TAIL)                                              \
																\
    label(.SUB9LOOP3)                                           \
																\
        UNROLL_K_LOOP_9x1_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB9LOOP3)                                             \
																\
    vaddpd(zmm8, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm7, zmm7)                                    \
																\
    label(.SUB9TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB9DPOSTACCUM)                                         \
																\
    label(.SUB9DLOOPKLEFT)                                      \
        vmovupd( mem(rax),zmm0 )                                \
        vmovsd( 0x40(rax),xmm1 )                                \
        add( r10,rax )                                          \
        K_LOOP_9x1_SET1(0, 1)                                   \
        dec(rsi)                                                \
    jne(.SUB9DLOOPKLEFT)                                        \
																\
    label(.SUB9DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB9DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB9DROWSTORED)                                         \
																\
    label(.SUB9DCOLSTORED)                                      \
    STORE_COLSTORED_C_9x1_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    STORE_ROWSTORED_C_9xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB9DROWSTORBZ)                                         \
																\
    label(.SUB9DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_9x1_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB9DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    STORE_ROWSTORED_C_9xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 8 edge kernel begins */                           \
    label(.EDGE8XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB8DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB8PREFETCHLOOP)                                      \
    label(.SUB8LOOP1)                                           \
        UNROLL_K_LOOP_8x1_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB8LOOP1)                                             \
																\
    label(.SUB8PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB8TAILITER)                                          \
																\
    label(.SUB8LOOP2)                                           \
        UNROLL_K_LOOP_8x1_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB8LOOP2)                                             \
    label(.SUB8TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB8TAIL)                                              \
																\
    label(.SUB8LOOP3)                                           \
        UNROLL_K_LOOP_8x1_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB8LOOP3)                                             \
																\
    vaddpd(zmm7, zmm6, zmm6)                                    \
																\
    label(.SUB8TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB8DPOSTACCUM)                                         \
																\
    label(.SUB8DLOOPKLEFT)                                      \
        vmovupd( mem(rax),zmm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_8x1_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB8DLOOPKLEFT)                                        \
																\
    label(.SUB8DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB8DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB8DROWSTORED)                                         \
																\
    label(.SUB8DCOLSTORED)                                      \
																\
    STORE_COLSTORED_C_8x1_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    STORE_ROWSTORED_C_8xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB8DROWSTORBZ)                                         \
    label(.SUB8DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_8x1_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB8DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    STORE_ROWSTORED_C_8xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 16 mask edge kernel begins */                     \
    label(.EDGE16MASKXN)                                        \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    sub(imm(8), rsi)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
    lea(mem(r11, r9, 4), r15)                                   \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB16MASKDLOOPKITER)                                 \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB16MASKPREFETCHLOOP)                                 \
																\
    label(.SUB16MASKLOOP1)                                      \
																\
        UNROLL_K_LOOP_16MASKx1_L1                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP1)                                        \
																\
    label(.SUB16MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.SUB16MASKTAILITER)                                     \
																\
    label(.SUB16MASKLOOP2)                                      \
																\
        UNROLL_K_LOOP_16MASKx1_L2                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP2)                                        \
    label(.SUB16MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB16MASKTAIL)                                         \
																\
    label(.SUB16MASKLOOP3)                                      \
																\
        UNROLL_K_LOOP_16MASKx1_L1                               \
        dec(rsi)                                                \
    jnz(.SUB16MASKLOOP3)                                        \
																\
    vaddpd(zmm8, zmm6, zmm6)                                    \
    vaddpd(zmm9, zmm7, zmm7)                                    \
																\
    label(.SUB16MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB16MASKDPOSTACCUM)                                    \
																\
    label(.SUB16MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 )                                \
        vmovupd( 0x40(rax),zmm1 MASK_KZ(2) )                    \
        add( r10,rax )                                          \
        K_LOOP_16MASKx1_SET1(0, 1)                              \
        dec(rsi)                                                \
    jne(.SUB16MASKDLOOPKLEFT)                                   \
																\
    label(.SUB16MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.SUB16MASKDBETAZERO)                                     \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16MASKDROWSTORED)                                    \
																\
    label(.SUB16MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_16MASKx1_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    STORE_ROWSTORED_C_16MASKxN_Bn                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.SUB16MASKDROWSTORBZ)                                    \
																\
    label(.SUB16MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_16MASKx1_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB16MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    vmulpd( zmm30,zmm7,zmm7 )                                   \
    STORE_ROWSTORED_C_16MASKxN_B0                               \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 4 edge kernel begins */                           \
    label(.EDGE4XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB4DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB4PREFETCHLOOP)                                      \
    label(.SUB4LOOP1)                                           \
        UNROLL_K_LOOP_4x1_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB4LOOP1)                                             \
																\
    label(.SUB4PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB4TAILITER)                                          \
																\
    label(.SUB4LOOP2)                                           \
																\
        UNROLL_K_LOOP_4x1_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB4LOOP2)                                             \
    label(.SUB4TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB4TAIL)                                              \
																\
    label(.SUB4LOOP3)                                           \
																\
        UNROLL_K_LOOP_4x1_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB4LOOP3)                                             \
																\
    vaddpd(ymm7, ymm6, ymm6)                                    \
																\
    label(.SUB4TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB4DPOSTACCUM)                                         \
																\
    label(.SUB4DLOOPKLEFT)                                      \
        vmovupd( mem(rax),ymm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_4x1_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB4DLOOPKLEFT)                                        \
																\
    label(.SUB4DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB4DBETAZERO)                                          \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB4DROWSTORED)                                         \
																\
    label(.SUB4DCOLSTORED)                                      \
    STORE_COLSTORED_C_4x1_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    STORE_ROWSTORED_C_4xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB4DROWSTORBZ)                                         \
																\
    label(.SUB4DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_4x1_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB4DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    STORE_ROWSTORED_C_4xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 2 edge kernel begins */                           \
    label(.EDGE2XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.SUB2DLOOPKITER)                                      \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.SUB2PREFETCHLOOP)                                      \
    label(.SUB2LOOP1)                                           \
        UNROLL_K_LOOP_2x1_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB2LOOP1)                                             \
																\
    label(.SUB2PREFETCHLOOP)                                    \
    add(imm(8), rsi)                                            \
    jle(.SUB2TAILITER)                                          \
																\
    label(.SUB2LOOP2)                                           \
																\
        UNROLL_K_LOOP_2x1_L2                                    \
        sub(imm(1), rsi)                                        \
    jnz(.SUB2LOOP2)                                             \
    label(.SUB2TAILITER)                                        \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.SUB2TAIL)                                              \
																\
    label(.SUB2LOOP3)                                           \
        UNROLL_K_LOOP_2x1_L1                                    \
        dec(rsi)                                                \
    jnz(.SUB2LOOP3)                                             \
																\
    vaddpd(xmm7, xmm6, xmm6)                                    \
																\
    label(.SUB2TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB2DPOSTACCUM)                                         \
																\
    label(.SUB2DLOOPKLEFT)                                      \
        vmovupd( mem(rax),xmm0 )                                \
        add( r10,rax )                                          \
        K_LOOP_2x1_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB2DLOOPKLEFT)                                        \
																\
    label(.SUB2DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB2DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB2DROWSTORED)                                         \
																\
    label(.SUB2DCOLSTORED)                                      \
    STORE_COLSTORED_C_2x1_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    STORE_ROWSTORED_C_2xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB2DROWSTORBZ)                                         \
																\
    label(.SUB2DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_2x1_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB2DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    mov(var(alpha), rdx) /* load address of alpha */            \
    vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */         \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    STORE_ROWSTORED_C_2xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 1 edge kernel begins */                           \
    label(.EDGE1XN)                                             \
    mov(r15, rax)                                               \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    mov(var(k_iter), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB1TAIL)                                               \
    label(.SUB1LOOP3)                                           \
        UNROLL_K_LOOP_1x1_L1                                    \
        dec(rsi)                                                \
        jnz(.SUB1LOOP3)                                         \
																\
    vaddpd(xmm7, xmm6, xmm6)                                    \
																\
    label(.SUB1TAIL)                                            \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.SUB1DPOSTACCUM)                                         \
																\
    label(.SUB1DLOOPKLEFT)                                      \
        vmovsd( mem(rax),xmm0 )                                 \
        add( r10,rax )                                          \
        K_LOOP_1x1_SET1(0)                                      \
        dec(rsi)                                                \
    jne(.SUB1DLOOPKLEFT)                                        \
																\
    label(.SUB1DPOSTACCUM)                                      \
    POST_ACCUMULATION                                           \
    je(.SUB1DBETAZERO)                                          \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB1DROWSTORED)                                         \
																\
    label(.SUB1DCOLSTORED)                                      \
    STORE_COLSTORED_C_1x1_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DROWSTORED)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    /* load address of alpha */                                 \
    mov(var(alpha), rdx)                                        \
    /* broadcast alpha */                                       \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    STORE_ROWSTORED_C_1xN_Bn                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DBETAZERO)                                       \
    cmp(imm(8), rdi)                                            \
																\
    jz(.SUB1DROWSTORBZ)                                         \
																\
    label(.SUB1DCOLSTORBZ)                                      \
    STORE_COLSTORED_C_1x1_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    label(.SUB1DROWSTORBZ)                                      \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    /* load address of alpha */                                 \
    mov(var(alpha), rdx)                                        \
    /* broadcast alpha */                                       \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    STORE_ROWSTORED_C_1xN_B0                                    \
    jmp(.CONCLUDE)                                              \
																\
    /* m_left 8 mask edge kernel begins */                      \
    label(.EDGE8MASKXN)                                         \
    mov(r15, rax)                                               \
    mov(imm(1), rdx)                                            \
    shlx(rsi, rdx, rdx)                                         \
    sub(imm(1), rdx)                                            \
    kmovw(edx, k(2))                                            \
    PRE_K_LOOP                                                  \
    PREPARE_SCRATCHPAD                                          \
																\
	/* r11 = rbx + 8*rs_b(B for prefetching) */                 \
    lea(mem(rbx, r8, 8, 7*8), r11)                              \
																\
    label(.EDGE8MASKDLOOPKITER)                                 \
    mov(var(k_iter), rsi)                                       \
    sub(imm( 8+TAIL_NITER), rsi)                                \
    jle(.EDGE8MASKPREFETCHLOOP)                                 \
    label(.EDGE8MASKLOOP1)                                      \
        UNROLL_K_LOOP_8MASKx1_L1                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP1)                                        \
																\
    label(.EDGE8MASKPREFETCHLOOP)                               \
    add(imm(8), rsi)                                            \
    jle(.EDGE8MASKTAILITER)                                     \
																\
    label(.EDGE8MASKLOOP2)                                      \
																\
        UNROLL_K_LOOP_8MASKx1_L2                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP2)                                        \
    label(.EDGE8MASKTAILITER)                                   \
    add(imm(TAIL_NITER), rsi)                                   \
    jle(.EDGE8MASKTAIL)                                         \
																\
    label(.EDGE8MASKLOOP3)                                      \
																\
        UNROLL_K_LOOP_8MASKx1_L1                                \
        dec(rsi)                                                \
    jnz(.EDGE8MASKLOOP3)                                        \
																\
    vaddpd(zmm7, zmm6, zmm6)                                    \
																\
    label(.EDGE8MASKTAIL)                                       \
    mov(var(k_left), rsi)                                       \
    test(rsi, rsi)                                              \
    je(.EDGE8MASKDPOSTACCUM)                                    \
																\
    label(.EDGE8MASKDLOOPKLEFT)                                 \
        vmovupd( mem(rax),zmm0 MASK_KZ(2) )                     \
        add( r10,rax )                                          \
        K_LOOP_8MASKx1_SET1(0)                                  \
        dec(rsi)                                                \
    jne(.EDGE8MASKDLOOPKLEFT)                                   \
																\
    label(.EDGE8MASKDPOSTACCUM)                                 \
    POST_ACCUMULATION                                           \
    je(.EDGE8MASKDBETAZERO)                                     \
																\
    cmp(imm(8), rdi)                                            \
																\
    jz(.EDGE8MASKDROWSTORED)                                    \
																\
    label(.EDGE8MASKDCOLSTORED)                                 \
    STORE_COLSTORED_C_8MASKx1_Bn                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDROWSTORED)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    /* load address of alpha */                                 \
    mov(var(alpha), rdx)                                        \
    /* broadcast alpha */                                       \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    STORE_ROWSTORED_C_8MASKxN_Bn                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDBETAZERO)                                  \
    cmp(imm(8), rdi)                                            \
    jz(.EDGE8MASKDROWSTORBZ)                                    \
																\
    label(.EDGE8MASKDCOLSTORBZ)                                 \
    STORE_COLSTORED_C_8MASKx1_B0                                \
    jmp(.CONCLUDE)                                              \
																\
    label(.EDGE8MASKDROWSTORBZ)                                 \
    mov(imm(1), rbx)                                            \
    mov(imm(1), r9)                                             \
    shlx(r9, rbx, rbx)                                          \
    sub(imm(1), rbx)                                            \
    kmovw(ebx, k(3))                                            \
    /* load address of alpha */                                 \
    mov(var(alpha), rdx)                                        \
    /* broadcast alpha */                                       \
    vbroadcastsd(mem(rdx), zmm30)                               \
    vmulpd( zmm30,zmm6,zmm6 )                                   \
    STORE_ROWSTORED_C_8MASKxN_B0                                \
    jmp(.CONCLUDE)


/**
 * @brief bli_dgemmsup_cv_zen4_asm_24x1m_new Kernel
 * This kernel performs a double-precision general matrix-matrix multiplication
 * (DGEMM) operation. It processes matrices A, B, and C, where A is MxK, B is KxN, and C is MxN. 
 * 
 * this kernel computes MX1, where M could be the original
 * m input or an MC block, depending on the framework calls. The computation
 * is facilitated by several helper macros:
 * - PREPARE_M_LOOP: [IR Loop] For one iteration of this loop, a block of MRx1 is computed
 * - PRE_K_LOOP: loads all the necessary parameters and matrix offsets needed for computing GEMM
 * - PREPARE_SCRATCHPAD: clear out vector registers zmm6-29
 * 
 * The main computation of A*B is performed by the following macros:
 * - UNROLL_K_LOOP_24x8_L1: Unrolls the K loop by 8, storing results
 * in vector registers zmm6-29.
 * - UNROLL_K_LOOP_24x8_L2: Unrolls the K loop by 8, storing results
 * in vector registers zmm6-29.
 * 
 * After the computation, the kernel checks the storage type of matrix C and
 * uses the appropriate macros to store the results:
 * - STORE_COLSTORED_C_24x8_Bn/0: For column-stored C matrix.
 * - STORE_ROWSTORED_C_24x8_Bn/0: For row-stored C matrix.
 */
void bli_dgemmsup_cv_zen4_asm_24x1m_new
(
       conj_t    conja,
       conj_t    conjb,
       dim_t     m0,
       dim_t     n0,
       dim_t     k0,
       double*    restrict alpha,
#if   BLIS_INT_TYPE_SIZE == 32
       double*    restrict a, inc_t rs_a, inc_t cs_a,
       double*    restrict b, inc_t rs_b, inc_t cs_b,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c, inc_t cs_c,
#else
       double*    restrict a, inc_t rs_a0, inc_t cs_a0,
       double*    restrict b, inc_t rs_b0, inc_t cs_b0,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c0, inc_t cs_c0,
#endif
       auxinfo_t* restrict data,
       cntx_t*    restrict cntx
     )
{
    AOCL_DTL_TRACE_ENTRY(AOCL_DTL_LEVEL_TRACE_7);

    uint64_t m_iter = (uint64_t)m0 / 24;
    uint64_t m_left = (uint64_t)m0 % 24;
    uint64_t k_iter = (uint64_t)k0 / 8;
    uint64_t k_left = (uint64_t)k0 % 8;
    uint64_t ps_a = bli_auxinfo_ps_a( data );
    uint64_t ps_a8  = ps_a * sizeof( double );

#if   BLIS_INT_TYPE_SIZE == 32
    uint64_t rs_a0   = (uint64_t)rs_a;
    uint64_t cs_a0   = (uint64_t)cs_a;
    uint64_t rs_b0   = (uint64_t)rs_b;
    uint64_t cs_b0   = (uint64_t)cs_b;
    uint64_t rs_c0   = (uint64_t)rs_c;
    uint64_t cs_c0   = (uint64_t)cs_c;
#endif

    {
        begin_asm()
        /**
         * @brief [IR Loop] For one iteration of this loop, a block of MRxNR is computed
         * This loop moves along m-dimension of c matrix with steps of MR*rs_c,
         * and moves along m-dimension in a matrix with steps of MR*rs_a.
         */
        PREPARE_M_LOOP
        PRE_K_LOOP

		/* panel stride of A */
		mov(var(ps_a8), r14)
        /**
         * Prefetch the next panel of matrix A into the cache.
         * This helps in reducing cache misses and improving performance.
         *
         * lea(mem(rax, r14, 1, 7*8), r14)
         * - Load the effective address of the next panel of A into r14.
         * - rax: Base address of A.
         * - r14: Offset to the current panel of A.
         * - 1: Scale factor (no scaling in this case).
         * - 7*8: Offset to the next panel (assuming each panel is 8 bytes and we are moving 7 panels ahead).
         * - r11 = b + 8 * rs_b + 7*8(r11 points to next NR + offset(7*8))
         *
         * lea(mem(rbx, r8, 8, 7*8), r11)
         * - Calculate the address for prefetching B into r11.
         * - rbx: Base address of B.
         * - r8: Row stride of B (rs_b).
         * - 8: Scale factor (assuming 8 bytes per element).
         * - 7*8: Offset to the next set of elements for prefetching.
         * - r15 = r11(next NR + offset of b matrix) + 4 * cs_b
         */
		lea(mem(rax, r14, 1, 7*8), r14)
		lea(mem(rbx, r8, 8, 7*8), r11)

        /**
         * @brief Clear ZMM vector registers 6 to 28, which were used to store the results of Alpha * A * B.
         *
         * This function initializes zmm6 to zero using the vxorpd instruction and then copies
         * the zeroed value from zmm6 to the other ZMM registers (zmm7 to zmm28) using vmovapd instructions.
         * zmm0, 1, 2 and zmm3, 4, 5 are used to load 24 elements of a matrix of respective columns.
         * zmm30, zmm31 are used to broadcast elements of b matrix of resepctive rows.
         */
        PREPARE_SCRATCHPAD

        label(.DLOOPKITER)
        mov(var(k_iter), rsi)
        sub(imm( 8+TAIL_NITER), rsi)
        jle(.PREFETCHLOOP)
        label(.LOOP1)
            UNROLL_K_LOOP_24x1_L1
            dec(rsi)
        jnz(.LOOP1)

        label(.PREFETCHLOOP)
        add(imm(8), rsi)
        jle(.TAILITER)

        label(.LOOP2)
            UNROLL_K_LOOP_24x1_L2
            dec(rsi)
        jnz(.LOOP2)
        label(.TAILITER)
        add(imm(TAIL_NITER), rsi)
        jle(.TAIL)
        label(.LOOP3)
            UNROLL_K_LOOP_24x1_L1
            dec(rsi)
        jnz(.LOOP3)

        vaddpd(zmm8, zmm6, zmm6)
        vaddpd(zmm9, zmm7, zmm7)
        vaddpd(zmm29, zmm28, zmm28)

        label(.TAIL)
        mov(var(k_left), rsi)
        test(rsi, rsi)
        je(.DPOSTACCUM)
        label(.DLOOPKLEFT)
            vmovupd( mem(rax), zmm0 )
            vmovupd( 0x40(rax), zmm1 )
            vmovupd( 0x80(rax), zmm2 )
            /* a += cs_a */
            add( r10, rax )
            K_LOOP_24x1_SET1(0, 1, 2)
            dec(rsi)
        jne(.DLOOPKLEFT)

        label(.DPOSTACCUM)
        POST_ACCUMULATION
        /* if ZF == 1, jump to beta == 0 case */
        je(.DBETAZERO)
        /* set ZF if (8*cs_c) == 8 */
        cmp(imm(8), rdi)
        jz(.DROWSTORED)

        label(.DCOLSTORED)
        STORE_COLSTORED_C_24x1_Bn
        jmp(.DDONE)

        label(.DROWSTORED)
		mov(imm(1), rbx)
		mov(imm(1), r9)
		shlx(r9, rbx, rbx)
		sub(imm(1), rbx)
		kmovw(ebx, k(2))
        /* load address of alpha */
        mov(var(alpha), rdx)
        /* broadcast alpha */
        vbroadcastsd(mem(rdx), zmm30)
        vmulpd( zmm30, zmm6, zmm6 )
        vmulpd( zmm30, zmm7, zmm7 )
        vmulpd( zmm30, zmm28, zmm28 )
        STORE_ROWSTORED_C_24xN_Bn
        jmp(.DDONE)

        label(.DBETAZERO)
        /* set ZF if (8*cs_c) == 8 */
        cmp(imm(8), rdi)
        /* jump to row storage case */
        jz(.DROWSTORBZ)
        label(.DCOLSTORBZ)
        STORE_COLSTORED_C_24x1_B0
        jmp(.DDONE)

        label(.DROWSTORBZ)
		mov(imm(1), rbx)
		mov(imm(1), r9)
		shlx(r9, rbx, rbx)
		sub(imm(1), rbx)
		kmovw(ebx, k(2))
        /* load address of alpha */
        mov(var(alpha), rdx)
        /* broadcast alpha */
        vbroadcastsd(mem(rdx), zmm30)
        vmulpd( zmm30, zmm6, zmm6 )
        vmulpd( zmm30, zmm7, zmm7 )
        vmulpd( zmm30, zmm28, zmm28 )
        STORE_ROWSTORED_C_24xN_B0
        jmp(.DDONE)

        label(.DDONE)
        POST_K_LOOP
        M_LEFT_24x1

        label(.CONCLUDE)
        vzeroupper()

        end_asm(
          : // output operands (none)
          : // input operands
            [ps_a8] "m" (ps_a8),
            [m_iter] "m" (m_iter),
            [m_left] "m" (m_left),
            [k_iter] "m" (k_iter),
            [k_left] "m" (k_left),
            [a]      "m" (a),
            [rs_a0]   "m" (rs_a0),
            [cs_a0]   "m" (cs_a0),
            [b]      "m" (b),
            [rs_b0]   "m" (rs_b0),
            [cs_b0]   "m" (cs_b0),
            [alpha]  "m" (alpha),
            [beta]   "m" (beta),
            [c]      "m" (c),
            [rs_c0]   "m" (rs_c0),
            [cs_c0]   "m" (cs_c0)
            : // register clobber list
            "rax", "rbx", "rcx", "rdx", "rsi", "rdi",
            "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15",
            "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm6", "xmm7",
            "xmm9", "xmm28", "xmm29", "xmm30", "xmm31",
            "ymm0", "ymm1", "ymm2", "ymm3", "ymm4", "ymm6", "ymm7",
            "ymm9", "ymm28", "ymm29", "ymm30", "ymm31",
            "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5", "zmm6",
            "zmm7", "zmm8", "zmm9", "zmm10", "zmm11", "zmm12", "zmm13",
            "zmm14", "zmm15", "zmm16", "zmm17", "zmm18", "zmm19",
            "zmm20", "zmm21", "zmm22", "zmm23", "zmm24", "zmm25",
            "zmm26", "zmm27", "zmm28", "zmm29", "zmm30", "zmm31",
            "k2", "k3",
            "memory"
        )
    } //mloop
    AOCL_DTL_TRACE_EXIT(AOCL_DTL_LEVEL_TRACE_7);
}

/**
 * @brief bli_dgemmsup_cv_zen4_asm_24x2m_new Kernel
 * This kernel performs a double-precision general matrix-matrix multiplication
 * (DGEMM) operation. It processes matrices A, B, and C, where A is MxK, B is KxN, and C is MxN. 
 * 
 * this kernel computes MX2, where M could be the original
 * m input or an MC block, depending on the framework calls. The computation
 * is facilitated by several helper macros:
 * - PREPARE_M_LOOP: [IR Loop] For one iteration of this loop, a block of MRx2 is computed
 * - PRE_K_LOOP: loads all the necessary parameters and matrix offsets needed for computing GEMM
 * - PREPARE_SCRATCHPAD: clear out vector registers zmm6-29
 * 
 * The main computation of A*B is performed by the following macros:
 * - UNROLL_K_LOOP_24x8_L1: Unrolls the K loop by 8, storing results
 * in vector registers zmm6-29.
 * - UNROLL_K_LOOP_24x8_L2: Unrolls the K loop by 8, storing results
 * in vector registers zmm6-29.
 * 
 * After the computation, the kernel checks the storage type of matrix C and
 * uses the appropriate macros to store the results:
 * - STORE_COLSTORED_C_24x8_Bn/0: For column-stored C matrix.
 * - STORE_ROWSTORED_C_24x8_Bn/0: For row-stored C matrix.
 */
void bli_dgemmsup_cv_zen4_asm_24x2m_new
(
       conj_t    conja,
       conj_t    conjb,
       dim_t     m0,
       dim_t     n0,
       dim_t     k0,
       double*    restrict alpha,
#if   BLIS_INT_TYPE_SIZE == 32
       double*    restrict a, inc_t rs_a, inc_t cs_a,
       double*    restrict b, inc_t rs_b, inc_t cs_b,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c, inc_t cs_c,
#else
       double*    restrict a, inc_t rs_a0, inc_t cs_a0,
       double*    restrict b, inc_t rs_b0, inc_t cs_b0,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c0, inc_t cs_c0,
#endif
       auxinfo_t* restrict data,
       cntx_t*    restrict cntx
     )
{
    AOCL_DTL_TRACE_ENTRY(AOCL_DTL_LEVEL_TRACE_7);

    uint64_t m_iter = (uint64_t)m0 / 24;
    uint64_t m_left = (uint64_t)m0 % 24;
    uint64_t k_iter = (uint64_t)k0 / 8;
    uint64_t k_left = (uint64_t)k0 % 8;
    uint64_t ps_a = bli_auxinfo_ps_a( data );
    uint64_t ps_a8  = ps_a * sizeof( double );

#if   BLIS_INT_TYPE_SIZE == 32
    uint64_t rs_a0   = (uint64_t)rs_a;
    uint64_t cs_a0   = (uint64_t)cs_a;
    uint64_t rs_b0   = (uint64_t)rs_b;
    uint64_t cs_b0   = (uint64_t)cs_b;
    uint64_t rs_c0   = (uint64_t)rs_c;
    uint64_t cs_c0   = (uint64_t)cs_c;
#endif

    {
        begin_asm()
        /**
         * @brief [IR Loop] For one iteration of this loop, a block of MRxNR is computed
         * This loop moves along m-dimension of c matrix with steps of MR*rs_c,
         * and moves along m-dimension in a matrix with steps of MR*rs_a.
         */
        PREPARE_M_LOOP
        PRE_K_LOOP

		/* panel stride of A */
		mov(var(ps_a8), r14)
        /**
         * Prefetch the next panel of matrix A into the cache.
         * This helps in reducing cache misses and improving performance.
         *
         * lea(mem(rax, r14, 1, 7*8), r14)
         * - Load the effective address of the next panel of A into r14.
         * - rax: Base address of A.
         * - r14: Offset to the current panel of A.
         * - 1: Scale factor (no scaling in this case).
         * - 7*8: Offset to the next panel (assuming each panel is 8 bytes and we are moving 7 panels ahead).
         * - r11 = b + 8 * rs_b + 7*8(r11 points to next NR + offset(7*8))
         *
         * lea(mem(rbx, r8, 8, 7*8), r11)
         * - Calculate the address for prefetching B into r11.
         * - rbx: Base address of B.
         * - r8: Row stride of B (rs_b).
         * - 8: Scale factor (assuming 8 bytes per element).
         * - 7*8: Offset to the next set of elements for prefetching.
         * - r15 = r11(next NR + offset of b matrix) + 4 * cs_b
         */
		lea(mem(rax, r14, 1, 7*8), r14)
		lea(mem(rbx, r8, 8, 7*8), r11)

        /**
         * @brief Clear ZMM vector registers 6 to 28, which were used to store the results of Alpha * A * B.
         *
         * This function initializes zmm6 to zero using the vxorpd instruction and then copies
         * the zeroed value from zmm6 to the other ZMM registers (zmm7 to zmm28) using vmovapd instructions.
         * zmm0, 1, 2 and zmm3, 4, 5 are used to load 24 elements of a matrix of respective columns.
         * zmm30, zmm31 are used to broadcast elements of b matrix of resepctive rows.
         */
        PREPARE_SCRATCHPAD

        label(.DLOOPKITER)
        mov(var(k_iter), rsi)
        sub(imm( 8+TAIL_NITER), rsi)
        jle(.PREFETCHLOOP)
        label(.LOOP1)
            UNROLL_K_LOOP_24x2_L1
            dec(rsi)
        jnz(.LOOP1)

        label(.PREFETCHLOOP)
        add(imm(8), rsi)
        jle(.TAILITER)

        label(.LOOP2)
            UNROLL_K_LOOP_24x2_L2
            dec(rsi)
        jnz(.LOOP2)
        label(.TAILITER)
        add(imm(TAIL_NITER), rsi)
        jle(.TAIL)
        label(.LOOP3)
            UNROLL_K_LOOP_24x2_L1
            dec(rsi)
        jnz(.LOOP3)
        vaddpd(zmm10, zmm6, zmm6)
        vaddpd(zmm11, zmm7, zmm7)
        vaddpd(zmm12, zmm28, zmm28)
        vaddpd(zmm13, zmm8, zmm8)
        vaddpd(zmm14, zmm9, zmm9)
        vaddpd(zmm15, zmm29, zmm29)
        label(.TAIL)
        mov(var(k_left), rsi)
        test(rsi, rsi)
        je(.DPOSTACCUM)
        label(.DLOOPKLEFT)
            vmovupd( mem(rax), zmm0 )
            vmovupd( 0x40(rax), zmm1 )
            vmovupd( 0x80(rax), zmm2 )
            /* a += cs_a */
            add( r10, rax )
            K_LOOP_24x2_SET1(0, 1, 2)
            dec(rsi)
        jne(.DLOOPKLEFT)

        label(.DPOSTACCUM)
        POST_ACCUMULATION
        /* if ZF == 1, jump to beta == 0 case */
        je(.DBETAZERO)
        /* set ZF if (8*cs_c) == 8 */
        cmp(imm(8), rdi)
        jz(.DROWSTORED)

        label(.DCOLSTORED)
        STORE_COLSTORED_C_24x2_Bn
        jmp(.DDONE)

        label(.DROWSTORED)
		mov(imm(1), rbx)
		mov(imm(2), r9)
		shlx(r9, rbx, rbx)
		sub(imm(1), rbx)
		kmovw(ebx, k(2))
        /* load address of alpha */
        mov(var(alpha), rdx)
        /* broadcast alpha */
        vbroadcastsd(mem(rdx), zmm30)
        vmulpd( zmm30, zmm6, zmm6 )
        vmulpd( zmm30, zmm7, zmm7 )
        vmulpd( zmm30, zmm28, zmm28 )
        vmulpd( zmm30, zmm8, zmm8 )
        vmulpd( zmm30, zmm9, zmm9 )
        vmulpd( zmm30, zmm29, zmm29 )
        STORE_ROWSTORED_C_24xN_Bn
        jmp(.DDONE)

        label(.DBETAZERO)
        /* set ZF if (8*cs_c) == 8 */
        cmp(imm(8), rdi)
        /* jump to row storage case */
        jz(.DROWSTORBZ)
        label(.DCOLSTORBZ)
        STORE_COLSTORED_C_24x2_B0
        jmp(.DDONE)

        label(.DROWSTORBZ)
		mov(imm(1), rbx)
		mov(imm(2), r9)
		shlx(r9, rbx, rbx)
		sub(imm(1), rbx)
		kmovw(ebx, k(2))
        /* load address of alpha */
        mov(var(alpha), rdx)
        /* broadcast alpha */
        vbroadcastsd(mem(rdx), zmm30)
        vmulpd( zmm30, zmm6, zmm6 )
        vmulpd( zmm30, zmm7, zmm7 )
        vmulpd( zmm30, zmm28, zmm28 )
        vmulpd( zmm30, zmm8, zmm8 )
        vmulpd( zmm30, zmm9, zmm9 )
        vmulpd( zmm30, zmm29, zmm29 )
        STORE_ROWSTORED_C_24xN_B0
        jmp(.DDONE)

        label(.DDONE)
        POST_K_LOOP
        M_LEFT_24x2

        label(.CONCLUDE)
        vzeroupper()

        end_asm(
          : // output operands (none)
          : // input operands
            [ps_a8] "m" (ps_a8),
            [m_iter] "m" (m_iter),
            [m_left] "m" (m_left),
            [k_iter] "m" (k_iter),
            [k_left] "m" (k_left),
            [a]      "m" (a),
            [rs_a0]   "m" (rs_a0),
            [cs_a0]   "m" (cs_a0),
            [b]      "m" (b),
            [rs_b0]   "m" (rs_b0),
            [cs_b0]   "m" (cs_b0),
            [alpha]  "m" (alpha),
            [beta]   "m" (beta),
            [c]      "m" (c),
            [rs_c0]   "m" (rs_c0),
            [cs_c0]   "m" (cs_c0)
            : // register clobber list
            "rax", "rbx", "rcx", "rdx", "rsi", "rdi",
            "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15",
            "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm6", "xmm7",
            "xmm8", "xmm9", "xmm11", "xmm12", "xmm13", "xmm14",
            "xmm15", "xmm28", "xmm29", "xmm30", "xmm31",
            "ymm0", "ymm1", "ymm2", "ymm3", "ymm4", "ymm6", "ymm7",
            "ymm8", "ymm9", "ymm11", "ymm12", "ymm13", "ymm15",
            "ymm28", "ymm29", "ymm30", "ymm31",
            "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5", "zmm6",
            "zmm7", "zmm8", "zmm9", "zmm10", "zmm11", "zmm12", "zmm13",
            "zmm14", "zmm15", "zmm16", "zmm17", "zmm18", "zmm19",
            "zmm20", "zmm21", "zmm22", "zmm23", "zmm24", "zmm25",
            "zmm26", "zmm27", "zmm28", "zmm29", "zmm30", "zmm31",
            "k2", "k3",
            "memory"
        )
    } //mloop
    AOCL_DTL_TRACE_EXIT(AOCL_DTL_LEVEL_TRACE_7);
}

/**
 * @brief bli_dgemmsup_cv_zen4_asm_24x3m_new Kernel
 * This kernel performs a double-precision general matrix-matrix multiplication
 * (DGEMM) operation. It processes matrices A, B, and C, where A is MxK, B is KxN, and C is MxN. 
 * 
 * this kernel computes MX3, where M could be the original
 * m input or an MC block, depending on the framework calls. The computation
 * is facilitated by several helper macros:
 * - PREPARE_M_LOOP: [IR Loop] For one iteration of this loop, a block of MRx3 is computed
 * - PRE_K_LOOP: loads all the necessary parameters and matrix offsets needed for computing GEMM
 * - PREPARE_SCRATCHPAD: clear out vector registers zmm6-29
 * 
 * The main computation of A*B is performed by the following macros:
 * - UNROLL_K_LOOP_24x8_L1: Unrolls the K loop by 8, storing results
 * in vector registers zmm6-29.
 * - UNROLL_K_LOOP_24x8_L2: Unrolls the K loop by 8, storing results
 * in vector registers zmm6-29.
 * 
 * After the computation, the kernel checks the storage type of matrix C and
 * uses the appropriate macros to store the results:
 * - STORE_COLSTORED_C_24x8_Bn/0: For column-stored C matrix.
 * - STORE_ROWSTORED_C_24x8_Bn/0: For row-stored C matrix.
 */
void bli_dgemmsup_cv_zen4_asm_24x3m_new
(
       conj_t    conja,
       conj_t    conjb,
       dim_t     m0,
       dim_t     n0,
       dim_t     k0,
       double*    restrict alpha,
#if   BLIS_INT_TYPE_SIZE == 32
       double*    restrict a, inc_t rs_a, inc_t cs_a,
       double*    restrict b, inc_t rs_b, inc_t cs_b,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c, inc_t cs_c,
#else
       double*    restrict a, inc_t rs_a0, inc_t cs_a0,
       double*    restrict b, inc_t rs_b0, inc_t cs_b0,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c0, inc_t cs_c0,
#endif
       auxinfo_t* restrict data,
       cntx_t*    restrict cntx
     )
{
    AOCL_DTL_TRACE_ENTRY(AOCL_DTL_LEVEL_TRACE_7);

    uint64_t m_iter = (uint64_t)m0 / 24;
    uint64_t m_left = (uint64_t)m0 % 24;
    uint64_t k_iter = (uint64_t)k0 / 8;
    uint64_t k_left = (uint64_t)k0 % 8;
    uint64_t ps_a = bli_auxinfo_ps_a( data );
    uint64_t ps_a8  = ps_a * sizeof( double );

#if   BLIS_INT_TYPE_SIZE == 32
    uint64_t rs_a0   = (uint64_t)rs_a;
    uint64_t cs_a0   = (uint64_t)cs_a;
    uint64_t rs_b0   = (uint64_t)rs_b;
    uint64_t cs_b0   = (uint64_t)cs_b;
    uint64_t rs_c0   = (uint64_t)rs_c;
    uint64_t cs_c0   = (uint64_t)cs_c;
#endif

    {
        begin_asm()
        /**
         * @brief [IR Loop] For one iteration of this loop, a block of MRxNR is computed
         * This loop moves along m-dimension of c matrix with steps of MR*rs_c,
         * and moves along m-dimension in a matrix with steps of MR*rs_a.
         */
        PREPARE_M_LOOP
        PRE_K_LOOP

		/* panel stride of A */
		mov(var(ps_a8), r14)
        /**
         * Prefetch the next panel of matrix A into the cache.
         * This helps in reducing cache misses and improving performance.
         *
         * lea(mem(rax, r14, 1, 7*8), r14)
         * - Load the effective address of the next panel of A into r14.
         * - rax: Base address of A.
         * - r14: Offset to the current panel of A.
         * - 1: Scale factor (no scaling in this case).
         * - 7*8: Offset to the next panel (assuming each panel is 8 bytes and we are moving 7 panels ahead).
         * - r11 = b + 8 * rs_b + 7*8(r11 points to next NR + offset(7*8))
         *
         * lea(mem(rbx, r8, 8, 7*8), r11)
         * - Calculate the address for prefetching B into r11.
         * - rbx: Base address of B.
         * - r8: Row stride of B (rs_b).
         * - 8: Scale factor (assuming 8 bytes per element).
         * - 7*8: Offset to the next set of elements for prefetching.
         * - r15 = r11(next NR + offset of b matrix) + 4 * cs_b
         */
		lea(mem(rax, r14, 1, 7*8), r14)
		lea(mem(rbx, r8, 8, 7*8), r11)

        /**
         * @brief Clear ZMM vector registers 6 to 28, which were used to store the results of Alpha * A * B.
         *
         * This function initializes zmm6 to zero using the vxorpd instruction and then copies
         * the zeroed value from zmm6 to the other ZMM registers (zmm7 to zmm28) using vmovapd instructions.
         * zmm0, 1, 2 and zmm3, 4, 5 are used to load 24 elements of a matrix of respective columns.
         * zmm30, zmm31 are used to broadcast elements of b matrix of resepctive rows.
         */
        PREPARE_SCRATCHPAD

        label(.DLOOPKITER)
        mov(var(k_iter), rsi)
        sub(imm( 8+TAIL_NITER), rsi)
        jle(.PREFETCHLOOP)
        label(.LOOP1)
            UNROLL_K_LOOP_24x3_L1
            dec(rsi)
        jnz(.LOOP1)
        label(.PREFETCHLOOP)
        add(imm(8), rsi)
        jle(.TAILITER)

        label(.LOOP2)
            UNROLL_K_LOOP_24x3_L2
            dec(rsi)
        jnz(.LOOP2)
        label(.TAILITER)
        add(imm(TAIL_NITER), rsi)
        jle(.TAIL)
        label(.LOOP3)
            UNROLL_K_LOOP_24x3_L1
            dec(rsi)
        jnz(.LOOP3)
        vaddpd(zmm12, zmm6, zmm6)
        vaddpd(zmm13, zmm7, zmm7)
        vaddpd(zmm14, zmm28, zmm28)
        vaddpd(zmm15, zmm8, zmm8)
        vaddpd(zmm16, zmm9, zmm9)
        vaddpd(zmm17, zmm29, zmm29)
        vaddpd(zmm18, zmm10, zmm10)
        vaddpd(zmm19, zmm11, zmm11)
        vaddpd(zmm20, zmm26, zmm26)
        label(.TAIL)
        mov(var(k_left), rsi)
        test(rsi, rsi)
        je(.DPOSTACCUM)
        label(.DLOOPKLEFT)
            vmovupd( mem(rax), zmm0 )
            vmovupd( 0x40(rax), zmm1 )
            vmovupd( 0x80(rax), zmm2 )
            /* a += cs_a */
            add( r10, rax )
            K_LOOP_24x3_SET1(0, 1, 2)
            dec(rsi)
        jne(.DLOOPKLEFT)

        label(.DPOSTACCUM)
        POST_ACCUMULATION
        /* if ZF == 1, jump to beta == 0 case */
        je(.DBETAZERO)
        /* set ZF if (8*cs_c) == 8 */
        cmp(imm(8), rdi)
        jz(.DROWSTORED)

        label(.DCOLSTORED)
        STORE_COLSTORED_C_24x3_Bn
        jmp(.DDONE)

        label(.DROWSTORED)
		mov(imm(1), rbx)
		mov(imm(3), r9)
		shlx(r9, rbx, rbx)
		sub(imm(1), rbx)
		kmovw(ebx, k(2))
        /* load address of alpha */
        mov(var(alpha), rdx)
        /* broadcast alpha */
        vbroadcastsd(mem(rdx), zmm30)
        vmulpd( zmm30, zmm6, zmm6 )
        vmulpd( zmm30, zmm7, zmm7 )
        vmulpd( zmm30, zmm28, zmm28 )
        vmulpd( zmm30, zmm8, zmm8 )
        vmulpd( zmm30, zmm9, zmm9 )
        vmulpd( zmm30, zmm29, zmm29 )
        vmulpd( zmm30, zmm10, zmm10 )
        vmulpd( zmm30, zmm11, zmm11 )
        vmulpd( zmm30, zmm26, zmm26 )
        STORE_ROWSTORED_C_24xN_Bn
        jmp(.DDONE)

        label(.DBETAZERO)
        /* set ZF if (8*cs_c) == 8 */
        cmp(imm(8), rdi)
        /* jump to row storage case */
        jz(.DROWSTORBZ)
        label(.DCOLSTORBZ)
        STORE_COLSTORED_C_24x3_B0
        jmp(.DDONE)

        label(.DROWSTORBZ)
		mov(imm(1), rbx)
		mov(imm(3), r9)
		shlx(r9, rbx, rbx)
		sub(imm(1), rbx)
		kmovw(ebx, k(2))
        /* load address of alpha */
        mov(var(alpha), rdx)
        /* broadcast alpha */
        vbroadcastsd(mem(rdx), zmm30)
        vmulpd( zmm30, zmm6, zmm6 )
        vmulpd( zmm30, zmm7, zmm7 )
        vmulpd( zmm30, zmm28, zmm28 )
        vmulpd( zmm30, zmm8, zmm8 )
        vmulpd( zmm30, zmm9, zmm9 )
        vmulpd( zmm30, zmm29, zmm29 )
        vmulpd( zmm30, zmm10, zmm10 )
        vmulpd( zmm30, zmm11, zmm11 )
        vmulpd( zmm30, zmm26, zmm26 )
        STORE_ROWSTORED_C_24xN_B0
        jmp(.DDONE)

        label(.DDONE)
        POST_K_LOOP
        M_LEFT_24x3

        label(.CONCLUDE)
        vzeroupper()

        end_asm(
          : // output operands (none)
          : // input operands
            [ps_a8] "m" (ps_a8),
            [m_iter] "m" (m_iter),
            [m_left] "m" (m_left),
            [k_iter] "m" (k_iter),
            [k_left] "m" (k_left),
            [a]      "m" (a),
            [rs_a0]   "m" (rs_a0),
            [cs_a0]   "m" (cs_a0),
            [b]      "m" (b),
            [rs_b0]   "m" (rs_b0),
            [cs_b0]   "m" (cs_b0),
            [alpha]  "m" (alpha),
            [beta]   "m" (beta),
            [c]      "m" (c),
            [rs_c0]   "m" (rs_c0),
            [cs_c0]   "m" (cs_c0)
            : // register clobber list
            "rax", "rbx", "rcx", "rdx", "rsi", "rdi",
            "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15",
            "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm6", "xmm7",
            "xmm8", "xmm9", "xmm10", "xmm11", "xmm13", "xmm14",
            "xmm15", "xmm16", "xmm17", "xmm19", "xmm20", "xmm26",
            "xmm28", "xmm29", "xmm30", "xmm31",
            "ymm0", "ymm1", "ymm2", "ymm3", "ymm4", "ymm6", "ymm7",
            "ymm8", "ymm9", "ymm10", "ymm11", "ymm13", "ymm14",
            "ymm15", "ymm17", "ymm20", "ymm26", "ymm28", "ymm29",
            "ymm30", "ymm31",
            "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5", "zmm6",
            "zmm7", "zmm8", "zmm9", "zmm10", "zmm11", "zmm12", "zmm13",
            "zmm14", "zmm15", "zmm16", "zmm17", "zmm18", "zmm19",
            "zmm20", "zmm21", "zmm22", "zmm23", "zmm24", "zmm25",
            "zmm26", "zmm27", "zmm28", "zmm29", "zmm30", "zmm31",
            "k2", "k3",
            "memory"
        )
    } //mloop
    AOCL_DTL_TRACE_EXIT(AOCL_DTL_LEVEL_TRACE_7);
}

/**
 * @brief bli_dgemmsup_cv_zen4_asm_24x4m_new Kernel
 * This kernel performs a double-precision general matrix-matrix multiplication
 * (DGEMM) operation. It processes matrices A, B, and C, where A is MxK, B is KxN, and C is MxN. 
 * 
 * this kernel computes MX4, where M could be the original
 * m input or an MC block, depending on the framework calls. The computation
 * is facilitated by several helper macros:
 * - PREPARE_M_LOOP: [IR Loop] For one iteration of this loop, a block of MRx4 is computed
 * - PRE_K_LOOP: loads all the necessary parameters and matrix offsets needed for computing GEMM
 * - PREPARE_SCRATCHPAD: clear out vector registers zmm6-29
 * 
 * The main computation of A*B is performed by the following macros:
 * - UNROLL_K_LOOP_24x8_L1: Unrolls the K loop by 8, storing results
 * in vector registers zmm6-29.
 * - UNROLL_K_LOOP_24x8_L2: Unrolls the K loop by 8, storing results
 * in vector registers zmm6-29.
 * 
 * After the computation, the kernel checks the storage type of matrix C and
 * uses the appropriate macros to store the results:
 * - STORE_COLSTORED_C_24x8_Bn/0: For column-stored C matrix.
 * - STORE_ROWSTORED_C_24x8_Bn/0: For row-stored C matrix.
 */
void bli_dgemmsup_cv_zen4_asm_24x4m_new
(
       conj_t    conja,
       conj_t    conjb,
       dim_t     m0,
       dim_t     n0,
       dim_t     k0,
       double*    restrict alpha,
#if   BLIS_INT_TYPE_SIZE == 32
       double*    restrict a, inc_t rs_a, inc_t cs_a,
       double*    restrict b, inc_t rs_b, inc_t cs_b,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c, inc_t cs_c,
#else
       double*    restrict a, inc_t rs_a0, inc_t cs_a0,
       double*    restrict b, inc_t rs_b0, inc_t cs_b0,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c0, inc_t cs_c0,
#endif
       auxinfo_t* restrict data,
       cntx_t*    restrict cntx
     )
{
    AOCL_DTL_TRACE_ENTRY(AOCL_DTL_LEVEL_TRACE_7);

    uint64_t m_iter = (uint64_t)m0 / 24;
    uint64_t m_left = (uint64_t)m0 % 24;
    uint64_t k_iter = (uint64_t)k0 / 8;
    uint64_t k_left = (uint64_t)k0 % 8;
    uint64_t ps_a = bli_auxinfo_ps_a( data );
    uint64_t ps_a8  = ps_a * sizeof( double );

#if   BLIS_INT_TYPE_SIZE == 32
    uint64_t rs_a0   = (uint64_t)rs_a;
    uint64_t cs_a0   = (uint64_t)cs_a;
    uint64_t rs_b0   = (uint64_t)rs_b;
    uint64_t cs_b0   = (uint64_t)cs_b;
    uint64_t rs_c0   = (uint64_t)rs_c;
    uint64_t cs_c0   = (uint64_t)cs_c;
#endif

    {
        begin_asm()
        /**
         * @brief [IR Loop] For one iteration of this loop, a block of MRxNR is computed
         * This loop moves along m-dimension of c matrix with steps of MR*rs_c,
         * and moves along m-dimension in a matrix with steps of MR*rs_a.
         */
        PREPARE_M_LOOP
        PRE_K_LOOP

		/* panel stride of A */
		mov(var(ps_a8), r14)
        /**
         * Prefetch the next panel of matrix A into the cache.
         * This helps in reducing cache misses and improving performance.
         *
         * lea(mem(rax, r14, 1, 7*8), r14)
         * - Load the effective address of the next panel of A into r14.
         * - rax: Base address of A.
         * - r14: Offset to the current panel of A.
         * - 1: Scale factor (no scaling in this case).
         * - 7*8: Offset to the next panel (assuming each panel is 8 bytes and we are moving 7 panels ahead).
         * - r11 = b + 8 * rs_b + 7*8(r11 points to next NR + offset(7*8))
         *
         * lea(mem(rbx, r8, 8, 7*8), r11)
         * - Calculate the address for prefetching B into r11.
         * - rbx: Base address of B.
         * - r8: Row stride of B (rs_b).
         * - 8: Scale factor (assuming 8 bytes per element).
         * - 7*8: Offset to the next set of elements for prefetching.
         * - r15 = r11(next NR + offset of b matrix) + 4 * cs_b
         */
		lea(mem(rax, r14, 1, 7*8), r14)
		lea(mem(rbx, r8, 8, 7*8), r11)

        /**
         * @brief Clear ZMM vector registers 6 to 28, which were used to store the results of Alpha * A * B.
         *
         * This function initializes zmm6 to zero using the vxorpd instruction and then copies
         * the zeroed value from zmm6 to the other ZMM registers (zmm7 to zmm28) using vmovapd instructions.
         * zmm0, 1, 2 and zmm3, 4, 5 are used to load 24 elements of a matrix of respective columns.
         * zmm30, zmm31 are used to broadcast elements of b matrix of resepctive rows.
         */
        PREPARE_SCRATCHPAD

        label(.DLOOPKITER)
        mov(var(k_iter), rsi)
        sub(imm( 8+TAIL_NITER), rsi)
        jle(.PREFETCHLOOP)
        label(.LOOP1)
            UNROLL_K_LOOP_24x4_L1
            dec(rsi)
        jnz(.LOOP1)

        label(.PREFETCHLOOP)
        add(imm(8), rsi)
        jle(.TAILITER)

        label(.LOOP2)
            UNROLL_K_LOOP_24x4_L2
            dec(rsi)
        jnz(.LOOP2)
        label(.TAILITER)
        add(imm(TAIL_NITER), rsi)
        jle(.TAIL)
        label(.LOOP3)
            UNROLL_K_LOOP_24x4_L1
            dec(rsi)
        jnz(.LOOP3)
        vaddpd(zmm14, zmm6, zmm6)
        vaddpd(zmm15, zmm7, zmm7)
        vaddpd(zmm16, zmm28, zmm28)
        vaddpd(zmm17, zmm8, zmm8)
        vaddpd(zmm18, zmm9, zmm9)
        vaddpd(zmm19, zmm29, zmm29)
        vaddpd(zmm20, zmm10, zmm10)
        vaddpd(zmm21, zmm11, zmm11)
        vaddpd(zmm22, zmm26, zmm26)
        vaddpd(zmm23, zmm12, zmm12)
        vaddpd(zmm24, zmm13, zmm13)
        vaddpd(zmm25, zmm27, zmm27)
        label(.TAIL)
        mov(var(k_left), rsi)
        test(rsi, rsi)
        je(.DPOSTACCUM)
        label(.DLOOPKLEFT)
            vmovupd( mem(rax), zmm0 )
            vmovupd( 0x40(rax), zmm1 )
            vmovupd( 0x80(rax), zmm2 )
            /* a += cs_a */
            add( r10, rax )
            K_LOOP_24x4_SET1(0, 1, 2)
            dec(rsi)
        jne(.DLOOPKLEFT)

        label(.DPOSTACCUM)
        POST_ACCUMULATION
        /* if ZF == 1, jump to beta == 0 case */
        je(.DBETAZERO)
        /* set ZF if (8*cs_c) == 8 */
        cmp(imm(8), rdi)
        jz(.DROWSTORED)

        label(.DCOLSTORED)
        STORE_COLSTORED_C_24x4_Bn
        jmp(.DDONE)

        label(.DROWSTORED)
		mov(imm(1), rbx)
		mov(imm(4), r9)
		shlx(r9, rbx, rbx)
		sub(imm(1), rbx)
		kmovw(ebx, k(2))
        /* broadcast alpha */
        mov(var(alpha), rdx)
        /* broadcast alpha */
        vbroadcastsd(mem(rdx), zmm30)
        vmulpd( zmm30, zmm6, zmm6 )
        vmulpd( zmm30, zmm7, zmm7 )
        vmulpd( zmm30, zmm28, zmm28 )
        vmulpd( zmm30, zmm8, zmm8 )
        vmulpd( zmm30, zmm9, zmm9 )
        vmulpd( zmm30, zmm29, zmm29 )
        vmulpd( zmm30, zmm10, zmm10 )
        vmulpd( zmm30, zmm11, zmm11 )
        vmulpd( zmm30, zmm26, zmm26 )
        vmulpd( zmm30, zmm12, zmm12 )
        vmulpd( zmm30, zmm13, zmm13 )
        vmulpd( zmm30, zmm27, zmm27 )
        STORE_ROWSTORED_C_24xN_Bn
        jmp(.DDONE)

        label(.DBETAZERO)
        /* set ZF if (8*cs_c) == 8 */
        cmp(imm(8), rdi)

        /* jump to row storage case */
        jz(.DROWSTORBZ)
        label(.DCOLSTORBZ)
        STORE_COLSTORED_C_24x4_B0
        jmp(.DDONE)

        label(.DROWSTORBZ)
		mov(imm(1), rbx)
		mov(imm(4), r9)
		shlx(r9, rbx, rbx)
		sub(imm(1), rbx)
		kmovw(ebx, k(2))
        /* load address of alpha */
        mov(var(alpha), rdx)
        /* broadcast alpha */
        vbroadcastsd(mem(rdx), zmm30)
        vmulpd( zmm30, zmm6, zmm6 )
        vmulpd( zmm30, zmm7, zmm7 )
        vmulpd( zmm30, zmm28, zmm28 )
        vmulpd( zmm30, zmm8, zmm8 )
        vmulpd( zmm30, zmm9, zmm9 )
        vmulpd( zmm30, zmm29, zmm29 )
        vmulpd( zmm30, zmm10, zmm10 )
        vmulpd( zmm30, zmm11, zmm11 )
        vmulpd( zmm30, zmm26, zmm26 )
        vmulpd( zmm30, zmm12, zmm12 )
        vmulpd( zmm30, zmm13, zmm13 )
        vmulpd( zmm30, zmm27, zmm27 )
        STORE_ROWSTORED_C_24xN_B0
        jmp(.DDONE)

        label(.DDONE)
        POST_K_LOOP
        M_LEFT_24x4

        label(.CONCLUDE)
        vzeroupper()

        end_asm(
          : // output operands (none)
          : // input operands
            [ps_a8] "m" (ps_a8),
            [m_iter] "m" (m_iter),
            [m_left] "m" (m_left),
            [k_iter] "m" (k_iter),
            [k_left] "m" (k_left),
            [a]      "m" (a),
            [rs_a0]   "m" (rs_a0),
            [cs_a0]   "m" (cs_a0),
            [b]      "m" (b),
            [rs_b0]   "m" (rs_b0),
            [cs_b0]   "m" (cs_b0),
            [alpha]  "m" (alpha),
            [beta]   "m" (beta),
            [c]      "m" (c),
            [rs_c0]   "m" (rs_c0),
            [cs_c0]   "m" (cs_c0)
            : // register clobber list
            "rax", "rbx", "rcx", "rdx", "rsi", "rdi",
            "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15",
            "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm6", "xmm7",
            "xmm8", "xmm9", "xmm10", "xmm11", "xmm12", "xmm13",
            "xmm15", "xmm16", "xmm17", "xmm18", "xmm19", "xmm21",
            "xmm22", "xmm24", "xmm25", "xmm26", "xmm27", "xmm28",
            "xmm29", "xmm30", "xmm31",
            "ymm0", "ymm1", "ymm2", "ymm3", "ymm4", "ymm6", "ymm7",
            "ymm8", "ymm9", "ymm10", "ymm11", "ymm12", "ymm13",
            "ymm15", "ymm16", "ymm17", "ymm19", "ymm21", "ymm22",
            "ymm25", "ymm26", "ymm27", "ymm28", "ymm29", "ymm30",
            "ymm31",
            "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5", "zmm6",
            "zmm7", "zmm8", "zmm9", "zmm10", "zmm11", "zmm12", "zmm13",
            "zmm14", "zmm15", "zmm16", "zmm17", "zmm18", "zmm19",
            "zmm20", "zmm21", "zmm22", "zmm23", "zmm24", "zmm25",
            "zmm26", "zmm27", "zmm28", "zmm29", "zmm30", "zmm31",
            "k2", "k3",
            "memory"
        )
    } //mloop
    AOCL_DTL_TRACE_EXIT(AOCL_DTL_LEVEL_TRACE_7);
}

/**
 * @brief bli_dgemmsup_cv_zen4_asm_24x5m_new Kernel
 * This kernel performs a double-precision general matrix-matrix multiplication
 * (DGEMM) operation. It processes matrices A, B, and C, where A is MxK, B is KxN, and C is MxN. 
 * 
 * this kernel computes MX5, where M could be the original
 * m input or an MC block, depending on the framework calls. The computation
 * is facilitated by several helper macros:
 * - PREPARE_M_LOOP: [IR Loop] For one iteration of this loop, a block of MRx5 is computed
 * - PRE_K_LOOP: loads all the necessary parameters and matrix offsets needed for computing GEMM
 * - PREPARE_SCRATCHPAD: clear out vector registers zmm6-29
 * 
 * The main computation of A*B is performed by the following macros:
 * - UNROLL_K_LOOP_24x8_L1: Unrolls the K loop by 8, storing results
 * in vector registers zmm6-29.
 * - UNROLL_K_LOOP_24x8_L2: Unrolls the K loop by 8, storing results
 * in vector registers zmm6-29.
 * 
 * After the computation, the kernel checks the storage type of matrix C and
 * uses the appropriate macros to store the results:
 * - STORE_COLSTORED_C_24x8_Bn/0: For column-stored C matrix.
 * - STORE_ROWSTORED_C_24x8_Bn/0: For row-stored C matrix.
 */
void bli_dgemmsup_cv_zen4_asm_24x5m_new
(
       conj_t    conja,
       conj_t    conjb,
       dim_t     m0,
       dim_t     n0,
       dim_t     k0,
       double*    restrict alpha,
#if   BLIS_INT_TYPE_SIZE == 32
       double*    restrict a, inc_t rs_a, inc_t cs_a,
       double*    restrict b, inc_t rs_b, inc_t cs_b,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c, inc_t cs_c,
#else
       double*    restrict a, inc_t rs_a0, inc_t cs_a0,
       double*    restrict b, inc_t rs_b0, inc_t cs_b0,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c0, inc_t cs_c0,
#endif
       auxinfo_t* restrict data,
       cntx_t*    restrict cntx
     )
{
    AOCL_DTL_TRACE_ENTRY(AOCL_DTL_LEVEL_TRACE_7);

    uint64_t m_iter = (uint64_t)m0 / 24;
    uint64_t m_left = (uint64_t)m0 % 24;
    uint64_t k_iter = (uint64_t)k0 / 8;
    uint64_t k_left = (uint64_t)k0 % 8;
    uint64_t ps_a = bli_auxinfo_ps_a( data );
    uint64_t ps_a8  = ps_a * sizeof( double );

#if   BLIS_INT_TYPE_SIZE == 32
    uint64_t rs_a0   = (uint64_t)rs_a;
    uint64_t cs_a0   = (uint64_t)cs_a;
    uint64_t rs_b0   = (uint64_t)rs_b;
    uint64_t cs_b0   = (uint64_t)cs_b;
    uint64_t rs_c0   = (uint64_t)rs_c;
    uint64_t cs_c0   = (uint64_t)cs_c;
#endif

    {
        begin_asm()
        /**
         * @brief [IR Loop] For one iteration of this loop, a block of MRxNR is computed
         * This loop moves along m-dimension of c matrix with steps of MR*rs_c,
         * and moves along m-dimension in a matrix with steps of MR*rs_a.
         */
        PREPARE_M_LOOP
        PRE_K_LOOP

		/* panel stride of A */
		mov(var(ps_a8), r14)
        /**
         * Prefetch the next panel of matrix A into the cache.
         * This helps in reducing cache misses and improving performance.
         *
         * lea(mem(rax, r14, 1, 7*8), r14)
         * - Load the effective address of the next panel of A into r14.
         * - rax: Base address of A.
         * - r14: Offset to the current panel of A.
         * - 1: Scale factor (no scaling in this case).
         * - 7*8: Offset to the next panel (assuming each panel is 8 bytes and we are moving 7 panels ahead).
         * - r14 = a + ps_a + 7*8
         *
         * lea(mem(rbx, r8, 8, 7*8), r11)
         * - Calculate the address for prefetching B into r11.
         * - rbx: Base address of B.
         * - r8: Row stride of B (rs_b).
         * - 8: Scale factor (assuming 8 bytes per element).
         * - 7*8: Offset to the next set of elements for prefetching.
         * - r11 = b + 8 * rs_b + 7*8(r11 points to next NR + offset(7*8))
         *
         * lea(mem(r11, r9, 4), r15)
         * - Further adjust the address in r11 by adding 4 times the column stride of B (cs_b) and store it in r15.
         * - r11: Intermediate address for B prefetching.
         * - r9: Column stride of B (cs_b).
         * - 4: Scale factor (assuming 4 columns ahead).
         * - r15 = r11(next NR + offset of b matrix) + 4 * cs_b
         */
		lea(mem(rax, r14, 1, 7*8), r14)
		lea(mem(rbx, r8, 8, 7*8), r11)
		lea(mem(r11, r9, 4), r15)

        /**
         * @brief Clear ZMM vector registers 6 to 28, which were used to store the results of Alpha * A * B.
         *
         * This function initializes zmm6 to zero using the vxorpd instruction and then copies
         * the zeroed value from zmm6 to the other ZMM registers (zmm7 to zmm28) using vmovapd instructions.
         * zmm0, 1, 2 and zmm3, 4, 5 are used to load 24 elements of a matrix of respective columns.
         * zmm30, zmm31 are used to broadcast elements of b matrix of resepctive rows.
         */
        PREPARE_SCRATCHPAD

        label(.DLOOPKITER)
        mov(var(k_iter), rsi)
        sub(imm( 8+TAIL_NITER), rsi)
        jle(.PREFETCHLOOP)
        label(.LOOP1)
            UNROLL_K_LOOP_24x5_L1
            dec(rsi)
        jnz(.LOOP1)

        label(.PREFETCHLOOP)
        add(imm(8), rsi)
        jle(.TAILITER)

        label(.LOOP2)
            UNROLL_K_LOOP_24x5_L2
            dec(rsi)
        jnz(.LOOP2)
        label(.TAILITER)
        add(imm(TAIL_NITER), rsi)
        jle(.TAIL)
        label(.LOOP3)
            UNROLL_K_LOOP_24x5_L1
            dec(rsi)
        jnz(.LOOP3)

        label(.TAIL)
        mov(var(k_left), rsi)
        test(rsi, rsi)
        je(.DPOSTACCUM)
        label(.DLOOPKLEFT)
            vmovupd( mem(rax), zmm0 )
            vmovupd( 0x40(rax), zmm1 )
            vmovupd( 0x80(rax), zmm2 )
            /* a += cs_a */
            add( r10, rax )
            K_LOOP_24x5(0, 1, 2)
            dec(rsi)
        jne(.DLOOPKLEFT)

        label(.DPOSTACCUM)
        POST_ACCUMULATION
        /* if ZF == 1, jump to beta == 0 case */
        je(.DBETAZERO)
        /* set ZF if (8*cs_c) == 8 */
        cmp(imm(8), rdi)
        jz(.DROWSTORED)

        label(.DCOLSTORED)
        STORE_COLSTORED_C_24x5_Bn
        jmp(.DDONE)

        label(.DROWSTORED)
		mov(imm(1), rbx)
		mov(imm(5), r9)
		shlx(r9, rbx, rbx)
		sub(imm(1), rbx)
		kmovw(ebx, k(2))
        /* load address of alpha */
        mov(var(alpha), rdx)
        /* broadcast alpha */
        vbroadcastsd(mem(rdx), zmm30)
        vmulpd( zmm30, zmm6, zmm6 )
        vmulpd( zmm30, zmm7, zmm7 )
        vmulpd( zmm30, zmm28, zmm28 )
        vmulpd( zmm30, zmm8, zmm8 )
        vmulpd( zmm30, zmm9, zmm9 )
        vmulpd( zmm30, zmm29, zmm29 )
        vmulpd( zmm30, zmm10, zmm10 )
        vmulpd( zmm30, zmm11, zmm11 )
        vmulpd( zmm30, zmm26, zmm26 )
        vmulpd( zmm30, zmm12, zmm12 )
        vmulpd( zmm30, zmm13, zmm13 )
        vmulpd( zmm30, zmm27, zmm27 )
        vmulpd( zmm30, zmm14, zmm14 )
        vmulpd( zmm30, zmm15, zmm15 )
        vmulpd( zmm30, zmm24, zmm24 )
        STORE_ROWSTORED_C_24xN_Bn
        jmp(.DDONE)

        label(.DBETAZERO)
        /* set ZF if (8*cs_c) == 8 */
        cmp(imm(8), rdi)
        /* jump to row storage case */
        jz(.DROWSTORBZ)
        label(.DCOLSTORBZ)
        STORE_COLSTORED_C_24x5_B0
        jmp(.DDONE)

        label(.DROWSTORBZ)
		mov(imm(1), rbx)
		mov(imm(5), r9)
		shlx(r9, rbx, rbx)
		sub(imm(1), rbx)
		kmovw(ebx, k(2))
        /* load address of alpha */
        mov(var(alpha), rdx)
        /* broadcast alpha */
        vbroadcastsd(mem(rdx), zmm30)
        vmulpd( zmm30, zmm6, zmm6 )
        vmulpd( zmm30, zmm7, zmm7 )
        vmulpd( zmm30, zmm28, zmm28 )
        vmulpd( zmm30, zmm8, zmm8 )
        vmulpd( zmm30, zmm9, zmm9 )
        vmulpd( zmm30, zmm29, zmm29 )
        vmulpd( zmm30, zmm10, zmm10 )
        vmulpd( zmm30, zmm11, zmm11 )
        vmulpd( zmm30, zmm26, zmm26 )
        vmulpd( zmm30, zmm12, zmm12 )
        vmulpd( zmm30, zmm13, zmm13 )
        vmulpd( zmm30, zmm27, zmm27 )
        vmulpd( zmm30, zmm14, zmm14 )
        vmulpd( zmm30, zmm15, zmm15 )
        vmulpd( zmm30, zmm24, zmm24 )
        STORE_ROWSTORED_C_24xN_B0
        jmp(.DDONE)

        label(.DDONE)
        POST_K_LOOP
        M_LEFT_24x5

        label(.CONCLUDE)
        vzeroupper()

        end_asm(
          : // output operands (none)
          : // input operands
            [ps_a8] "m" (ps_a8),
            [m_iter] "m" (m_iter),
            [m_left] "m" (m_left),
            [k_iter] "m" (k_iter),
            [k_left] "m" (k_left),
            [a]      "m" (a),
            [rs_a0]   "m" (rs_a0),
            [cs_a0]   "m" (cs_a0),
            [b]      "m" (b),
            [rs_b0]   "m" (rs_b0),
            [cs_b0]   "m" (cs_b0),
            [alpha]  "m" (alpha),
            [beta]   "m" (beta),
            [c]      "m" (c),
            [rs_c0]   "m" (rs_c0),
            [cs_c0]   "m" (cs_c0)
            : // register clobber list
            "rax", "rbx", "rcx", "rdx", "rsi", "rdi",
            "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15",
            "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm6", "xmm7",
            "xmm8", "xmm9", "xmm10", "xmm11", "xmm12", "xmm13",
            "xmm14", "xmm15", "xmm17", "xmm19", "xmm21", "xmm23",
            "xmm24", "xmm25", "xmm26", "xmm27", "xmm28", "xmm29",
            "xmm30", "xmm31",
            "ymm0", "ymm1", "ymm2", "ymm3", "ymm4", "ymm6", "ymm7",
            "ymm8", "ymm9", "ymm10", "ymm11", "ymm12", "ymm13",
            "ymm14", "ymm15", "ymm17", "ymm19", "ymm21", "ymm23",
            "ymm24", "ymm25", "ymm26", "ymm27", "ymm28", "ymm29",
            "ymm30", "ymm31",
            "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5", "zmm6",
            "zmm7", "zmm8", "zmm9", "zmm10", "zmm11", "zmm12", "zmm13",
            "zmm14", "zmm15", "zmm16", "zmm17", "zmm18", "zmm19",
            "zmm20", "zmm21", "zmm22", "zmm23", "zmm24", "zmm25",
            "zmm26", "zmm27", "zmm28", "zmm29", "zmm30", "zmm31",
            "k2", "k3",
            "memory"
        )
    } //mloop
    AOCL_DTL_TRACE_EXIT(AOCL_DTL_LEVEL_TRACE_7);
}

/**
 * @brief bli_dgemmsup_cv_zen4_asm_24x6m_new Kernel
 * This kernel performs a double-precision general matrix-matrix multiplication
 * (DGEMM) operation. It processes matrices A, B, and C, where A is MxK, B is KxN, and C is MxN. 
 * 
 * this kernel computes MX6, where M could be the original
 * m input or an MC block, depending on the framework calls. The computation
 * is facilitated by several helper macros:
 * - PREPARE_M_LOOP: [IR Loop] For one iteration of this loop, a block of MRx6 is computed
 * - PRE_K_LOOP: loads all the necessary parameters and matrix offsets needed for computing GEMM
 * - PREPARE_SCRATCHPAD: clear out vector registers zmm6-29
 * 
 * The main computation of A*B is performed by the following macros:
 * - UNROLL_K_LOOP_24x8_L1: Unrolls the K loop by 8, storing results
 * in vector registers zmm6-29.
 * - UNROLL_K_LOOP_24x8_L2: Unrolls the K loop by 8, storing results
 * in vector registers zmm6-29.
 * 
 * After the computation, the kernel checks the storage type of matrix C and
 * uses the appropriate macros to store the results:
 * - STORE_COLSTORED_C_24x8_Bn/0: For column-stored C matrix.
 * - STORE_ROWSTORED_C_24x8_Bn/0: For row-stored C matrix.
 */
void bli_dgemmsup_cv_zen4_asm_24x6m_new
(
       conj_t    conja,
       conj_t    conjb,
       dim_t     m0,
       dim_t     n0,
       dim_t     k0,
       double*    restrict alpha,
#if   BLIS_INT_TYPE_SIZE == 32
       double*    restrict a, inc_t rs_a, inc_t cs_a,
       double*    restrict b, inc_t rs_b, inc_t cs_b,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c, inc_t cs_c,
#else
       double*    restrict a, inc_t rs_a0, inc_t cs_a0,
       double*    restrict b, inc_t rs_b0, inc_t cs_b0,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c0, inc_t cs_c0,
#endif
       auxinfo_t* restrict data,
       cntx_t*    restrict cntx
     )
{
    AOCL_DTL_TRACE_ENTRY(AOCL_DTL_LEVEL_TRACE_7);

    uint64_t m_iter = (uint64_t)m0 / 24;
    uint64_t m_left = (uint64_t)m0 % 24;
    uint64_t k_iter = (uint64_t)k0 / 8;
    uint64_t k_left = (uint64_t)k0 % 8;
    uint64_t ps_a = bli_auxinfo_ps_a( data );
    uint64_t ps_a8  = ps_a * sizeof( double );

#if   BLIS_INT_TYPE_SIZE == 32
    uint64_t rs_a0   = (uint64_t)rs_a;
    uint64_t cs_a0   = (uint64_t)cs_a;
    uint64_t rs_b0   = (uint64_t)rs_b;
    uint64_t cs_b0   = (uint64_t)cs_b;
    uint64_t rs_c0   = (uint64_t)rs_c;
    uint64_t cs_c0   = (uint64_t)cs_c;
#endif

    {
        begin_asm()
        /**
         * @brief [IR Loop] For one iteration of this loop, a block of MRxNR is computed
         * This loop moves along m-dimension of c matrix with steps of MR*rs_c,
         * and moves along m-dimension in a matrix with steps of MR*rs_a.
         */
        PREPARE_M_LOOP
        PRE_K_LOOP

		/* panel stride of A */
		mov(var(ps_a8), r14)
        /**
         * Prefetch the next panel of matrix A into the cache.
         * This helps in reducing cache misses and improving performance.
         *
         * lea(mem(rax, r14, 1, 7*8), r14)
         * - Load the effective address of the next panel of A into r14.
         * - rax: Base address of A.
         * - r14: Offset to the current panel of A.
         * - 1: Scale factor (no scaling in this case).
         * - 7*8: Offset to the next panel (assuming each panel is 8 bytes and we are moving 7 panels ahead).
         * - r14 = a + ps_a + 7*8
         *
         * lea(mem(rbx, r8, 8, 7*8), r11)
         * - Calculate the address for prefetching B into r11.
         * - rbx: Base address of B.
         * - r8: Row stride of B (rs_b).
         * - 8: Scale factor (assuming 8 bytes per element).
         * - 7*8: Offset to the next set of elements for prefetching.
         * - r11 = b + 8 * rs_b + 7*8(r11 points to next NR + offset(7*8))
         *
         * lea(mem(r11, r9, 4), r15)
         * - Further adjust the address in r11 by adding 4 times the column stride of B (cs_b) and store it in r15.
         * - r11: Intermediate address for B prefetching.
         * - r9: Column stride of B (cs_b).
         * - 4: Scale factor (assuming 4 columns ahead).
         * - r15 = r11(next NR + offset of b matrix) + 4 * cs_b
         */
		lea(mem(rax, r14, 1, 7*8), r14)
		lea(mem(rbx, r8, 8, 7*8), r11)
		lea(mem(r11, r9, 4), r15)

        /**
         * @brief Clear ZMM vector registers 6 to 28, which were used to store the results of Alpha * A * B.
         *
         * This function initializes zmm6 to zero using the vxorpd instruction and then copies
         * the zeroed value from zmm6 to the other ZMM registers (zmm7 to zmm28) using vmovapd instructions.
         * zmm0, 1, 2 and zmm3, 4, 5 are used to load 24 elements of a matrix of respective columns.
         * zmm30, zmm31 are used to broadcast elements of b matrix of resepctive rows.
         */
        PREPARE_SCRATCHPAD

        label(.DLOOPKITER)
        mov(var(k_iter), rsi)
        sub(imm( 8+TAIL_NITER), rsi)
        jle(.PREFETCHLOOP)
        label(.LOOP1)
            UNROLL_K_LOOP_24x6_L1
            dec(rsi)
        jnz(.LOOP1)

        label(.PREFETCHLOOP)
        add(imm(8), rsi)
        jle(.TAILITER)

        label(.LOOP2)
            UNROLL_K_LOOP_24x6_L2
            dec(rsi)
        jnz(.LOOP2)
        label(.TAILITER)
        add(imm(TAIL_NITER), rsi)
        jle(.TAIL)
        label(.LOOP3)
            UNROLL_K_LOOP_24x6_L1
            dec(rsi)
        jnz(.LOOP3)

        label(.TAIL)
        mov(var(k_left), rsi)
        test(rsi, rsi)
        je(.DPOSTACCUM)
        label(.DLOOPKLEFT)
            vmovupd( mem(rax), zmm0 )
            vmovupd( 0x40(rax), zmm1 )
            vmovupd( 0x80(rax), zmm2 )
            /* a += cs_a */
            add( r10, rax )
            K_LOOP_24x6(0, 1, 2)
            dec(rsi)
        jne(.DLOOPKLEFT)

        label(.DPOSTACCUM)
        POST_ACCUMULATION
        /* if ZF == 1, jump to beta == 0 case */
        je(.DBETAZERO)
        /* set ZF if (8*cs_c) == 8 */
        cmp(imm(8), rdi)
        jz(.DROWSTORED)

        label(.DCOLSTORED)
        STORE_COLSTORED_C_24x6_Bn
        jmp(.DDONE)

        label(.DROWSTORED)
		mov(imm(1), rbx)
		mov(imm(6), r9)
		shlx(r9, rbx, rbx)
		sub(imm(1), rbx)
		kmovw(ebx, k(2))
        /* load address of alpha */
        mov(var(alpha), rdx)
        /* broadcast alpha */
        vbroadcastsd(mem(rdx), zmm30)
        vmulpd( zmm30, zmm6, zmm6 )
        vmulpd( zmm30, zmm7, zmm7 )
        vmulpd( zmm30, zmm28, zmm28 )
        vmulpd( zmm30, zmm8, zmm8 )
        vmulpd( zmm30, zmm9, zmm9 )
        vmulpd( zmm30, zmm29, zmm29 )
        vmulpd( zmm30, zmm10, zmm10 )
        vmulpd( zmm30, zmm11, zmm11 )
        vmulpd( zmm30, zmm26, zmm26 )
        vmulpd( zmm30, zmm12, zmm12 )
        vmulpd( zmm30, zmm13, zmm13 )
        vmulpd( zmm30, zmm27, zmm27 )
        vmulpd( zmm30, zmm14, zmm14 )
        vmulpd( zmm30, zmm15, zmm15 )
        vmulpd( zmm30, zmm24, zmm24 )
        vmulpd( zmm30, zmm16, zmm16 )
        vmulpd( zmm30, zmm17, zmm17 )
        vmulpd( zmm30, zmm25, zmm25 )
        STORE_ROWSTORED_C_24xN_Bn
        jmp(.DDONE)

        label(.DBETAZERO)
        /* set ZF if (8*cs_c) == 8 */
        cmp(imm(8), rdi)
        /* jump to row storage case */
        jz(.DROWSTORBZ)
        label(.DCOLSTORBZ)
        STORE_COLSTORED_C_24x6_B0
        jmp(.DDONE)

        label(.DROWSTORBZ)
		mov(imm(1), rbx)
		mov(imm(6), r9)
		shlx(r9, rbx, rbx)
		sub(imm(1), rbx)
		kmovw(ebx, k(2))
        mov(var(alpha), rdx) /* load address of alpha */
        vbroadcastsd(mem(rdx), zmm30) /* broadcast alpha */
        vmulpd( zmm30, zmm6, zmm6 )
        vmulpd( zmm30, zmm7, zmm7 )
        vmulpd( zmm30, zmm28, zmm28 )
        vmulpd( zmm30, zmm8, zmm8 )
        vmulpd( zmm30, zmm9, zmm9 )
        vmulpd( zmm30, zmm29, zmm29 )
        vmulpd( zmm30, zmm10, zmm10 )
        vmulpd( zmm30, zmm11, zmm11 )
        vmulpd( zmm30, zmm26, zmm26 )
        vmulpd( zmm30, zmm12, zmm12 )
        vmulpd( zmm30, zmm13, zmm13 )
        vmulpd( zmm30, zmm27, zmm27 )
        vmulpd( zmm30, zmm14, zmm14 )
        vmulpd( zmm30, zmm15, zmm15 )
        vmulpd( zmm30, zmm24, zmm24 )
        vmulpd( zmm30, zmm16, zmm16 )
        vmulpd( zmm30, zmm17, zmm17 )
        vmulpd( zmm30, zmm25, zmm25 )
        STORE_ROWSTORED_C_24xN_B0
        jmp(.DDONE)

        label(.DDONE)
        POST_K_LOOP
        M_LEFT_24x6

        label(.CONCLUDE)
        vzeroupper()

        end_asm(
          : // output operands (none)
          : // input operands
            [ps_a8] "m" (ps_a8),
            [m_iter] "m" (m_iter),
            [m_left] "m" (m_left),
            [k_iter] "m" (k_iter),
            [k_left] "m" (k_left),
            [a]      "m" (a),
            [rs_a0]   "m" (rs_a0),
            [cs_a0]   "m" (cs_a0),
            [b]      "m" (b),
            [rs_b0]   "m" (rs_b0),
            [cs_b0]   "m" (cs_b0),
            [alpha]  "m" (alpha),
            [beta]   "m" (beta),
            [c]      "m" (c),
            [rs_c0]   "m" (rs_c0),
            [cs_c0]   "m" (cs_c0)
            : // register clobber list
            "rax", "rbx", "rcx", "rdx", "rsi", "rdi",
            "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15",
            "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6",
            "xmm7", "xmm8", "xmm9", "xmm10", "xmm11", "xmm12", "xmm13",
            "xmm14", "xmm15", "xmm16", "xmm17", "xmm19", "xmm21",
            "xmm23", "xmm24", "xmm25", "xmm26", "xmm27", "xmm28",
            "xmm29", "xmm30", "xmm31",
            "ymm0", "ymm1", "ymm2", "ymm3", "ymm4", "ymm5", "ymm6",
            "ymm7", "ymm8", "ymm9", "ymm10", "ymm11", "ymm12", "ymm13",
            "ymm14", "ymm15", "ymm16", "ymm17", "ymm19", "ymm21",
            "ymm23", "ymm24", "ymm25", "ymm26", "ymm27", "ymm28",
            "ymm29", "ymm30", "ymm31",
            "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5", "zmm6",
            "zmm7", "zmm8", "zmm9", "zmm10", "zmm11", "zmm12", "zmm13",
            "zmm14", "zmm15", "zmm16", "zmm17", "zmm18", "zmm19",
            "zmm20", "zmm21", "zmm22", "zmm23", "zmm24", "zmm25",
            "zmm26", "zmm27", "zmm28", "zmm29", "zmm30", "zmm31",
            "k2", "k3",
            "memory"
        )
    } //mloop
    AOCL_DTL_TRACE_EXIT(AOCL_DTL_LEVEL_TRACE_7);
}

/**
 * @brief bli_dgemmsup_cv_zen4_asm_24x7m_new Kernel
 * This kernel performs a double-precision general matrix-matrix multiplication
 * (DGEMM) operation. It processes matrices A, B, and C, where A is MxK, B is KxN, and C is MxN. 
 * 
 * this kernel computes MX7, where M could be the original
 * m input or an MC block, depending on the framework calls. The computation
 * is facilitated by several helper macros:
 * - PREPARE_M_LOOP: [IR Loop] For one iteration of this loop, a block of MRx7 is computed
 * - PRE_K_LOOP: loads all the necessary parameters and matrix offsets needed for computing GEMM
 * - PREPARE_SCRATCHPAD: clear out vector registers zmm6-29
 * 
 * The main computation of A*B is performed by the following macros:
 * - UNROLL_K_LOOP_24x8_L1: Unrolls the K loop by 8, storing results
 * in vector registers zmm6-29.
 * - UNROLL_K_LOOP_24x8_L2: Unrolls the K loop by 8, storing results
 * in vector registers zmm6-29.
 * 
 * After the computation, the kernel checks the storage type of matrix C and
 * uses the appropriate macros to store the results:
 * - STORE_COLSTORED_C_24x8_Bn/0: For column-stored C matrix.
 * - STORE_ROWSTORED_C_24x8_Bn/0: For row-stored C matrix.
 */
void bli_dgemmsup_cv_zen4_asm_24x7m_new
(
       conj_t    conja,
       conj_t    conjb,
       dim_t     m0,
       dim_t     n0,
       dim_t     k0,
       double*    restrict alpha,
#if   BLIS_INT_TYPE_SIZE == 32
       double*    restrict a, inc_t rs_a, inc_t cs_a,
       double*    restrict b, inc_t rs_b, inc_t cs_b,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c, inc_t cs_c,
#else
       double*    restrict a, inc_t rs_a0, inc_t cs_a0,
       double*    restrict b, inc_t rs_b0, inc_t cs_b0,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c0, inc_t cs_c0,
#endif
       auxinfo_t* restrict data,
       cntx_t*    restrict cntx
     )
{
    AOCL_DTL_TRACE_ENTRY(AOCL_DTL_LEVEL_TRACE_7);

    uint64_t m_iter = (uint64_t)m0 / 24;
    uint64_t m_left = (uint64_t)m0 % 24;
    uint64_t k_iter = (uint64_t)k0 / 8;
    uint64_t k_left = (uint64_t)k0 % 8;
    uint64_t ps_a = bli_auxinfo_ps_a( data );
    uint64_t ps_a8  = ps_a * sizeof( double );

#if   BLIS_INT_TYPE_SIZE == 32
    uint64_t rs_a0   = (uint64_t)rs_a;
    uint64_t cs_a0   = (uint64_t)cs_a;
    uint64_t rs_b0   = (uint64_t)rs_b;
    uint64_t cs_b0   = (uint64_t)cs_b;
    uint64_t rs_c0   = (uint64_t)rs_c;
    uint64_t cs_c0   = (uint64_t)cs_c;
#endif

    {
        begin_asm()
        /**
         * @brief [IR Loop] For one iteration of this loop, a block of MRxNR is computed
         * This loop moves along m-dimension of c matrix with steps of MR*rs_c,
         * and moves along m-dimension in a matrix with steps of MR*rs_a.
         */
        PREPARE_M_LOOP
        PRE_K_LOOP

		/* panel stride of A */
		mov(var(ps_a8), r14)
        /**
         * Prefetch the next panel of matrix A into the cache.
         * This helps in reducing cache misses and improving performance.
         *
         * lea(mem(rax, r14, 1, 7*8), r14)
         * - Load the effective address of the next panel of A into r14.
         * - rax: Base address of A.
         * - r14: Offset to the current panel of A.
         * - 1: Scale factor (no scaling in this case).
         * - 7*8: Offset to the next panel (assuming each panel is 8 bytes and we are moving 7 panels ahead).
         * - r14 = a + ps_a + 7*8
         *
         * lea(mem(rbx, r8, 8, 7*8), r11)
         * - Calculate the address for prefetching B into r11.
         * - rbx: Base address of B.
         * - r8: Row stride of B (rs_b).
         * - 8: Scale factor (assuming 8 bytes per element).
         * - 7*8: Offset to the next set of elements for prefetching.
         * - r11 = b + 8 * rs_b + 7*8(r11 points to next NR + offset(7*8))
         *
         * lea(mem(r11, r9, 4), r15)
         * - Further adjust the address in r11 by adding 4 times the column stride of B (cs_b) and store it in r15.
         * - r11: Intermediate address for B prefetching.
         * - r9: Column stride of B (cs_b).
         * - 4: Scale factor (assuming 4 columns ahead).
         * - r15 = r11(next NR + offset of b matrix) + 4 * cs_b
         */
		lea(mem(rax, r14, 1, 7*8), r14)
		lea(mem(rbx, r8, 8, 7*8), r11)
		lea(mem(r11, r9, 4), r15)

        /**
         * @brief Clear ZMM vector registers 6 to 28, which were used to store the results of Alpha * A * B.
         *
         * This function initializes zmm6 to zero using the vxorpd instruction and then copies
         * the zeroed value from zmm6 to the other ZMM registers (zmm7 to zmm28) using vmovapd instructions.
         * zmm0, 1, 2 and zmm3, 4, 5 are used to load 24 elements of a matrix of respective columns.
         * zmm30, zmm31 are used to broadcast elements of b matrix of resepctive rows.
         */
        PREPARE_SCRATCHPAD

        label(.DLOOPKITER)
        mov(var(k_iter), rsi)
        sub(imm( 8+TAIL_NITER), rsi)
        jle(.PREFETCHLOOP)
        label(.LOOP1)
            UNROLL_K_LOOP_24x7_L1
            dec(rsi)
        jnz(.LOOP1)

        label(.PREFETCHLOOP)
        add(imm(8), rsi)
        jle(.TAILITER)

        label(.LOOP2)
            UNROLL_K_LOOP_24x7_L2
            dec(rsi)
        jnz(.LOOP2)
        label(.TAILITER)
        add(imm(TAIL_NITER), rsi)
        jle(.TAIL)
        label(.LOOP3)
            UNROLL_K_LOOP_24x7_L1
            dec(rsi)
        jnz(.LOOP3)

        label(.TAIL)
        mov(var(k_left), rsi)
        test(rsi, rsi)
        je(.DPOSTACCUM)
        label(.DLOOPKLEFT)
            vmovupd( mem(rax), zmm0 )
            vmovupd( 0x40(rax), zmm1 )
            vmovupd( 0x80(rax), zmm2 )
            /* a += cs_a */
            add( r10, rax )
            K_LOOP_24x7(0, 1, 2)
            dec(rsi)
        jne(.DLOOPKLEFT)


        label(.DPOSTACCUM)
        POST_ACCUMULATION
        /* if ZF == 1, jump to beta == 0 case */
        je(.DBETAZERO)
        /* set ZF if (8*cs_c) == 8 */
        cmp(imm(8), rdi)
        jz(.DROWSTORED)

        label(.DCOLSTORED)
        STORE_COLSTORED_C_24x7_Bn
        jmp(.DDONE)

        label(.DROWSTORED)
		mov(imm(1), rbx)
		mov(imm(7), r9)
		shlx(r9, rbx, rbx)
		sub(imm(1), rbx)
		kmovw(ebx, k(2))
        /* load address of alpha */
        mov(var(alpha), rdx)
        /* broadcast alpha */
        vbroadcastsd(mem(rdx), zmm30)
        vmulpd( zmm30, zmm6, zmm6 )
        vmulpd( zmm30, zmm7, zmm7 )
        vmulpd( zmm30, zmm28, zmm28 )
        vmulpd( zmm30, zmm8, zmm8 )
        vmulpd( zmm30, zmm9, zmm9 )
        vmulpd( zmm30, zmm29, zmm29 )
        vmulpd( zmm30, zmm10, zmm10 )
        vmulpd( zmm30, zmm11, zmm11 )
        vmulpd( zmm30, zmm26, zmm26 )
        vmulpd( zmm30, zmm12, zmm12 )
        vmulpd( zmm30, zmm13, zmm13 )
        vmulpd( zmm30, zmm27, zmm27 )
        vmulpd( zmm30, zmm14, zmm14 )
        vmulpd( zmm30, zmm15, zmm15 )
        vmulpd( zmm30, zmm24, zmm24 )
        vmulpd( zmm30, zmm16, zmm16 )
        vmulpd( zmm30, zmm17, zmm17 )
        vmulpd( zmm30, zmm25, zmm25 )
        vmulpd( zmm30, zmm18, zmm18 )
        vmulpd( zmm30, zmm19, zmm19 )
        vmulpd( zmm30, zmm22, zmm22 )
        STORE_ROWSTORED_C_24xN_Bn
        jmp(.DDONE)

        label(.DBETAZERO)
        /* set ZF if (8*cs_c) == 8 */
        cmp(imm(8), rdi)
        /* jump to row storage case */
        jz(.DROWSTORBZ)
        label(.DCOLSTORBZ)
        STORE_COLSTORED_C_24x7_B0
        jmp(.DDONE)

        label(.DROWSTORBZ)
	    mov(imm(1), rbx)
		mov(imm(7), r9)
		shlx(r9, rbx, rbx)
		sub(imm(1), rbx)
		kmovw(ebx, k(2))
        /* load address of alpha */
        mov(var(alpha), rdx)
        /* broadcast alpha */
        vbroadcastsd(mem(rdx), zmm30)
        vmulpd( zmm30, zmm6, zmm6 )
        vmulpd( zmm30, zmm7, zmm7 )
        vmulpd( zmm30, zmm28, zmm28 )
        vmulpd( zmm30, zmm8, zmm8 )
        vmulpd( zmm30, zmm9, zmm9 )
        vmulpd( zmm30, zmm29, zmm29 )
        vmulpd( zmm30, zmm10, zmm10 )
        vmulpd( zmm30, zmm11, zmm11 )
        vmulpd( zmm30, zmm26, zmm26 )
        vmulpd( zmm30, zmm12, zmm12 )
        vmulpd( zmm30, zmm13, zmm13 )
        vmulpd( zmm30, zmm27, zmm27 )
        vmulpd( zmm30, zmm14, zmm14 )
        vmulpd( zmm30, zmm15, zmm15 )
        vmulpd( zmm30, zmm24, zmm24 )
        vmulpd( zmm30, zmm16, zmm16 )
        vmulpd( zmm30, zmm17, zmm17 )
        vmulpd( zmm30, zmm25, zmm25 )
        vmulpd( zmm30, zmm18, zmm18 )
        vmulpd( zmm30, zmm19, zmm19 )
        vmulpd( zmm30, zmm22, zmm22 )
        STORE_ROWSTORED_C_24xN_B0
        jmp(.DDONE)

        label(.DDONE)
        POST_K_LOOP
        M_LEFT_24x7

        label(.CONCLUDE)
        vzeroupper()

        end_asm(
          : // output operands (none)
          : // input operands
            [ps_a8] "m" (ps_a8),
            [m_iter] "m" (m_iter),
            [m_left] "m" (m_left),
            [k_iter] "m" (k_iter),
            [k_left] "m" (k_left),
            [a]      "m" (a),
            [rs_a0]   "m" (rs_a0),
            [cs_a0]   "m" (cs_a0),
            [b]      "m" (b),
            [rs_b0]   "m" (rs_b0),
            [cs_b0]   "m" (cs_b0),
            [alpha]  "m" (alpha),
            [beta]   "m" (beta),
            [c]      "m" (c),
            [rs_c0]   "m" (rs_c0),
            [cs_c0]   "m" (cs_c0)
            : // register clobber list
            "rax", "rbx", "rcx", "rdx", "rsi", "rdi",
            "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15",
            "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6",
            "xmm7", "xmm8", "xmm9", "xmm10", "xmm11", "xmm12", "xmm13",
            "xmm14", "xmm15", "xmm16", "xmm17", "xmm18", "xmm19",
            "xmm22", "xmm24", "xmm25", "xmm26", "xmm27", "xmm28",
            "xmm29", "xmm30", "xmm31",
            "ymm0", "ymm1", "ymm2", "ymm3", "ymm4", "ymm5", "ymm6",
            "ymm7", "ymm8", "ymm9", "ymm10", "ymm11", "ymm12", "ymm13",
            "ymm14", "ymm15", "ymm16", "ymm17", "ymm18", "ymm19",
            "ymm22", "ymm24", "ymm25", "ymm26", "ymm27", "ymm28",
            "ymm29", "ymm30", "ymm31",
            "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5", "zmm6",
            "zmm7", "zmm8", "zmm9", "zmm10", "zmm11", "zmm12", "zmm13",
            "zmm14", "zmm15", "zmm16", "zmm17", "zmm18", "zmm19",
            "zmm20", "zmm21", "zmm22", "zmm23", "zmm24", "zmm25",
            "zmm26", "zmm27", "zmm28", "zmm29", "zmm30", "zmm31",
            "k2", "k3",
            "memory"
        )
    } //mloop
    AOCL_DTL_TRACE_EXIT(AOCL_DTL_LEVEL_TRACE_7);
}

/**
 * @brief bli_dgemmsup_cv_zen4_asm_24x8m_new Kernel
 * This kernel performs a double-precision general matrix-matrix multiplication
 * (DGEMM) operation. It processes matrices A, B, and C, where A is MxK, B is KxN, and C is MxN. 
 * The kernel incorporates a JR loop which iterates over the B and C matrices in the N dimension in
 * multiples of the NR block size (8 in this case).
 * 
 * Within this JR loop, the kernel computes MXNR, where M could be the original
 * m input or an MC block, depending on the framework calls. The computation
 * is facilitated by several helper macros:
 * - PREPARE_M_LOOP: [IR Loop] For one iteration of this loop, a block of MRx8 is computed
 * - PRE_K_LOOP: loads all the necessary parameters and matrix offsets needed for computing GEMM
 * - PREPARE_SCRATCHPAD: clear out vector registers zmm6-29
 * 
 * The main computation of A*B is performed by the following macros:
 * - UNROLL_K_LOOP_24x8_L1: Unrolls the K loop by 8, storing results
 * in vector registers zmm6-29.
 * - UNROLL_K_LOOP_24x8_L2: Unrolls the K loop by 8, storing results
 * in vector registers zmm6-29.
 * 
 * After the computation, the kernel checks the storage type of matrix C and
 * uses the appropriate macros to store the results:
 * - STORE_COLSTORED_C_24x8_Bn/0: For column-stored C matrix.
 * - STORE_ROWSTORED_C_24x8_Bn/0: For row-stored C matrix.
 */
void bli_dgemmsup_cv_zen4_asm_24x8m_new
(
       conj_t    conja,
       conj_t    conjb,
       dim_t     m0,
       dim_t     n0,
       dim_t     k0,
       double*    restrict alpha,
#if   BLIS_INT_TYPE_SIZE == 32
       double*    restrict a, inc_t rs_a, inc_t cs_a,
       double*    restrict b, inc_t rs_b, inc_t cs_b,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c, inc_t cs_c,
#else
       double*    restrict a, inc_t rs_a0, inc_t cs_a0,
       double*    restrict b, inc_t rs_b0, inc_t cs_b0,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c0, inc_t cs_c0,
#endif
       auxinfo_t* restrict data,
       cntx_t*    restrict cntx
     )
{
    AOCL_DTL_TRACE_ENTRY(AOCL_DTL_LEVEL_TRACE_7);

    uint64_t n_iter = (uint64_t)n0 / 8;
    uint64_t m_iter = 0;
    uint64_t m_iter_ref = (uint64_t)m0 / 24;
    uint64_t m_left = (uint64_t)m0 % 24;
    uint64_t k_iter = (uint64_t)k0 / 8;
    uint64_t k_left = (uint64_t)k0 % 8;
    double *bb = b;
    double *cc = c;
    double *a_ref = a;
    dim_t iter = 0;
    uint64_t ps_a = bli_auxinfo_ps_a( data );
    uint64_t ps_a8  = ps_a * sizeof( double );

#if   BLIS_INT_TYPE_SIZE == 32
    uint64_t rs_a0   = (uint64_t)rs_a;
    uint64_t cs_a0   = (uint64_t)cs_a;
    uint64_t rs_b0   = (uint64_t)rs_b;
    uint64_t cs_b0   = (uint64_t)cs_b;
    uint64_t rs_c0   = (uint64_t)rs_c;
    uint64_t cs_c0   = (uint64_t)cs_c;
#endif

    /**
     * @brief [JR Loop] For one iteration of this loop, a block of MCxNR is computed
     * This loop moves along n-dimension of c matrix with steps of NR*cs_c,
     * and moves along n-dimension in b matrix with steps of NR*cs_b.
     */
    for(; iter < n_iter; iter++)
    {
        b = bb + iter * 8 *  cs_b0;
        c = cc + iter * 8 *  cs_c0;
        a = a_ref;
        m_iter = m_iter_ref;

        begin_asm()
        /**
         * @brief [IR Loop] For one iteration of this loop, a block of MRxNR is computed
         * This loop moves along m-dimension of c matrix with steps of MR*rs_c,
         * and moves along m-dimension in a matrix with steps of MR*rs_a.
         */
        PREPARE_M_LOOP
        PRE_K_LOOP

        /* panel stride of A; r14 = a * 1*ps_a */
        mov(var(ps_a8), r14)
        /**
         * @brief Prefetch the next panel of matrix A into the cache.
         * This helps in reducing cache misses and improving performance.
         *
         * lea(mem(rax, r14, 1, 7*8), r14)
         * - Load the effective address of the next panel of A into r14.
         * - rax: Base address of A.
         * - r14: Offset to the current panel of A.
         * - 1: Scale factor (no scaling in this case).
         * - 7*8: Offset to the next panel (assuming each panel is 8 bytes and we are moving 7 panels ahead).
         * - r14 = a + ps_a + 7*8
         *
         * lea(mem(rbx, r8, 8, 7*8), r11)
         * - Calculate the address for prefetching B into r11.
         * - rbx: Base address of B.
         * - r8: Row stride of B (rs_b).
         * - 8: Scale factor (assuming 8 bytes per element).
         * - 7*8: Offset to the next set of elements for prefetching.
         * - r11 = b + 8 * rs_b + 7*8(r11 points to next NR + offset(7*8))
         *
         * lea(mem(r11, r9, 4), r15)
         * - Further adjust the address in r11 by adding 4 times the column stride of B (cs_b) and store it in r15.
         * - r11: Intermediate address for B prefetching.
         * - r9: Column stride of B (cs_b).
         * - 4: Scale factor (assuming 4 columns ahead).
         * - r15 = r11(next NR + offset of b matrix) + 4 * cs_b
         */
        lea(mem(rax, r14, 1, 7*8), r14)
        lea(mem(rbx, r8, 8, 7*8), r11)
        lea(mem(r11, r9, 4), r15)

        /**
         * @brief Clear ZMM vector registers 6 to 28, which were used to store the results of Alpha * A * B.
         *
         * This function initializes zmm6 to zero using the vxorpd instruction and then copies
         * the zeroed value from zmm6 to the other ZMM registers (zmm7 to zmm28) using vmovapd instructions.
         * zmm0, 1, 2 and zmm3, 4, 5 are used to load 24 elements of a matrix of respective columns.
         * zmm30, zmm31 are used to broadcast elements of b matrix of resepctive rows.
         */
        PREPARE_SCRATCHPAD

        /**
         * @brief K is unrolled by 8 to facilitate prefetch of B
         * Assuming B to be col-stored, for each iteration of K,
         * one cacheline of B_next is prefetched where b_next = b + (unroll)*rs_b
         */
        /* main loop */
        label(.DLOOPKITER)
        /* i = k_iter */
        mov(var(k_iter), rsi)
        /* i -= NR + TAIL_NITER */
        sub(imm( 8+TAIL_NITER), rsi)
        /* jump if i <= 0 */
        jle(.PREFETCHLOOP)
        /* LOOP1 begins */
        label(.LOOP1)
        UNROLL_K_LOOP_24x8_L1
        dec(rsi)
        jnz(.LOOP1)
        /* LOOP1 ends */

        label(.PREFETCHLOOP)
        /* i += NR */
        add(imm(8), rsi)
        /* jump if i <= 0. */
        jle(.TAILITER)

        /* LOOP2 begins */
        label(.LOOP2)
        UNROLL_K_LOOP_24x8_L2
        dec(rsi)
        jnz(.LOOP2)
        /* LOOP1 ends */

        label(.TAILITER)
        /* i += TAIL_NITER */
        add(imm(TAIL_NITER), rsi)
        /* jump if i <= 0 */
        jle(.TAIL)

        /* LOOP3 begins */
        label(.LOOP3)
        UNROLL_K_LOOP_24x8_L1
        dec(rsi)
        jnz(.LOOP3)
        /* LOOP3 ends */

        label(.TAIL)
        mov(var(k_left), rsi)
        test(rsi, rsi)
        je(.DPOSTACCUM)
        label(.DLOOPKLEFT)
        vmovupd( mem(rax), zmm0 )
        vmovupd( 0x40(rax), zmm1 )
        vmovupd( 0x80(rax), zmm2 )
        /* a += cs_a */
        add( r10, rax )
        K_LOOP_24x8(0, 1, 2)
        dec(rsi)
        jne(.DLOOPKLEFT)

        label(.DPOSTACCUM)
        POST_ACCUMULATION
        /* if ZF == 1, jump to beta == 0 case */
        je(.DBETAZERO)
        /* set ZF if (8*cs_c) == 8 */
        cmp(imm(8), rdi)
        jz(.DROWSTORED)

        label(.DCOLSTORED)
        STORE_COLSTORED_C_24x8_Bn
        /* jump to end */
        jmp(.DDONE)

        label(.DROWSTORED)
        /* load address of alpha */
        mov(var(alpha), rdx)
        /* broadcast alpha */
        vbroadcastsd(mem(rdx), zmm30)
        vmulpd( zmm30, zmm6, zmm6 )
        vmulpd( zmm30, zmm7, zmm7 )
        vmulpd( zmm30, zmm28, zmm28 )
        vmulpd( zmm30, zmm8, zmm8 )
        vmulpd( zmm30, zmm9, zmm9 )
        vmulpd( zmm30, zmm29, zmm29 )
        vmulpd( zmm30, zmm10, zmm10 )
        vmulpd( zmm30, zmm11, zmm11 )
        vmulpd( zmm30, zmm26, zmm26 )
        vmulpd( zmm30, zmm12, zmm12 )
        vmulpd( zmm30, zmm13, zmm13 )
        vmulpd( zmm30, zmm27, zmm27 )
        vmulpd( zmm30, zmm14, zmm14 )
        vmulpd( zmm30, zmm15, zmm15 )
        vmulpd( zmm30, zmm24, zmm24 )
        vmulpd( zmm30, zmm16, zmm16 )
        vmulpd( zmm30, zmm17, zmm17 )
        vmulpd( zmm30, zmm25, zmm25 )
        vmulpd( zmm30, zmm18, zmm18 )
        vmulpd( zmm30, zmm19, zmm19 )
        vmulpd( zmm30, zmm22, zmm22 )
        vmulpd( zmm30, zmm20, zmm20 )
        vmulpd( zmm30, zmm21, zmm21 )
        vmulpd( zmm30, zmm23, zmm23 )
        STORE_ROWSTORED_C_24x8_Bn
        /* jump to end */
        jmp(.DDONE)

        label(.DBETAZERO)
        /* set ZF if (8*cs_c) == 8 */
        cmp(imm(8), rdi)
        /* jump to row storage case */
        jz(.DROWSTORBZ)
        label(.DCOLSTORBZ)
        STORE_COLSTORED_C_24x8_B0
        /* jump to end */
        jmp(.DDONE)

        label(.DROWSTORBZ)
        /* load address of alpha */
        mov(var(alpha), rdx)
        /* broadcast alpha */
        vbroadcastsd(mem(rdx), zmm30)
        vmulpd( zmm30, zmm6, zmm6 )
        vmulpd( zmm30, zmm7, zmm7 )
        vmulpd( zmm30, zmm28, zmm28 )
        vmulpd( zmm30, zmm8, zmm8 )
        vmulpd( zmm30, zmm9, zmm9 )
        vmulpd( zmm30, zmm29, zmm29 )
        vmulpd( zmm30, zmm10, zmm10 )
        vmulpd( zmm30, zmm11, zmm11 )
        vmulpd( zmm30, zmm26, zmm26 )
        vmulpd( zmm30, zmm12, zmm12 )
        vmulpd( zmm30, zmm13, zmm13 )
        vmulpd( zmm30, zmm27, zmm27 )
        vmulpd( zmm30, zmm14, zmm14 )
        vmulpd( zmm30, zmm15, zmm15 )
        vmulpd( zmm30, zmm24, zmm24 )
        vmulpd( zmm30, zmm16, zmm16 )
        vmulpd( zmm30, zmm17, zmm17 )
        vmulpd( zmm30, zmm25, zmm25 )
        vmulpd( zmm30, zmm18, zmm18 )
        vmulpd( zmm30, zmm19, zmm19 )
        vmulpd( zmm30, zmm22, zmm22 )
        vmulpd( zmm30, zmm20, zmm20 )
        vmulpd( zmm30, zmm21, zmm21 )
        vmulpd( zmm30, zmm23, zmm23 )
        STORE_ROWSTORED_C_24x8_B0
        jmp(.DDONE)

        label(.DDONE)

        POST_K_LOOP
        M_LEFT_24x8

        label(.CONCLUDE)
        vzeroupper()

        end_asm(
          : // output operands (none)
          : // input operands
            [ps_a8]  "m" (ps_a8),
            [m_iter] "m" (m_iter),
            [m_left] "m" (m_left),
            [k_iter] "m" (k_iter),
            [k_left] "m" (k_left),
            [a]      "m" (a),
            [rs_a0]  "m" (rs_a0),
            [cs_a0]  "m" (cs_a0),
            [b]      "m" (b),
            [rs_b0]  "m" (rs_b0),
            [cs_b0]  "m" (cs_b0),
            [alpha]  "m" (alpha),
            [beta]   "m" (beta),
            [c]      "m" (c),
            [rs_c0]  "m" (rs_c0),
            [cs_c0]  "m" (cs_c0)
            : // register clobber list
            "rax", "rbx", "rcx", "rdx", "rsi", "rdi",
            "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15",
            "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6",
            "xmm7", "xmm8", "xmm9", "xmm10", "xmm11", "xmm12", "xmm13",
            "xmm14", "xmm15", "xmm16", "xmm17", "xmm18", "xmm19",
            "xmm20", "xmm21", "xmm22", "xmm23", "xmm24", "xmm25",
            "xmm26", "xmm27", "xmm28", "xmm29", "xmm30", "xmm31",
            "ymm0", "ymm1", "ymm2", "ymm3", "ymm4", "ymm5", "ymm6",
            "ymm7", "ymm8", "ymm9", "ymm10", "ymm11", "ymm12", "ymm13",
            "ymm14", "ymm15", "ymm16", "ymm17", "ymm18", "ymm19",
            "ymm20", "ymm21", "ymm22", "ymm23", "ymm24", "ymm25",
            "ymm26", "ymm27", "ymm28", "ymm29", "ymm30", "ymm31",
            "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5", "zmm6",
            "zmm7", "zmm8", "zmm9", "zmm10", "zmm11", "zmm12", "zmm13",
            "zmm14", "zmm15", "zmm16", "zmm17", "zmm18", "zmm19",
            "zmm20", "zmm21", "zmm22", "zmm23", "zmm24", "zmm25",
            "zmm26", "zmm27", "zmm28", "zmm29", "zmm30", "zmm31",
            "k2", "k3", "k4",
            "memory"
        )
    } //mloop

    uint64_t n_left = (uint64_t)n0 % 8;
    // First check whether this is a edge case in the n dimension. If so,
    // dispatch other nx? kernels, as needed
    if( n_left )
    {
        dgemmsup_ker_ft ker_fps[8] =
        {
        NULL,
        bli_dgemmsup_cv_zen4_asm_24x1m_new,
        bli_dgemmsup_cv_zen4_asm_24x2m_new,
        bli_dgemmsup_cv_zen4_asm_24x3m_new,
        bli_dgemmsup_cv_zen4_asm_24x4m_new,
        bli_dgemmsup_cv_zen4_asm_24x5m_new,
        bli_dgemmsup_cv_zen4_asm_24x6m_new,
        bli_dgemmsup_cv_zen4_asm_24x7m_new,
        };

        dgemmsup_ker_ft ker_fp = ker_fps[ n_left ];
        b = bb + iter * 8 *  cs_b0;
        c = cc + iter * 8 *  cs_c0;
        a = a_ref;
        ker_fp
        (
        conja, conjb, m0, n_left, k0,
        alpha, a, rs_a0, cs_a0, b, rs_b0, cs_b0,
        beta, c, rs_c0, cs_c0, data, cntx
        );

        return;
    }
    AOCL_DTL_TRACE_EXIT(AOCL_DTL_LEVEL_TRACE_7);
}

//previous implementation
/* These kernels Assume that A matrix needs to be in col-major order
 * B matrix can be col/row-major
 * C matrix can be col/row-major
 * Prefetch for C is done assuming that C is col-stored.
 * Prefetch of B is done assuming that the matrix is col-stored.
 * Prefetch for B and C matrices when row-stored is yet to be added.
 * Prefetch of A matrix is not done in edge-case kernels.
 */

void bli_dgemmsup_cv_zen4_asm_24x8m
(
       conj_t    conja,
       conj_t    conjb,
       dim_t     m0,
       dim_t     n0,
       dim_t     k0,
       double*    restrict alpha,
       double*    restrict a, inc_t rs_a0, inc_t cs_a0,
       double*    restrict b, inc_t rs_b0, inc_t cs_b0,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c0, inc_t cs_c0,
       auxinfo_t* restrict data,
       cntx_t*    restrict cntx
     )
{
    AOCL_DTL_TRACE_ENTRY(AOCL_DTL_LEVEL_TRACE_7);
    double *abuf = a;
    double *bbuf = b;
    double *cbuf = c;

    // n0 is actually n_left which is calculated at JR loop.
    uint64_t n_left = (uint64_t)n0 % 8;

    // First check whether this is a edge case in the n dimension. If so,
    // dispatch other nx? kernels, as needed
    if( n_left )
    {
        dgemmsup_ker_ft ker_fps[8] =
        {
          NULL,
          bli_dgemmsup_cv_zen4_asm_24x1m,
          bli_dgemmsup_cv_zen4_asm_24x2m,
          bli_dgemmsup_cv_zen4_asm_24x3m,
          bli_dgemmsup_cv_zen4_asm_24x4m,
          bli_dgemmsup_cv_zen4_asm_24x5m,
          bli_dgemmsup_cv_zen4_asm_24x6m,
          bli_dgemmsup_cv_zen4_asm_24x7m,
        };

        dgemmsup_ker_ft ker_fp = ker_fps[ n_left ];

        ker_fp
        (
          conja, conjb, m0, n_left, k0,
          alpha, abuf, rs_a0, cs_a0, bbuf, rs_b0, cs_b0,
          beta, cbuf, rs_c0, cs_c0, data, cntx
        );

        return;
    }

    // Typecast local copies of integers in case dim_t and inc_t are a
    // different size than is expected by load instructions.
    uint64_t m_iter = (uint64_t)m0 / 24;
    uint64_t m_left = (uint64_t)m0 % 24;

    uint64_t rs_a   = rs_a0;
    uint64_t cs_a   = cs_a0;
    uint64_t rs_b   = rs_b0;
    uint64_t cs_b   = cs_b0;
    uint64_t rs_c   = rs_c0;
    uint64_t cs_c   = cs_c0;

    uint64_t ps_a = bli_auxinfo_ps_a( data );
    uint64_t ps_a8  = ps_a * sizeof( double );

    uint64_t k_iter = (uint64_t)k0 / 8;
    uint64_t k_left = (uint64_t)k0 % 8;

    if ( m_iter == 0 ) goto consider_edge_cases;

    /* For one iteration of this loop, a block of MRxNR is computed
     * This loop moves along m-dimension of c matrix with steps of MR*rs_c.
     */
    for(dim_t m=0; m < m_iter; m++)
    {

        a = abuf + m * ps_a ; // Move to next MRXKC in MCXKC (where MC>=MR)
        b = bbuf;  //Same KCXNR is used across different MRXKC in MCXKC
        c = cbuf + m * rs_c * 24; // Move to next MRxNR in MCxNR (where MC >= MR)

        // -------------------------------------------------------------------------
        begin_asm()

        mov(var(a), rax)                // load address of a
        mov(var(cs_a), r10)             // load cs_a
        mov(var(b), rbx)                // load address of b
        mov(var(rs_b), r8)              // load rs_b
        mov(var(cs_b), r9)              // load cs_b
        mov(var(c), rcx)                // load address of c
        mov(var(cs_c), rdi)             // load cs_c
        lea(mem(, r8, 8), r8)           // rs_b *= sizeof(double)
        lea(mem(, r9, 8), r9)           // cs_b *= sizeof(double)
        lea(mem(, r10, 8), r10)         // cs_a *= sizeof(double)
        lea(mem(, rdi, 8), rdi)         // cs_c *= sizeof(double)
        lea(mem(r9, r9, 2 ), r13)       // r13 = 3*cs_b
        // if n > 4, a second pointer(r12) which points to rbx + 4*cs_b
        //is also used to traverse B matrix
        lea(mem(rbx, r9, 4), r12)       // r12 = rbx + 4*cs_b
        lea(mem(rcx, 7*8), rdx)         // C for prefetching
        mov(var(ps_a8), r14)            // panel stride of A
        lea(mem(rax, r14, 1, 7*8), r14) // prefetch next panel of A
        lea(mem(rbx, r8, 8, 7*8), r11)  // r11 = rbx + 8*rs_b(B for prefetching)
        // if n > 4, a second pointer which point to r11 + 4*cs_b
        //is also used to prefetch from B matrix
        lea(mem(r11, r9, 4), r15)       // r15 = r11 + 4* cs_b(B for prefetching)

        /* Register usage: zmm0-5 are used to load A matrix
         *                 zmm6-29 are used for accumulation
         *                 zmm30-31 are used for broadcasting B matrix
         */

        // zero out all accumulation registers
        vxorpd(zmm6, zmm6, zmm6)
        vxorpd(zmm7, zmm7, zmm7)
        vxorpd(zmm28, zmm28, zmm28)
        vxorpd(zmm8, zmm8, zmm8)
        vxorpd(zmm9, zmm9, zmm9)
        vxorpd(zmm29, zmm29, zmm29)
        vxorpd(zmm10, zmm10, zmm10)
        vxorpd(zmm11, zmm11, zmm11)
        vxorpd(zmm26, zmm26, zmm26)
        vxorpd(zmm12, zmm12, zmm12)
        vxorpd(zmm13, zmm13, zmm13)
        vxorpd(zmm27,zmm27, zmm27)
        vxorpd(zmm14, zmm14, zmm14)
        vxorpd(zmm15, zmm15, zmm15)
        vxorpd(zmm24, zmm24, zmm24)
        vxorpd(zmm16, zmm16, zmm16)
        vxorpd(zmm17, zmm17, zmm17)
        vxorpd(zmm25, zmm25, zmm25)
        vxorpd(zmm18, zmm18, zmm18)
        vxorpd(zmm19, zmm19, zmm19)
        vxorpd(zmm22, zmm22, zmm22)
        vxorpd(zmm20, zmm20, zmm20)
        vxorpd(zmm21,zmm21, zmm21)
        vxorpd(zmm23, zmm23, zmm23)

        // K is unrolled by 8 to facilitate prefetch of B
        // Assuming B to be col-stored, for each iteration of K,
        //one cacheline of B_next is prefetched where b_next = b + (unroll)*rs_b
        label(.DLOOPKITER)                                     // main loop
        mov(var(k_iter), rsi)                                  // i = k_iter
        sub(imm( 8+TAIL_NITER), rsi)                           // i -= NR + TAIL_NITER
        jle(.PREFETCHLOOP)                                     // jump if i <= 0

        label(.LOOP1)

            // ---------------------------------- iteration 1

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm20 )
            vfmadd231pd( zmm1,zmm31,zmm21 )
            vfmadd231pd( zmm2,zmm31,zmm23 )

            // ---------------------------------- iteration 2

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm20 )
            vfmadd231pd( zmm4,zmm31,zmm21 )
            vfmadd231pd( zmm5,zmm31,zmm23 )

            // ---------------------------------- iteration 3

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm20 )
            vfmadd231pd( zmm1,zmm31,zmm21 )
            vfmadd231pd( zmm2,zmm31,zmm23 )

            // ---------------------------------- iteration 4

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm20 )
            vfmadd231pd( zmm4,zmm31,zmm21 )
            vfmadd231pd( zmm5,zmm31,zmm23 )

            // ---------------------------------- iteration 5

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm20 )
            vfmadd231pd( zmm1,zmm31,zmm21 )
            vfmadd231pd( zmm2,zmm31,zmm23 )

            // ---------------------------------- iteration 6

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm20 )
            vfmadd231pd( zmm4,zmm31,zmm21 )
            vfmadd231pd( zmm5,zmm31,zmm23 )

            // ---------------------------------- iteration 7

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm20 )
            vfmadd231pd( zmm1,zmm31,zmm21 )
            vfmadd231pd( zmm2,zmm31,zmm23 )

            // ---------------------------------- iteration 8

            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm20 )
            vfmadd231pd( zmm4,zmm31,zmm21 )
            vfmadd231pd( zmm5,zmm31,zmm23 )
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            lea(mem(r15,r8,8), r15)                            // second pointer to b_next += 8*rs_b
            dec(rsi)                                           // i -= 1
        jnz(.LOOP1)                                            // iterate again if i != 0.

        label(.PREFETCHLOOP)
        add(imm(8), rsi)                                       // i += NR
        jle(.TAILITER)                                         // jump if i <= 0.

        label(.LOOP2)

            // ---------------------------------- iteration 1
            prefetchw0( mem(rdx))                              // prefetch C
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm20 )
            vfmadd231pd( zmm1,zmm31,zmm21 )
            vfmadd231pd( zmm2,zmm31,zmm23 )

            // ---------------------------------- iteration 2
            prefetchw0( mem(rdx, 64))                          // prefetch C
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm20 )
            vfmadd231pd( zmm4,zmm31,zmm21 )
            vfmadd231pd( zmm5,zmm31,zmm23 )

            // ---------------------------------- iteration 3
            prefetchw0( mem(rdx, 128))                        // prefetch C
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm20 )
            vfmadd231pd( zmm1,zmm31,zmm21 )
            vfmadd231pd( zmm2,zmm31,zmm23 )

            // ---------------------------------- iteration 4
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm20 )
            vfmadd231pd( zmm4,zmm31,zmm21 )
            vfmadd231pd( zmm5,zmm31,zmm23 )

            // ---------------------------------- iteration 5
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm20 )
            vfmadd231pd( zmm1,zmm31,zmm21 )
            vfmadd231pd( zmm2,zmm31,zmm23 )

            // ---------------------------------- iteration 6
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm20 )
            vfmadd231pd( zmm4,zmm31,zmm21 )
            vfmadd231pd( zmm5,zmm31,zmm23 )

            // ---------------------------------- iteration 7
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm20 )
            vfmadd231pd( zmm1,zmm31,zmm21 )
            vfmadd231pd( zmm2,zmm31,zmm23 )

            // ---------------------------------- iteration 8
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm20 )
            vfmadd231pd( zmm4,zmm31,zmm21 )
            vfmadd231pd( zmm5,zmm31,zmm23 )
            lea(mem(rdx, rdi, 1), rdx)                         // C += cs_c
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            lea(mem(r15,r8,8), r15)                            // second pointer of b_next += 8*rs_b
            sub(imm(1), rsi)                                   // i -= 1
        jnz(.LOOP2)                                            // iterate again if i != 0.
        label(.TAILITER)
        add(imm(TAIL_NITER), rsi)                              // i += TAIL_NITER
        jle(.TAIL)                                             // jump if i <= 0

        label(.LOOP3)

            // ---------------------------------- iteration 1
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm20 )
            vfmadd231pd( zmm1,zmm31,zmm21 )
            vfmadd231pd( zmm2,zmm31,zmm23 )

            // ---------------------------------- iteration 2
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm20 )
            vfmadd231pd( zmm4,zmm31,zmm21 )
            vfmadd231pd( zmm5,zmm31,zmm23 )

            // ---------------------------------- iteration 3
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm20 )
            vfmadd231pd( zmm1,zmm31,zmm21 )
            vfmadd231pd( zmm2,zmm31,zmm23 )

            // ---------------------------------- iteration 4
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm20 )
            vfmadd231pd( zmm4,zmm31,zmm21 )
            vfmadd231pd( zmm5,zmm31,zmm23 )

            // ---------------------------------- iteration 5
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm20 )
            vfmadd231pd( zmm1,zmm31,zmm21 )
            vfmadd231pd( zmm2,zmm31,zmm23 )

            // ---------------------------------- iteration 6
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm20 )
            vfmadd231pd( zmm4,zmm31,zmm21 )
            vfmadd231pd( zmm5,zmm31,zmm23 )

            // ---------------------------------- iteration 7
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm20 )
            vfmadd231pd( zmm1,zmm31,zmm21 )
            vfmadd231pd( zmm2,zmm31,zmm23 )

            // ---------------------------------- iteration 8
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm20 )
            vfmadd231pd( zmm4,zmm31,zmm21 )
            vfmadd231pd( zmm5,zmm31,zmm23 )
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            lea(mem(r15,r8,8), r15)                            // Second pointer of b_next += 8*rs_b
            dec(rsi)                                           // i -= 1
        jnz(.LOOP3)                                            // iterate again if i != 0.


        label(.TAIL)
        mov(var(k_left), rsi)                                  // i = k_left
        test(rsi, rsi)                                         // check i via logical AND
        je(.DPOSTACCUM)                                        // if i == 0, jump to post-accumulation

        label(.DLOOPKLEFT)                                     // k_left loop
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            vbroadcastsd( mem(r12,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm20 )
            vfmadd231pd( zmm1,zmm31,zmm21 )
            vfmadd231pd( zmm2,zmm31,zmm23 )
            dec(rsi)                                           // i -= 1
        jne(.DLOOPKLEFT)                                       // iterate again if i != 0.


        label(.DPOSTACCUM)
        mov(var(alpha), rdx)                                   // load address of alpha
        vbroadcastsd(mem(rdx), zmm30)                           // broadcast alpha
        mov(var(beta), rax)                                    // load address of beta
        vbroadcastsd(mem(rax), zmm31)                           // broadcast beta

        // scale by alpha
        vmulpd( zmm30,zmm6,zmm6 )
        vmulpd( zmm30,zmm7,zmm7 )
        vmulpd( zmm30,zmm28,zmm28 )
        vmulpd( zmm30,zmm8,zmm8 )
        vmulpd( zmm30,zmm9,zmm9 )
        vmulpd( zmm30,zmm29,zmm29 )
        vmulpd( zmm30,zmm10,zmm10 )
        vmulpd( zmm30,zmm11,zmm11 )
        vmulpd( zmm30,zmm26,zmm26 )
        vmulpd( zmm30,zmm12,zmm12 )
        vmulpd( zmm30,zmm13,zmm13 )
        vmulpd( zmm30,zmm27,zmm27 )
        vmulpd( zmm30,zmm14,zmm14 )
        vmulpd( zmm30,zmm15,zmm15 )
        vmulpd( zmm30,zmm24,zmm24 )
        vmulpd( zmm30,zmm16,zmm16 )
        vmulpd( zmm30,zmm17,zmm17 )
        vmulpd( zmm30,zmm25,zmm25 )
        vmulpd( zmm30,zmm18,zmm18 )
        vmulpd( zmm30,zmm19,zmm19 )
        vmulpd( zmm30,zmm22,zmm22 )
        vmulpd( zmm30,zmm20,zmm20 )
        vmulpd( zmm30,zmm21,zmm21 )
        vmulpd( zmm30,zmm23,zmm23 )


        mov(var(rs_c), rsi)                                    // load rs_c
        lea(mem(, rsi, 8), rsi)                                // rsi = rs_c * sizeof(double)
        lea(mem(rcx, rdi, 4), rdx)                             // rdx = rcx + 4 * cs_c
        lea(mem(rdi, rdi, 2), r13)                             // r13 = 3*cs_c
        vxorpd(ymm2, ymm2, ymm2)
        vucomisd(xmm2, xmm31)                                   // set ZF if beta == 0
        je(.DBETAZERO)                                         // if ZF == 1, jump to beta == 0 case


        cmp(imm(8), rdi)                                       // set ZF if (8*cs_c) == 8


        jz(.DROWSTORED)                                        // jump to row storage case

        label(.DCOLSTORED)
        vfmadd231pd( mem(rcx),zmm31,zmm6)
        vmovupd( zmm6,(rcx))
        vfmadd231pd( 0x40(rcx),zmm31,zmm7)
        vmovupd( zmm7,0x40(rcx))
        vfmadd231pd( 0x80(rcx),zmm31,zmm28)
        vmovupd( zmm28,0x80(rcx))
        vfmadd231pd( mem(rcx,rdi,1),zmm31,zmm8)
        vmovupd( zmm8,(rcx,rdi,1))
        vfmadd231pd( 0x40(rcx,rdi,1),zmm31,zmm9)
        vmovupd( zmm9,0x40(rcx,rdi,1))
        vfmadd231pd( 0x80(rcx,rdi,1),zmm31,zmm29)
        vmovupd( zmm29,0x80(rcx,rdi,1))
        vfmadd231pd( mem(rcx,rdi,2),zmm31,zmm10)
        vmovupd( zmm10,(rcx,rdi,2))
        vfmadd231pd( 0x40(rcx,rdi,2),zmm31,zmm11)
        vmovupd( zmm11,0x40(rcx,rdi,2))
        vfmadd231pd( 0x80(rcx,rdi,2),zmm31,zmm26)
        vmovupd( zmm26,0x80(rcx,rdi,2))
        vfmadd231pd( mem(rcx,r13,1),zmm31,zmm12)
        vmovupd( zmm12,(rcx,r13,1))
        vfmadd231pd( 0x40(rcx,r13,1),zmm31,zmm13)
        vmovupd( zmm13,0x40(rcx,r13,1))
        vfmadd231pd( 0x80(rcx,r13,1),zmm31,zmm27)
        vmovupd( zmm27,0x80(rcx,r13,1))
        vfmadd231pd( mem(rdx),zmm31,zmm14)
        vmovupd( zmm14,(rdx))
        vfmadd231pd( 0x40(rdx),zmm31,zmm15)
        vmovupd( zmm15,0x40(rdx))
        vfmadd231pd( 0x80(rdx),zmm31,zmm24)
        vmovupd( zmm24,0x80(rdx))
        vfmadd231pd( mem(rdx,rdi,1),zmm31,zmm16)
        vmovupd( zmm16,(rdx,rdi,1))
        vfmadd231pd( 0x40(rdx,rdi,1),zmm31,zmm17)
        vmovupd( zmm17,0x40(rdx,rdi,1))
        vfmadd231pd( 0x80(rdx,rdi,1),zmm31,zmm25)
        vmovupd( zmm25,0x80(rdx,rdi,1))
        vfmadd231pd( mem(rdx,rdi,2),zmm31,zmm18)
        vmovupd( zmm18,(rdx,rdi,2))
        vfmadd231pd( 0x40(rdx,rdi,2),zmm31,zmm19)
        vmovupd( zmm19,0x40(rdx,rdi,2))
        vfmadd231pd( 0x80(rdx,rdi,2),zmm31,zmm22)
        vmovupd( zmm22,0x80(rdx,rdi,2))
        vfmadd231pd( mem(rdx,r13,1),zmm31,zmm20)
        vmovupd( zmm20,(rdx,r13,1))
        vfmadd231pd( 0x40(rdx,r13,1),zmm31,zmm21)
        vmovupd( zmm21,0x40(rdx,r13,1))
        vfmadd231pd( 0x80(rdx,r13,1),zmm31,zmm23)
        vmovupd( zmm23,0x80(rdx,r13,1))

        jmp(.DDONE)                                           // jump to end.

        label(.DROWSTORED)
        // r12 = 3*rs_c
        lea(mem(rsi,  rsi,  2), r12)
        // r13 = 5*rs_c
        lea(mem(r12, rsi,  2), r13)
        // rdx = 7*rs_c
        lea(mem(r12, rsi,  4), rdx)
        lea(mem(   , rsi, 8), r14)
        UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)

        UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)

        vbroadcastsd(mem(rax), zmm31)
        UPDATE_C
        //First 8x8 tile updated

        UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_C
        //Second 8x8 tile updated

        UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_C
        //Third 8x8 tile updated
        jmp(.DDONE)                                          // jump to end.


        label(.DBETAZERO)
        cmp(imm(8), rdi)                                     // set ZF if (8*cs_c) == 8

        jz(.DROWSTORBZ)                                      // jump to row storage case
        label(.DCOLSTORBZ)
        vmovupd( zmm6,(rcx))
        vmovupd( zmm7,0x40(rcx))
        vmovupd( zmm28,0x80(rcx))
        vmovupd( zmm8,(rcx,rdi,1))
        vmovupd( zmm9,0x40(rcx,rdi,1))
        vmovupd( zmm29,0x80(rcx,rdi,1))
        vmovupd( zmm10,(rcx,rdi,2))
        vmovupd( zmm11,0x40(rcx,rdi,2))
        vmovupd( zmm26,0x80(rcx,rdi,2))
        vmovupd( zmm12,(rcx,r13,1))
        vmovupd( zmm13,0x40(rcx,r13,1))
        vmovupd( zmm27,0x80(rcx,r13,1))
        vmovupd( zmm14,(rdx))
        vmovupd( zmm15,0x40(rdx))
        vmovupd( zmm24,0x80(rdx))
        vmovupd( zmm16,(rdx,rdi,1))
        vmovupd( zmm17,0x40(rdx,rdi,1))
        vmovupd( zmm25,0x80(rdx,rdi,1))
        vmovupd( zmm18,(rdx,rdi,2))
        vmovupd( zmm19,0x40(rdx,rdi,2))
        vmovupd( zmm22,0x80(rdx,rdi,2))
        vmovupd( zmm20,(rdx,r13,1))
        vmovupd( zmm21,0x40(rdx,r13,1))
        vmovupd( zmm23,0x80(rdx,r13,1))

        jmp(.DDONE)                                          // jump to end.


        label(.DROWSTORBZ)
        // r12 = 3*rs_c
        lea(mem(rsi,  rsi,  2), r12)
        // r13 = 5*rs_c
        lea(mem(r12, rsi,  2), r13)
        // rdx = 7*rs_c
        lea(mem(r12, rsi,  4), rdx)
        lea(mem(   , rsi, 8), r14)
        UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)

        UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)

        UPDATE_C_BZ
        //First 8x8 tile updated

        UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_C_BZ
        //Second 8x8 tile updated

        UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_C_BZ
        //Third 8x8 tile updated
        label(.DDONE)


        vzeroupper()

        end_asm(
          : // output operands (none)
          : // input operands
            [k_iter] "m" (k_iter),
            [k_left] "m" (k_left),
            [a]      "m" (a),
            [rs_a]   "m" (rs_a),
            [cs_a]   "m" (cs_a),
            [ps_a8]  "m" (ps_a8),
            [b]      "m" (b),
            [rs_b]   "m" (rs_b),
            [cs_b]   "m" (cs_b),
            [alpha]  "m" (alpha),
            [beta]   "m" (beta),
            [c]      "m" (c),
            [rs_c]   "m" (rs_c),
            [cs_c]   "m" (cs_c)
            : // register clobber list
            "rax", "rbx", "rcx", "rdx", "rsi", "rdi",
            "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15",
            "xmm2", "xmm31",
            "ymm2",
            "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5", "zmm6",
            "zmm7", "zmm8", "zmm9", "zmm10", "zmm11", "zmm12", "zmm13",
            "zmm14", "zmm15", "zmm16", "zmm17", "zmm18", "zmm19",
            "zmm20", "zmm21", "zmm22", "zmm23", "zmm24", "zmm25",
            "zmm26", "zmm27", "zmm28", "zmm29", "zmm30", "zmm31",
            "memory"
        )
    } //mloop

    consider_edge_cases:

    // Handle edge cases in the m dimension, if they exist.
    if (m_left)
    {
        const dim_t nr_cur = 8;
        const dim_t i_edge = m0 - ( dim_t )m_left;
        double *restrict cij = cbuf + i_edge * rs_c;
        double *restrict ai  = abuf + m_iter * ps_a;
        double *restrict bj  = bbuf;
        // covers the range 16 < m_left <= 24 by using masked load/store instructions
        if( 16 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_24x8(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
        // covers the range 8 < m_left <= 16 by using masked load/store instructions
        else if( 8 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_16x8(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
        // covers the range 0 < m_left <= 8 by using masked load/store instructions
        else if( 0 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_8x8(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
    }
    AOCL_DTL_TRACE_EXIT(AOCL_DTL_LEVEL_TRACE_7);
}

void bli_dgemmsup_cv_zen4_asm_24x7m
(
       conj_t    conja,
       conj_t    conjb,
       dim_t     m0,
       dim_t     n0,
       dim_t     k0,
       double*    restrict alpha,
       double*    restrict a, inc_t rs_a0, inc_t cs_a0,
       double*    restrict b, inc_t rs_b0, inc_t cs_b0,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c0, inc_t cs_c0,
       auxinfo_t* restrict data,
       cntx_t*    restrict cntx
     )
{
    AOCL_DTL_TRACE_ENTRY(AOCL_DTL_LEVEL_TRACE_7);
    double *abuf = a;
    double *bbuf = b;
    double *cbuf = c;

    // Typecast local copies of integers in case dim_t and inc_t are a
    // different size than is expected by load instructions.
    uint64_t m_iter = (uint64_t)m0 / 24;
    uint64_t m_left = (uint64_t)m0 % 24;

    uint64_t rs_a   = rs_a0;
    uint64_t cs_a   = cs_a0;
    uint64_t rs_b   = rs_b0;
    uint64_t cs_b   = cs_b0;
    uint64_t rs_c   = rs_c0;
    uint64_t cs_c   = cs_c0;

    uint64_t ps_a = bli_auxinfo_ps_a( data );
    uint64_t ps_a8  = ps_a * sizeof( double );

    uint64_t k_iter = (uint64_t)k0 / 8;
    uint64_t k_left = (uint64_t)k0 % 8;

    uint8_t mask = (0xff >> (0x8 - (n0 & 7))); // calculate mask based on n_left

    if ( m_iter == 0 ) goto consider_edge_cases;

    /* For one iteration of this loop, a block of MRxNR is computed
     * This loop moves along m-dimension of c matrix with steps of MR*rs_c.
     */
    for(dim_t m=0; m < m_iter; m++)
    {

        a = abuf + m * ps_a ; // Move to next MRXKC in MCXKC (where MC>=MR)
        b = bbuf;  //Same KCXNR is used across different MRXKC in MCXKC
        c = cbuf + m * rs_c * 24; // Move to next MRxNR in MCxNR (where MC >= MR)

        // -------------------------------------------------------------------------
        begin_asm()

        mov(var(mask), rdx)             // load mask
        kmovw(edx, k(2))                // move mask to k2 register
        mov(var(a), rax)                // load address of a
        mov(var(cs_a), r10)             // load cs_a
        mov(var(b), rbx)                // load address of b
        mov(var(rs_b), r8)              // load rs_b
        mov(var(cs_b), r9)              // load cs_b
        mov(var(c), rcx)                // load address of c
        mov(var(cs_c), rdi)             // load cs_c
        lea(mem(, r8, 8), r8)           // rs_b *= sizeof(double)
        lea(mem(, r9, 8), r9)           // cs_b *= sizeof(double)
        lea(mem(, r10, 8), r10)         // cs_a *= sizeof(double)
        lea(mem(, rdi, 8), rdi)         // cs_c *= sizeof(double)
        lea(mem(r9, r9, 2 ), r13)       // r13 = 3*cs_b
        // if n > 4, a second pointer(r12) which points to rbx + 4*cs_b
        //is also used to traverse B matrix
        lea(mem(rbx, r9, 4), r12)       // r12 = rbx + 4*cs_b
        lea(mem(rcx, 7*8), rdx)         // C for prefetching
        mov(var(ps_a8), r14)            // panel stride of A
        lea(mem(rax, r14, 1, 7*8), r14) // prefetch next panel of A
        lea(mem(rbx, r8, 8, 7*8), r11)  // r11 = rbx + 8*rs_b(B for prefetching)
        // if n > 4, a second pointer which point to r11 + 4*cs_b
        //is also used to prefetch from B matrix
        lea(mem(r11, r9, 4), r15)       // r15 = r11 + 4* cs_b(B for prefetching)

        /* Register usage: zmm0-5 are used to load A matrix
         *                 zmm6-29 are used for accumulation
         *                 zmm30-31 are used for broadcasting B matrix
         */

        // zero out all accumulation registers
        vxorpd(zmm6, zmm6, zmm6)
        vxorpd(zmm7, zmm7, zmm7)
        vxorpd(zmm28, zmm28, zmm28)
        vxorpd(zmm8, zmm8, zmm8)
        vxorpd(zmm9, zmm9, zmm9)
        vxorpd(zmm29, zmm29, zmm29)
        vxorpd(zmm10, zmm10, zmm10)
        vxorpd(zmm11, zmm11, zmm11)
        vxorpd(zmm26, zmm26, zmm26)
        vxorpd(zmm12, zmm12, zmm12)
        vxorpd(zmm13, zmm13, zmm13)
        vxorpd(zmm27,zmm27, zmm27)
        vxorpd(zmm14, zmm14, zmm14)
        vxorpd(zmm15, zmm15, zmm15)
        vxorpd(zmm24, zmm24, zmm24)
        vxorpd(zmm16, zmm16, zmm16)
        vxorpd(zmm17, zmm17, zmm17)
        vxorpd(zmm25, zmm25, zmm25)
        vxorpd(zmm18, zmm18, zmm18)
        vxorpd(zmm19, zmm19, zmm19)
        vxorpd(zmm22, zmm22, zmm22)

        // K is unrolled by 8 to facilitate prefetch of B
        // Assuming B to be col-stored, for each iteration of K,
        //one cacheline of B_next is prefetched where b_next = b + (unroll)*rs_b
        label(.DLOOPKITER)                                     // main loop
        mov(var(k_iter), rsi)                                  // i = k_iter
        sub(imm( 7+TAIL_NITER), rsi)                           // i -= NR + TAIL_NITER
        jle(.PREFETCHLOOP)                                     // jump if i <= 0

        label(.LOOP1)

            // ---------------------------------- iteration 1

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )

            // ---------------------------------- iteration 2

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 3

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )

            // ---------------------------------- iteration 4

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 5

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )

            // ---------------------------------- iteration 6

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 7

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )

            // ---------------------------------- iteration 8

            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            lea(mem(r15,r8,8), r15)                            // second pointer to b_next += 8*rs_b
            dec(rsi)                                           // i -= 1
        jnz(.LOOP1)                                            // iterate again if i != 0.

        label(.PREFETCHLOOP)
        add(imm(7), rsi)                                       // i += NR
        jle(.TAILITER)                                         // jump if i <= 0.

        label(.LOOP2)

            // ---------------------------------- iteration 1
            prefetchw0( mem(rdx))                              // prefetch C
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )

            // ---------------------------------- iteration 2
            prefetchw0( mem(rdx, 64))                          // prefetch C
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 3
            prefetchw0( mem(rdx, 128))                        // prefetch C
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )

            // ---------------------------------- iteration 4
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 5
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )

            // ---------------------------------- iteration 6
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 7
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )

            // ---------------------------------- iteration 8
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            lea(mem(rdx, rdi, 1), rdx)                         // C += cs_c
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            lea(mem(r15,r8,8), r15)                            // second pointer of b_next += 8*rs_b
            sub(imm(1), rsi)                                   // i -= 1
        jnz(.LOOP2)                                            // iterate again if i != 0.
        label(.TAILITER)
        add(imm(TAIL_NITER), rsi)                              // i += TAIL_NITER
        jle(.TAIL)                                             // jump if i <= 0

        label(.LOOP3)

            // ---------------------------------- iteration 1
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )

            // ---------------------------------- iteration 2
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 3
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )

            // ---------------------------------- iteration 4
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 5
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )

            // ---------------------------------- iteration 6
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 7
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )

            // ---------------------------------- iteration 8
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm18 )
            vfmadd231pd( zmm4,zmm30,zmm19 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            lea(mem(r15,r8,8), r15)                            // Second pointer of b_next += 8*rs_b
            dec(rsi)                                           // i -= 1
        jnz(.LOOP3)                                            // iterate again if i != 0.


        label(.TAIL)
        mov(var(k_left), rsi)                                  // i = k_left
        test(rsi, rsi)                                         // check i via logical AND
        je(.DPOSTACCUM)                                        // if i == 0, jump to post-accumulation

        label(.DLOOPKLEFT)                                     // k_left loop
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            vbroadcastsd( mem(r12,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm18 )
            vfmadd231pd( zmm1,zmm30,zmm19 )
            vfmadd231pd( zmm2,zmm30,zmm22 )
            dec(rsi)                                           // i -= 1
        jne(.DLOOPKLEFT)                                       // iterate again if i != 0.


        label(.DPOSTACCUM)
        mov(var(alpha), rdx)                                   // load address of alpha
        vbroadcastsd(mem(rdx), zmm30)                           // broadcast alpha
        mov(var(beta), rax)                                    // load address of beta
        vbroadcastsd(mem(rax), zmm31)                           // broadcast beta

        // scale by alpha
        vmulpd( zmm30,zmm6,zmm6 )
        vmulpd( zmm30,zmm7,zmm7 )
        vmulpd( zmm30,zmm28,zmm28 )
        vmulpd( zmm30,zmm8,zmm8 )
        vmulpd( zmm30,zmm9,zmm9 )
        vmulpd( zmm30,zmm29,zmm29 )
        vmulpd( zmm30,zmm10,zmm10 )
        vmulpd( zmm30,zmm11,zmm11 )
        vmulpd( zmm30,zmm26,zmm26 )
        vmulpd( zmm30,zmm12,zmm12 )
        vmulpd( zmm30,zmm13,zmm13 )
        vmulpd( zmm30,zmm27,zmm27 )
        vmulpd( zmm30,zmm14,zmm14 )
        vmulpd( zmm30,zmm15,zmm15 )
        vmulpd( zmm30,zmm24,zmm24 )
        vmulpd( zmm30,zmm16,zmm16 )
        vmulpd( zmm30,zmm17,zmm17 )
        vmulpd( zmm30,zmm25,zmm25 )
        vmulpd( zmm30,zmm18,zmm18 )
        vmulpd( zmm30,zmm19,zmm19 )
        vmulpd( zmm30,zmm22,zmm22 )


        mov(var(rs_c), rsi)                                    // load rs_c
        lea(mem(, rsi, 8), rsi)                                // rsi = rs_c * sizeof(double)
        lea(mem(rcx, rdi, 4), rdx)                             // rdx = rcx + 4 * cs_c
        lea(mem(rdi, rdi, 2), r13)                             // r13 = 3*cs_c
        vxorpd(ymm2, ymm2, ymm2)
        vucomisd(xmm2, xmm31)                                   // set ZF if beta == 0
        je(.DBETAZERO)                                         // if ZF == 1, jump to beta == 0 case


        cmp(imm(8), rdi)                                       // set ZF if (8*cs_c) == 8


        jz(.DROWSTORED)                                        // jump to row storage case

        label(.DCOLSTORED)
        vfmadd231pd( mem(rcx),zmm31,zmm6)
        vmovupd( zmm6,(rcx))
        vfmadd231pd( 0x40(rcx),zmm31,zmm7)
        vmovupd( zmm7,0x40(rcx))
        vfmadd231pd( 0x80(rcx),zmm31,zmm28)
        vmovupd( zmm28,0x80(rcx))
        vfmadd231pd( mem(rcx,rdi,1),zmm31,zmm8)
        vmovupd( zmm8,(rcx,rdi,1))
        vfmadd231pd( 0x40(rcx,rdi,1),zmm31,zmm9)
        vmovupd( zmm9,0x40(rcx,rdi,1))
        vfmadd231pd( 0x80(rcx,rdi,1),zmm31,zmm29)
        vmovupd( zmm29,0x80(rcx,rdi,1))
        vfmadd231pd( mem(rcx,rdi,2),zmm31,zmm10)
        vmovupd( zmm10,(rcx,rdi,2))
        vfmadd231pd( 0x40(rcx,rdi,2),zmm31,zmm11)
        vmovupd( zmm11,0x40(rcx,rdi,2))
        vfmadd231pd( 0x80(rcx,rdi,2),zmm31,zmm26)
        vmovupd( zmm26,0x80(rcx,rdi,2))
        vfmadd231pd( mem(rcx,r13,1),zmm31,zmm12)
        vmovupd( zmm12,(rcx,r13,1))
        vfmadd231pd( 0x40(rcx,r13,1),zmm31,zmm13)
        vmovupd( zmm13,0x40(rcx,r13,1))
        vfmadd231pd( 0x80(rcx,r13,1),zmm31,zmm27)
        vmovupd( zmm27,0x80(rcx,r13,1))
        vfmadd231pd( mem(rdx),zmm31,zmm14)
        vmovupd( zmm14,(rdx))
        vfmadd231pd( 0x40(rdx),zmm31,zmm15)
        vmovupd( zmm15,0x40(rdx))
        vfmadd231pd( 0x80(rdx),zmm31,zmm24)
        vmovupd( zmm24,0x80(rdx))
        vfmadd231pd( mem(rdx,rdi,1),zmm31,zmm16)
        vmovupd( zmm16,(rdx,rdi,1))
        vfmadd231pd( 0x40(rdx,rdi,1),zmm31,zmm17)
        vmovupd( zmm17,0x40(rdx,rdi,1))
        vfmadd231pd( 0x80(rdx,rdi,1),zmm31,zmm25)
        vmovupd( zmm25,0x80(rdx,rdi,1))
        vfmadd231pd( mem(rdx,rdi,2),zmm31,zmm18)
        vmovupd( zmm18,(rdx,rdi,2))
        vfmadd231pd( 0x40(rdx,rdi,2),zmm31,zmm19)
        vmovupd( zmm19,0x40(rdx,rdi,2))
        vfmadd231pd( 0x80(rdx,rdi,2),zmm31,zmm22)
        vmovupd( zmm22,0x80(rdx,rdi,2))

        jmp(.DDONE)                                           // jump to end.

        label(.DROWSTORED)
        // r12 = 3*rs_c
        lea(mem(rsi,  rsi,  2), r12)
        // r13 = 5*rs_c
        lea(mem(r12, rsi,  2), r13)
        // rdx = 7*rs_c
        lea(mem(r12, rsi,  4), rdx)
        lea(mem(   , rsi, 8), r14)
        UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)

        UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)

        vbroadcastsd(mem(rax), zmm31)
        UPDATE_MASKED_C
        //First 8x7 tile updated

        UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C
        //Second 8x7 tile updated

        UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C
        //Third 8x7 tile updated
        jmp(.DDONE)                                          // jump to end.


        label(.DBETAZERO)
        cmp(imm(8), rdi)                                     // set ZF if (8*cs_c) == 8

        jz(.DROWSTORBZ)                                      // jump to row storage case
        label(.DCOLSTORBZ)
        vmovupd( zmm6,(rcx))
        vmovupd( zmm7,0x40(rcx))
        vmovupd( zmm28,0x80(rcx))
        vmovupd( zmm8,(rcx,rdi,1))
        vmovupd( zmm9,0x40(rcx,rdi,1))
        vmovupd( zmm29,0x80(rcx,rdi,1))
        vmovupd( zmm10,(rcx,rdi,2))
        vmovupd( zmm11,0x40(rcx,rdi,2))
        vmovupd( zmm26,0x80(rcx,rdi,2))
        vmovupd( zmm12,(rcx,r13,1))
        vmovupd( zmm13,0x40(rcx,r13,1))
        vmovupd( zmm27,0x80(rcx,r13,1))
        vmovupd( zmm14,(rdx))
        vmovupd( zmm15,0x40(rdx))
        vmovupd( zmm24,0x80(rdx))
        vmovupd( zmm16,(rdx,rdi,1))
        vmovupd( zmm17,0x40(rdx,rdi,1))
        vmovupd( zmm25,0x80(rdx,rdi,1))
        vmovupd( zmm18,(rdx,rdi,2))
        vmovupd( zmm19,0x40(rdx,rdi,2))
        vmovupd( zmm22,0x80(rdx,rdi,2))

        jmp(.DDONE)                                          // jump to end.


        label(.DROWSTORBZ)
        // r12 = 3*rs_c
        lea(mem(rsi,  rsi, 2), r12)
        // r13 = 5*rs_c
        lea(mem(r12, rsi,  2), r13)
        // rdx = 7*rs_c
        lea(mem(r12, rsi,  4), rdx)
        lea(mem(   , rsi,  8), r14)
        UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)

        UNPACK_LO_HIGH(16, 14, 0, 1, 20, 18, 2, 3)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)

        UPDATE_MASKED_C_BZ
        //First 8x7 tile updated

        UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        UNPACK_LO_HIGH(17, 15, 0, 1, 21, 19, 2, 3)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C_BZ
        //Second 8x7 tile updated

        UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        UNPACK_LO_HIGH(25, 24, 0, 1, 23, 22, 2, 3)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C_BZ
        //Third 8x7 tile updated
        label(.DDONE)


        vzeroupper()

        end_asm(
          : // output operands (none)
          : // input operands
            [k_iter] "m" (k_iter),
            [k_left] "m" (k_left),
            [a]      "m" (a),
            [rs_a]   "m" (rs_a),
            [cs_a]   "m" (cs_a),
            [ps_a8]  "m" (ps_a8),
            [b]      "m" (b),
            [rs_b]   "m" (rs_b),
            [cs_b]   "m" (cs_b),
            [alpha]  "m" (alpha),
            [beta]   "m" (beta),
            [c]      "m" (c),
            [rs_c]   "m" (rs_c),
            [cs_c]   "m" (cs_c),
            [mask]   "m" (mask)
          : // register clobber list
            "rax", "rbx", "rcx", "rdx", "rsi", "rdi",
            "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15",
            "xmm2", "xmm31",
            "ymm2",
            "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5", "zmm6",
            "zmm7", "zmm8", "zmm9", "zmm10", "zmm11", "zmm12", "zmm13",
            "zmm14", "zmm15", "zmm16", "zmm17", "zmm18", "zmm19",
            "zmm20", "zmm21", "zmm22", "zmm23", "zmm24", "zmm25",
            "zmm26", "zmm27", "zmm28", "zmm29", "zmm30", "zmm31",
            "k2",
            "memory"
        )
    } //mloop

    consider_edge_cases:

    // Handle edge cases in the m dimension, if they exist.
    if (m_left)
    {
        const dim_t nr_cur = 7;
        const dim_t i_edge = m0 - ( dim_t )m_left;
        double *restrict cij = cbuf + i_edge * rs_c;
        double *restrict ai  = abuf + m_iter * ps_a;
        double *restrict bj  = bbuf;
        // covers the range 16 < m_left <= 24 by using masked load/store instructions
        if( 16 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_24x7(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
        // covers the range 8 < m_left <= 16 by using masked load/store instructions
        else if( 8 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_16x7(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
        // covers the range 0 < m_left <= 8 by using masked load/store instructions
        else if( 0 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_8x7(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
    }
    AOCL_DTL_TRACE_EXIT(AOCL_DTL_LEVEL_TRACE_7);
}

void bli_dgemmsup_cv_zen4_asm_24x6m
(
       conj_t    conja,
       conj_t    conjb,
       dim_t     m0,
       dim_t     n0,
       dim_t     k0,
       double*    restrict alpha,
       double*    restrict a, inc_t rs_a0, inc_t cs_a0,
       double*    restrict b, inc_t rs_b0, inc_t cs_b0,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c0, inc_t cs_c0,
       auxinfo_t* restrict data,
       cntx_t*    restrict cntx
     )
{
    AOCL_DTL_TRACE_ENTRY(AOCL_DTL_LEVEL_TRACE_7);
    double *abuf = a;
    double *bbuf = b;
    double *cbuf = c;

    // Typecast local copies of integers in case dim_t and inc_t are a
    // different size than is expected by load instructions.
    uint64_t m_iter = (uint64_t)m0 / 24;
    uint64_t m_left = (uint64_t)m0 % 24;

    uint64_t rs_a   = rs_a0;
    uint64_t cs_a   = cs_a0;
    uint64_t rs_b   = rs_b0;
    uint64_t cs_b   = cs_b0;
    uint64_t rs_c   = rs_c0;
    uint64_t cs_c   = cs_c0;

    uint64_t ps_a = bli_auxinfo_ps_a( data );
    uint64_t ps_a8  = ps_a * sizeof( double );

    uint64_t k_iter = (uint64_t)k0 / 8;
    uint64_t k_left = (uint64_t)k0 % 8;

    uint8_t mask = (0xff >> (0x8 - (n0 & 7))); // calculate mask based on n_left

    if ( m_iter == 0 ) goto consider_edge_cases;

    /* For one iteration of this loop, a block of MRxNR is computed
     * This loop moves along m-dimension of c matrix with steps of MR*rs_c.
     */
    for(dim_t m=0; m < m_iter; m++)
    {

        a = abuf + m * ps_a ; // Move to next MRXKC in MCXKC (where MC>=MR)
        b = bbuf;  //Same KCXNR is used across different MRXKC in MCXKC
        c = cbuf + m * rs_c * 24; // Move to next MRxNR in MCxNR (where MC >= MR)

        // -------------------------------------------------------------------------
        begin_asm()

        mov(var(mask), rdx)             // load mask
        kmovw(edx, k(2))                // move mask to k2 register
        mov(var(a), rax)                // load address of a
        mov(var(cs_a), r10)             // load cs_a
        mov(var(b), rbx)                // load address of b
        mov(var(rs_b), r8)              // load rs_b
        mov(var(cs_b), r9)              // load cs_b
        mov(var(c), rcx)                // load address of c
        mov(var(cs_c), rdi)             // load cs_c
        lea(mem(, r8, 8), r8)           // rs_b *= sizeof(double)
        lea(mem(, r9, 8), r9)           // cs_b *= sizeof(double)
        lea(mem(, r10, 8), r10)         // cs_a *= sizeof(double)
        lea(mem(, rdi, 8), rdi)         // cs_c *= sizeof(double)
        lea(mem(r9, r9, 2 ), r13)       // r13 = 3*cs_b
        // if n > 4, a second pointer(r12) which points to rbx + 4*cs_b
        //is also used to traverse B matrix
        lea(mem(rbx, r9, 4), r12)       // r12 = rbx + 4*cs_b
        lea(mem(rcx, 7*8), rdx)         // C for prefetching
        mov(var(ps_a8), r14)            // panel stride of A
        lea(mem(rax, r14, 1, 7*8), r14) // prefetch next panel of A
        lea(mem(rbx, r8, 8, 7*8), r11)  // r11 = rbx + 8*rs_b(B for prefetching)
        // if n > 4, a second pointer which point to r11 + 4*cs_b
        //is also used to prefetch from B matrix
        lea(mem(r11, r9, 4), r15)       // r15 = r11 + 4* cs_b(B for prefetching)

        /* Register usage: zmm0-5 are used to load A matrix
         *                 zmm6-29 are used for accumulation
         *                 zmm30-31 are used for broadcasting B matrix
         */

        // zero out all accumulation registers
        vxorpd(zmm6, zmm6, zmm6)
        vxorpd(zmm7, zmm7, zmm7)
        vxorpd(zmm28, zmm28, zmm28)
        vxorpd(zmm8, zmm8, zmm8)
        vxorpd(zmm9, zmm9, zmm9)
        vxorpd(zmm29, zmm29, zmm29)
        vxorpd(zmm10, zmm10, zmm10)
        vxorpd(zmm11, zmm11, zmm11)
        vxorpd(zmm26, zmm26, zmm26)
        vxorpd(zmm12, zmm12, zmm12)
        vxorpd(zmm13, zmm13, zmm13)
        vxorpd(zmm27,zmm27, zmm27)
        vxorpd(zmm14, zmm14, zmm14)
        vxorpd(zmm15, zmm15, zmm15)
        vxorpd(zmm24, zmm24, zmm24)
        vxorpd(zmm16, zmm16, zmm16)
        vxorpd(zmm17, zmm17, zmm17)
        vxorpd(zmm25, zmm25, zmm25)

        // K is unrolled by 8 to facilitate prefetch of B
        // Assuming B to be col-stored, for each iteration of K,
        //one cacheline of B_next is prefetched where b_next = b + (unroll)*rs_b
        label(.DLOOPKITER)                                     // main loop
        mov(var(k_iter), rsi)                                  // i = k_iter
        sub(imm( 6+TAIL_NITER), rsi)                           // i -= NR + TAIL_NITER
        jle(.PREFETCHLOOP)                                     // jump if i <= 0

        label(.LOOP1)

            // ---------------------------------- iteration 1

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )

            // ---------------------------------- iteration 2

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 3

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )

            // ---------------------------------- iteration 4

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 5

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )

            // ---------------------------------- iteration 6

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 7

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )

            // ---------------------------------- iteration 8

            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            lea(mem(r15,r8,8), r15)                            // second pointer to b_next += 8*rs_b
            dec(rsi)                                           // i -= 1
        jnz(.LOOP1)                                            // iterate again if i != 0.

        label(.PREFETCHLOOP)
        add(imm(6), rsi)                                       // i += NR
        jle(.TAILITER)                                         // jump if i <= 0.

        label(.LOOP2)

            // ---------------------------------- iteration 1
            prefetchw0( mem(rdx))                              // prefetch C
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )

            // ---------------------------------- iteration 2
            prefetchw0( mem(rdx, 64))                          // prefetch C
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 3
            prefetchw0( mem(rdx, 128))                        // prefetch C
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )

            // ---------------------------------- iteration 4
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 5
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )

            // ---------------------------------- iteration 6
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 7
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )

            // ---------------------------------- iteration 8
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            lea(mem(rdx, rdi, 1), rdx)                         // C += cs_c
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            lea(mem(r15,r8,8), r15)                            // second pointer of b_next += 8*rs_b
            sub(imm(1), rsi)                                   // i -= 1
        jnz(.LOOP2)                                            // iterate again if i != 0.
        label(.TAILITER)
        add(imm(TAIL_NITER), rsi)                              // i += TAIL_NITER
        jle(.TAIL)                                             // jump if i <= 0

        label(.LOOP3)

            // ---------------------------------- iteration 1
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )

            // ---------------------------------- iteration 2
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 3
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )

            // ---------------------------------- iteration 4
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 5
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )

            // ---------------------------------- iteration 6
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 7
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )

            // ---------------------------------- iteration 8
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm16 )
            vfmadd231pd( zmm4,zmm31,zmm17 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            lea(mem(r15,r8,8), r15)                            // Second pointer of b_next += 8*rs_b
            dec(rsi)                                           // i -= 1
        jnz(.LOOP3)                                            // iterate again if i != 0.


        label(.TAIL)
        mov(var(k_left), rsi)                                  // i = k_left
        test(rsi, rsi)                                         // check i via logical AND
        je(.DPOSTACCUM)                                        // if i == 0, jump to post-accumulation

        label(.DLOOPKLEFT)                                     // k_left loop
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            vbroadcastsd( mem(r12,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm16 )
            vfmadd231pd( zmm1,zmm31,zmm17 )
            vfmadd231pd( zmm2,zmm31,zmm25 )
            dec(rsi)                                           // i -= 1
        jne(.DLOOPKLEFT)                                       // iterate again if i != 0.


        label(.DPOSTACCUM)
        mov(var(alpha), rdx)                                   // load address of alpha
        vbroadcastsd(mem(rdx), zmm30)                           // broadcast alpha
        mov(var(beta), rax)                                    // load address of beta
        vbroadcastsd(mem(rax), zmm31)                           // broadcast beta

        // scale by alpha
        vmulpd( zmm30,zmm6,zmm6 )
        vmulpd( zmm30,zmm7,zmm7 )
        vmulpd( zmm30,zmm28,zmm28 )
        vmulpd( zmm30,zmm8,zmm8 )
        vmulpd( zmm30,zmm9,zmm9 )
        vmulpd( zmm30,zmm29,zmm29 )
        vmulpd( zmm30,zmm10,zmm10 )
        vmulpd( zmm30,zmm11,zmm11 )
        vmulpd( zmm30,zmm26,zmm26 )
        vmulpd( zmm30,zmm12,zmm12 )
        vmulpd( zmm30,zmm13,zmm13 )
        vmulpd( zmm30,zmm27,zmm27 )
        vmulpd( zmm30,zmm14,zmm14 )
        vmulpd( zmm30,zmm15,zmm15 )
        vmulpd( zmm30,zmm24,zmm24 )
        vmulpd( zmm30,zmm16,zmm16 )
        vmulpd( zmm30,zmm17,zmm17 )
        vmulpd( zmm30,zmm25,zmm25 )


        mov(var(rs_c), rsi)                                    // load rs_c
        lea(mem(, rsi, 8), rsi)                                // rsi = rs_c * sizeof(double)
        lea(mem(rcx, rdi, 4), rdx)                             // rdx = rcx + 4 * cs_c
        lea(mem(rdi, rdi, 2), r13)                             // r13 = 3*cs_c
        vxorpd(ymm2, ymm2, ymm2)
        vucomisd(xmm2, xmm31)                                   // set ZF if beta == 0
        je(.DBETAZERO)                                         // if ZF == 1, jump to beta == 0 case


        cmp(imm(8), rdi)                                       // set ZF if (8*cs_c) == 8


        jz(.DROWSTORED)                                        // jump to row storage case

        label(.DCOLSTORED)
        vfmadd231pd( mem(rcx),zmm31,zmm6)
        vmovupd( zmm6,(rcx))
        vfmadd231pd( 0x40(rcx),zmm31,zmm7)
        vmovupd( zmm7,0x40(rcx))
        vfmadd231pd( 0x80(rcx),zmm31,zmm28)
        vmovupd( zmm28,0x80(rcx))
        vfmadd231pd( mem(rcx,rdi,1),zmm31,zmm8)
        vmovupd( zmm8,(rcx,rdi,1))
        vfmadd231pd( 0x40(rcx,rdi,1),zmm31,zmm9)
        vmovupd( zmm9,0x40(rcx,rdi,1))
        vfmadd231pd( 0x80(rcx,rdi,1),zmm31,zmm29)
        vmovupd( zmm29,0x80(rcx,rdi,1))
        vfmadd231pd( mem(rcx,rdi,2),zmm31,zmm10)
        vmovupd( zmm10,(rcx,rdi,2))
        vfmadd231pd( 0x40(rcx,rdi,2),zmm31,zmm11)
        vmovupd( zmm11,0x40(rcx,rdi,2))
        vfmadd231pd( 0x80(rcx,rdi,2),zmm31,zmm26)
        vmovupd( zmm26,0x80(rcx,rdi,2))
        vfmadd231pd( mem(rcx,r13,1),zmm31,zmm12)
        vmovupd( zmm12,(rcx,r13,1))
        vfmadd231pd( 0x40(rcx,r13,1),zmm31,zmm13)
        vmovupd( zmm13,0x40(rcx,r13,1))
        vfmadd231pd( 0x80(rcx,r13,1),zmm31,zmm27)
        vmovupd( zmm27,0x80(rcx,r13,1))
        vfmadd231pd( mem(rdx),zmm31,zmm14)
        vmovupd( zmm14,(rdx))
        vfmadd231pd( 0x40(rdx),zmm31,zmm15)
        vmovupd( zmm15,0x40(rdx))
        vfmadd231pd( 0x80(rdx),zmm31,zmm24)
        vmovupd( zmm24,0x80(rdx))
        vfmadd231pd( mem(rdx,rdi,1),zmm31,zmm16)
        vmovupd( zmm16,(rdx,rdi,1))
        vfmadd231pd( 0x40(rdx,rdi,1),zmm31,zmm17)
        vmovupd( zmm17,0x40(rdx,rdi,1))
        vfmadd231pd( 0x80(rdx,rdi,1),zmm31,zmm25)
        vmovupd( zmm25,0x80(rdx,rdi,1))

        jmp(.DDONE)                                           // jump to end.

        label(.DROWSTORED)
        // r12 = 3*rs_c
        lea(mem(rsi,  rsi,  2), r12)
        // r13 = 5*rs_c
        lea(mem(r12, rsi,  2), r13)
        // rdx = 7*rs_c
        lea(mem(r12, rsi,  4), rdx)
        lea(mem(   , rsi, 8), r14)
        UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)

        vunpcklpd(zmm16, zmm14, zmm0)
        vunpckhpd(zmm16, zmm14, zmm1)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)

        vbroadcastsd(mem(rax), zmm31)
        UPDATE_MASKED_C
        //First 8x6 tile updated

        UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        vunpcklpd(zmm17, zmm15, zmm0)
        vunpckhpd(zmm17, zmm15, zmm1)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C
        //Second 8x6 tile updated

        UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        vunpcklpd(zmm25, zmm24, zmm0)
        vunpckhpd(zmm25, zmm24, zmm1)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C
        //Third 8x6 tile updated
        jmp(.DDONE)                                          // jump to end.


        label(.DBETAZERO)
        cmp(imm(8), rdi)                                     // set ZF if (8*cs_c) == 8

        jz(.DROWSTORBZ)                                      // jump to row storage case
        label(.DCOLSTORBZ)
        vmovupd( zmm6,(rcx))
        vmovupd( zmm7,0x40(rcx))
        vmovupd( zmm28,0x80(rcx))
        vmovupd( zmm8,(rcx,rdi,1))
        vmovupd( zmm9,0x40(rcx,rdi,1))
        vmovupd( zmm29,0x80(rcx,rdi,1))
        vmovupd( zmm10,(rcx,rdi,2))
        vmovupd( zmm11,0x40(rcx,rdi,2))
        vmovupd( zmm26,0x80(rcx,rdi,2))
        vmovupd( zmm12,(rcx,r13,1))
        vmovupd( zmm13,0x40(rcx,r13,1))
        vmovupd( zmm27,0x80(rcx,r13,1))
        vmovupd( zmm14,(rdx))
        vmovupd( zmm15,0x40(rdx))
        vmovupd( zmm24,0x80(rdx))
        vmovupd( zmm16,(rdx,rdi,1))
        vmovupd( zmm17,0x40(rdx,rdi,1))
        vmovupd( zmm25,0x80(rdx,rdi,1))

        jmp(.DDONE)                                          // jump to end.


        label(.DROWSTORBZ)
        // r12 = 3*rs_c
        lea(mem(rsi,  rsi,  2), r12)
        // r13 = 5*rs_c
        lea(mem(r12, rsi,  2), r13)
        // rdx = 7*rs_c
        lea(mem(r12, rsi,  4), rdx)
        lea(mem(   , rsi, 8), r14)
        UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)

        vunpcklpd(zmm16, zmm14, zmm0)
        vunpckhpd(zmm16, zmm14, zmm1)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)

        UPDATE_MASKED_C_BZ
        //First 8x6 tile updated

        UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        vunpcklpd(zmm17, zmm15, zmm0)
        vunpckhpd(zmm17, zmm15, zmm1)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C_BZ
        //Second 8x6 tile updated

        UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        vunpcklpd(zmm25, zmm24, zmm0)
        vunpckhpd(zmm25, zmm24, zmm1)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C_BZ
        //Third 8x6 tile updated
        label(.DDONE)


        vzeroupper()

        end_asm(
          : // output operands (none)
          : // input operands
            [k_iter] "m" (k_iter),
            [k_left] "m" (k_left),
            [a]      "m" (a),
            [rs_a]   "m" (rs_a),
            [cs_a]   "m" (cs_a),
            [ps_a8]  "m" (ps_a8),
            [b]      "m" (b),
            [rs_b]   "m" (rs_b),
            [cs_b]   "m" (cs_b),
            [alpha]  "m" (alpha),
            [beta]   "m" (beta),
            [c]      "m" (c),
            [rs_c]   "m" (rs_c),
            [cs_c]   "m" (cs_c),
            [mask]   "m" (mask)
          : // register clobber list
            "rax", "rbx", "rcx", "rdx", "rsi", "rdi",
            "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15",
            "xmm2", "xmm31",
            "ymm2",
            "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5", "zmm6",
            "zmm7", "zmm8", "zmm9", "zmm10", "zmm11", "zmm12", "zmm13",
            "zmm14", "zmm15", "zmm16", "zmm17", "zmm18", "zmm24",
            "zmm25", "zmm26", "zmm27", "zmm28", "zmm29", "zmm30",
            "zmm31",
            "k2",
            "memory"
        )
    } //mloop

    consider_edge_cases:

    // Handle edge cases in the m dimension, if they exist.
    if (m_left)
    {
        const dim_t nr_cur = 6;
        const dim_t i_edge = m0 - ( dim_t )m_left;
        double *restrict cij = cbuf + i_edge * rs_c;
        double *restrict ai  = abuf + m_iter * ps_a;
        double *restrict bj  = bbuf;
        // covers the range 16 < m_left <= 24 by using masked load/store instructions
        if( 16 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_24x6(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
        // covers the range 8 < m_left <= 16 by using masked load/store instructions
        else if( 8 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_16x6(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
        // covers the range 0 < m_left <= 8 by using masked load/store instructions
        else if( 0 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_8x6(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
    }
    AOCL_DTL_TRACE_EXIT(AOCL_DTL_LEVEL_TRACE_7);
}

void bli_dgemmsup_cv_zen4_asm_24x5m
(
       conj_t    conja,
       conj_t    conjb,
       dim_t     m0,
       dim_t     n0,
       dim_t     k0,
       double*    restrict alpha,
       double*    restrict a, inc_t rs_a0, inc_t cs_a0,
       double*    restrict b, inc_t rs_b0, inc_t cs_b0,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c0, inc_t cs_c0,
       auxinfo_t* restrict data,
       cntx_t*    restrict cntx
     )
{
    AOCL_DTL_TRACE_ENTRY(AOCL_DTL_LEVEL_TRACE_7);
    double *abuf = a;
    double *bbuf = b;
    double *cbuf = c;

    // Typecast local copies of integers in case dim_t and inc_t are a
    // different size than is expected by load instructions.
    uint64_t m_iter = (uint64_t)m0 / 24;
    uint64_t m_left = (uint64_t)m0 % 24;

    uint64_t rs_a   = rs_a0;
    uint64_t cs_a   = cs_a0;
    uint64_t rs_b   = rs_b0;
    uint64_t cs_b   = cs_b0;
    uint64_t rs_c   = rs_c0;
    uint64_t cs_c   = cs_c0;

    uint64_t ps_a = bli_auxinfo_ps_a( data );
    uint64_t ps_a8  = ps_a * sizeof( double );

    uint64_t k_iter = (uint64_t)k0 / 8;
    uint64_t k_left = (uint64_t)k0 % 8;

    uint8_t mask = (0xff >> (0x8 - (n0 & 7))); // calculate mask based on n_left

    if ( m_iter == 0 ) goto consider_edge_cases;

    /* For one iteration of this loop, a block of MRxNR is computed
     * This loop moves along m-dimension of c matrix with steps of MR*rs_c.
     */
    for(dim_t m=0; m < m_iter; m++)
    {

        a = abuf + m * ps_a ; // Move to next MRXKC in MCXKC (where MC>=MR)
        b = bbuf;  //Same KCXNR is used across different MRXKC in MCXKC
        c = cbuf + m * rs_c * 24; // Move to next MRxNR in MCxNR (where MC >= MR)

        // -------------------------------------------------------------------------
        begin_asm()

        mov(var(mask), rdx)             // load mask
        kmovw(edx, k(2))                // move mask to k2 register
        mov(var(a), rax)                // load address of a
        mov(var(cs_a), r10)             // load cs_a
        mov(var(b), rbx)                // load address of b
        mov(var(rs_b), r8)              // load rs_b
        mov(var(cs_b), r9)              // load cs_b
        mov(var(c), rcx)                // load address of c
        mov(var(cs_c), rdi)             // load cs_c
        lea(mem(, r8, 8), r8)           // rs_b *= sizeof(double)
        lea(mem(, r9, 8), r9)           // cs_b *= sizeof(double)
        lea(mem(, r10, 8), r10)         // cs_a *= sizeof(double)
        lea(mem(, rdi, 8), rdi)         // cs_c *= sizeof(double)
        lea(mem(r9, r9, 2 ), r13)       // r13 = 3*cs_b
        // if n > 4, a second pointer(r12) which points to rbx + 4*cs_b
        //is also used to traverse B matrix
        lea(mem(rbx, r9, 4), r12)       // r12 = rbx + 4*cs_b
        lea(mem(rcx, 7*8), rdx)         // C for prefetching
        mov(var(ps_a8), r14)            // panel stride of A
        lea(mem(rax, r14, 1, 7*8), r14) // prefetch next panel of A
        lea(mem(rbx, r8, 8, 7*8), r11)  // r11 = rbx + 8*rs_b(B for prefetching)
        // if n > 4, a second pointer which point to r11 + 4*cs_b
        //is also used to prefetch from B matrix
        lea(mem(r11, r9, 4), r15)       // r15 = r11 + 4* cs_b(B for prefetching)

        /* Register usage: zmm0-5 are used to load A matrix
         *                 zmm6-29 are used for accumulation
         *                 zmm30-31 are used for broadcasting B matrix
         */

        // zero out all accumulation registers
        vxorpd(zmm6, zmm6, zmm6)
        vxorpd(zmm7, zmm7, zmm7)
        vxorpd(zmm28, zmm28, zmm28)
        vxorpd(zmm8, zmm8, zmm8)
        vxorpd(zmm9, zmm9, zmm9)
        vxorpd(zmm29, zmm29, zmm29)
        vxorpd(zmm10, zmm10, zmm10)
        vxorpd(zmm11, zmm11, zmm11)
        vxorpd(zmm26, zmm26, zmm26)
        vxorpd(zmm12, zmm12, zmm12)
        vxorpd(zmm13, zmm13, zmm13)
        vxorpd(zmm27,zmm27, zmm27)
        vxorpd(zmm14, zmm14, zmm14)
        vxorpd(zmm15, zmm15, zmm15)
        vxorpd(zmm24, zmm24, zmm24)

        // K is unrolled by 8 to facilitate prefetch of B
        // Assuming B to be col-stored, for each iteration of K,
        //one cacheline of B_next is prefetched where b_next = b + (unroll)*rs_b
        label(.DLOOPKITER)                                     // main loop
        mov(var(k_iter), rsi)                                  // i = k_iter
        sub(imm( 5+TAIL_NITER), rsi)                           // i -= NR + TAIL_NITER
        jle(.PREFETCHLOOP)                                     // jump if i <= 0

        label(.LOOP1)

            // ---------------------------------- iteration 1

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )

            // ---------------------------------- iteration 2

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )

            // ---------------------------------- iteration 3

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )

            // ---------------------------------- iteration 4

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )

            // ---------------------------------- iteration 5

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )

            // ---------------------------------- iteration 6

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )

            // ---------------------------------- iteration 7

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )

            // ---------------------------------- iteration 8

            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            lea(mem(r15,r8,8), r15)                            // second pointer to b_next += 8*rs_b
            dec(rsi)                                           // i -= 1
        jnz(.LOOP1)                                            // iterate again if i != 0.

        label(.PREFETCHLOOP)
        add(imm(5), rsi)                                       // i += NR
        jle(.TAILITER)                                         // jump if i <= 0.

        label(.LOOP2)

            // ---------------------------------- iteration 1
            prefetchw0( mem(rdx))                              // prefetch C
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )

            // ---------------------------------- iteration 2
            prefetchw0( mem(rdx, 64))                          // prefetch C
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )

            // ---------------------------------- iteration 3
            prefetchw0( mem(rdx, 128))                        // prefetch C
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )

            // ---------------------------------- iteration 4
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )

            // ---------------------------------- iteration 5
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )

            // ---------------------------------- iteration 6
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )

            // ---------------------------------- iteration 7
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )

            // ---------------------------------- iteration 8
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            lea(mem(rdx, rdi, 1), rdx)                         // C += cs_c
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            lea(mem(r15,r8,8), r15)                            // second pointer of b_next += 8*rs_b
            sub(imm(1), rsi)                                   // i -= 1
        jnz(.LOOP2)                                            // iterate again if i != 0.
        label(.TAILITER)
        add(imm(TAIL_NITER), rsi)                              // i += TAIL_NITER
        jle(.TAIL)                                             // jump if i <= 0

        label(.LOOP3)

            // ---------------------------------- iteration 1
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )

            // ---------------------------------- iteration 2
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )

            // ---------------------------------- iteration 3
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )

            // ---------------------------------- iteration 4
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )

            // ---------------------------------- iteration 5
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r15) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )

            // ---------------------------------- iteration 6
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )

            // ---------------------------------- iteration 7
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )

            // ---------------------------------- iteration 8
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm6 )
            vfmadd231pd( zmm4,zmm30,zmm7 )
            vfmadd231pd( zmm5,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm8 )
            vfmadd231pd( zmm4,zmm31,zmm9 )
            vfmadd231pd( zmm5,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm10 )
            vfmadd231pd( zmm4,zmm30,zmm11 )
            vfmadd231pd( zmm5,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm12 )
            vfmadd231pd( zmm4,zmm31,zmm13 )
            vfmadd231pd( zmm5,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm24 )
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            lea(mem(r15,r8,8), r15)                            // Second pointer of b_next += 8*rs_b
            dec(rsi)                                           // i -= 1
        jnz(.LOOP3)                                            // iterate again if i != 0.


        label(.TAIL)
        mov(var(k_left), rsi)                                  // i = k_left
        test(rsi, rsi)                                         // check i via logical AND
        je(.DPOSTACCUM)                                        // if i == 0, jump to post-accumulation

        label(.DLOOPKLEFT)                                     // k_left loop
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            vbroadcastsd( mem(r12),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            add( r8,r12 )                                     // second pointer of b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm14 )
            vfmadd231pd( zmm1,zmm30,zmm15 )
            vfmadd231pd( zmm2,zmm30,zmm24 )
            dec(rsi)                                           // i -= 1
        jne(.DLOOPKLEFT)                                       // iterate again if i != 0.


        label(.DPOSTACCUM)
        mov(var(alpha), rdx)                                   // load address of alpha
        vbroadcastsd(mem(rdx), zmm30)                           // broadcast alpha
        mov(var(beta), rax)                                    // load address of beta
        vbroadcastsd(mem(rax), zmm31)                           // broadcast beta

        // scale by alpha
        vmulpd( zmm30,zmm6,zmm6 )
        vmulpd( zmm30,zmm7,zmm7 )
        vmulpd( zmm30,zmm28,zmm28 )
        vmulpd( zmm30,zmm8,zmm8 )
        vmulpd( zmm30,zmm9,zmm9 )
        vmulpd( zmm30,zmm29,zmm29 )
        vmulpd( zmm30,zmm10,zmm10 )
        vmulpd( zmm30,zmm11,zmm11 )
        vmulpd( zmm30,zmm26,zmm26 )
        vmulpd( zmm30,zmm12,zmm12 )
        vmulpd( zmm30,zmm13,zmm13 )
        vmulpd( zmm30,zmm27,zmm27 )
        vmulpd( zmm30,zmm14,zmm14 )
        vmulpd( zmm30,zmm15,zmm15 )
        vmulpd( zmm30,zmm24,zmm24 )


        mov(var(rs_c), rsi)                                    // load rs_c
        lea(mem(, rsi, 8), rsi)                                // rsi = rs_c * sizeof(double)
        lea(mem(rcx, rdi, 4), rdx)                             // rdx = rcx + 4 * cs_c
        lea(mem(rdi, rdi, 2), r13)                             // r13 = 3*cs_c
        vxorpd(ymm2, ymm2, ymm2)
        vucomisd(xmm2, xmm31)                                   // set ZF if beta == 0
        je(.DBETAZERO)                                         // if ZF == 1, jump to beta == 0 case


        cmp(imm(8), rdi)                                       // set ZF if (8*cs_c) == 8


        jz(.DROWSTORED)                                        // jump to row storage case

        label(.DCOLSTORED)
        vfmadd231pd( mem(rcx),zmm31,zmm6)
        vmovupd( zmm6,(rcx))
        vfmadd231pd( 0x40(rcx),zmm31,zmm7)
        vmovupd( zmm7,0x40(rcx))
        vfmadd231pd( 0x80(rcx),zmm31,zmm28)
        vmovupd( zmm28,0x80(rcx))
        vfmadd231pd( mem(rcx,rdi,1),zmm31,zmm8)
        vmovupd( zmm8,(rcx,rdi,1))
        vfmadd231pd( 0x40(rcx,rdi,1),zmm31,zmm9)
        vmovupd( zmm9,0x40(rcx,rdi,1))
        vfmadd231pd( 0x80(rcx,rdi,1),zmm31,zmm29)
        vmovupd( zmm29,0x80(rcx,rdi,1))
        vfmadd231pd( mem(rcx,rdi,2),zmm31,zmm10)
        vmovupd( zmm10,(rcx,rdi,2))
        vfmadd231pd( 0x40(rcx,rdi,2),zmm31,zmm11)
        vmovupd( zmm11,0x40(rcx,rdi,2))
        vfmadd231pd( 0x80(rcx,rdi,2),zmm31,zmm26)
        vmovupd( zmm26,0x80(rcx,rdi,2))
        vfmadd231pd( mem(rcx,r13,1),zmm31,zmm12)
        vmovupd( zmm12,(rcx,r13,1))
        vfmadd231pd( 0x40(rcx,r13,1),zmm31,zmm13)
        vmovupd( zmm13,0x40(rcx,r13,1))
        vfmadd231pd( 0x80(rcx,r13,1),zmm31,zmm27)
        vmovupd( zmm27,0x80(rcx,r13,1))
        vfmadd231pd( mem(rdx),zmm31,zmm14)
        vmovupd( zmm14,(rdx))
        vfmadd231pd( 0x40(rdx),zmm31,zmm15)
        vmovupd( zmm15,0x40(rdx))
        vfmadd231pd( 0x80(rdx),zmm31,zmm24)
        vmovupd( zmm24,0x80(rdx))

        jmp(.DDONE)                                           // jump to end.

        label(.DROWSTORED)
        // r12 = 3*rs_c
        lea(mem(rsi,  rsi,  2), r12)
        // r13 = 5*rs_c
        lea(mem(r12, rsi,  2), r13)
        // rdx = 7*rs_c
        lea(mem(r12, rsi,  4), rdx)
        lea(mem(   , rsi, 8), r14)
        UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)

        vunpcklpd(zmm16, zmm14, zmm0)
        vunpckhpd(zmm16, zmm14, zmm1)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)

        vbroadcastsd(mem(rax), zmm31)
        UPDATE_MASKED_C
        //First 8x5 tile updated

        UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        vunpcklpd(zmm17, zmm15, zmm0)
        vunpckhpd(zmm17, zmm15, zmm1)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C
        //Second 8x5 tile updated

        UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        vunpcklpd(zmm25, zmm24, zmm0)
        vunpckhpd(zmm25, zmm24, zmm1)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C
        //Third 8x5 tile updated
        jmp(.DDONE)                                          // jump to end.


        label(.DBETAZERO)
        cmp(imm(8), rdi)                                     // set ZF if (8*cs_c) == 8

        jz(.DROWSTORBZ)                                      // jump to row storage case
        label(.DCOLSTORBZ)
        vmovupd( zmm6,(rcx))
        vmovupd( zmm7,0x40(rcx))
        vmovupd( zmm28,0x80(rcx))
        vmovupd( zmm8,(rcx,rdi,1))
        vmovupd( zmm9,0x40(rcx,rdi,1))
        vmovupd( zmm29,0x80(rcx,rdi,1))
        vmovupd( zmm10,(rcx,rdi,2))
        vmovupd( zmm11,0x40(rcx,rdi,2))
        vmovupd( zmm26,0x80(rcx,rdi,2))
        vmovupd( zmm12,(rcx,r13,1))
        vmovupd( zmm13,0x40(rcx,r13,1))
        vmovupd( zmm27,0x80(rcx,r13,1))
        vmovupd( zmm14,(rdx))
        vmovupd( zmm15,0x40(rdx))
        vmovupd( zmm24,0x80(rdx))

        jmp(.DDONE)                                          // jump to end.


        label(.DROWSTORBZ)
        // r12 = 3*rs_c
        lea(mem(rsi,  rsi,  2), r12)
        // r13 = 5*rs_c
        lea(mem(r12, rsi,  2), r13)
        // rdx = 7*rs_c
        lea(mem(r12, rsi,  4), rdx)
        lea(mem(   , rsi, 8), r14)
        UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)

        vunpcklpd(zmm16, zmm14, zmm0)
        vunpckhpd(zmm16, zmm14, zmm1)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)

        vbroadcastsd(mem(rax), zmm31)
        UPDATE_MASKED_C_BZ
        //First 8x5 tile updated

        UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        vunpcklpd(zmm17, zmm15, zmm0)
        vunpckhpd(zmm17, zmm15, zmm1)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C_BZ
        //Second 8x5 tile updated

        UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        vunpcklpd(zmm25, zmm24, zmm0)
        vunpckhpd(zmm25, zmm24, zmm1)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C_BZ
        //Third 8x5 tile updated
        label(.DDONE)


        vzeroupper()

        end_asm(
          : // output operands (none)
          : // input operands
            [k_iter] "m" (k_iter),
            [k_left] "m" (k_left),
            [a]      "m" (a),
            [rs_a]   "m" (rs_a),
            [cs_a]   "m" (cs_a),
            [ps_a8]  "m" (ps_a8),
            [b]      "m" (b),
            [rs_b]   "m" (rs_b),
            [cs_b]   "m" (cs_b),
            [alpha]  "m" (alpha),
            [beta]   "m" (beta),
            [c]      "m" (c),
            [rs_c]   "m" (rs_c),
            [cs_c]   "m" (cs_c),
            [mask]   "m" (mask)
          : // register clobber list
            "rax", "rbx", "rcx", "rdx", "rsi", "rdi",
            "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15",
            "xmm2", "xmm31",
            "ymm2",
            "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5", "zmm6",
            "zmm7", "zmm8", "zmm9", "zmm10", "zmm11", "zmm12", "zmm13",
            "zmm14", "zmm15", "zmm16", "zmm17", "zmm18", "zmm24",
            "zmm25", "zmm26", "zmm27", "zmm28", "zmm29", "zmm30",
            "zmm31",
            "k2",
            "memory"
        )
    } //mloop

    consider_edge_cases:

    // Handle edge cases in the m dimension, if they exist.
    if (m_left)
    {
        const dim_t nr_cur = 5;
        const dim_t i_edge = m0 - ( dim_t )m_left;
        double *restrict cij = cbuf + i_edge * rs_c;
        double *restrict ai  = abuf + m_iter * ps_a;
        double *restrict bj  = bbuf;
        // covers the range 16 < m_left <= 24 by using masked load/store instructions
        if( 16 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_24x5(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
        // covers the range 8 < m_left <= 16 by using masked load/store instructions
        else if( 8 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_16x5(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
        // covers the range 0 < m_left <= 8 by using masked load/store instructions
        else if( 0 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_8x5(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
    }
    AOCL_DTL_TRACE_EXIT(AOCL_DTL_LEVEL_TRACE_7);
}

void bli_dgemmsup_cv_zen4_asm_24x4m
(
       conj_t    conja,
       conj_t    conjb,
       dim_t     m0,
       dim_t     n0,
       dim_t     k0,
       double*    restrict alpha,
       double*    restrict a, inc_t rs_a0, inc_t cs_a0,
       double*    restrict b, inc_t rs_b0, inc_t cs_b0,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c0, inc_t cs_c0,
       auxinfo_t* restrict data,
       cntx_t*    restrict cntx
     )
{
    AOCL_DTL_TRACE_ENTRY(AOCL_DTL_LEVEL_TRACE_7);
    double *abuf = a;
    double *bbuf = b;
    double *cbuf = c;

    // Typecast local copies of integers in case dim_t and inc_t are a
    // different size than is expected by load instructions.
    uint64_t m_iter = (uint64_t)m0 / 24;
    uint64_t m_left = (uint64_t)m0 % 24;

    uint64_t rs_a   = rs_a0;
    uint64_t cs_a   = cs_a0;
    uint64_t rs_b   = rs_b0;
    uint64_t cs_b   = cs_b0;
    uint64_t rs_c   = rs_c0;
    uint64_t cs_c   = cs_c0;

    uint64_t ps_a = bli_auxinfo_ps_a( data );
    uint64_t ps_a8  = ps_a * sizeof( double );

    uint64_t k_iter = (uint64_t)k0 / 8;
    uint64_t k_left = (uint64_t)k0 % 8;

    uint8_t mask = (0xff >> (0x8 - (n0 & 7))); // calculate mask based on n_left

    if ( m_iter == 0 ) goto consider_edge_cases;

    /* For one iteration of this loop, a block of MRxNR is computed
     * This loop moves along m-dimension of c matrix with steps of MR*rs_c.
     */
    for(dim_t m=0; m < m_iter; m++)
    {

        a = abuf + m * ps_a ; // Move to next MRXKC in MCXKC (where MC>=MR)
        b = bbuf;  //Same KCXNR is used across different MRXKC in MCXKC
        c = cbuf + m * rs_c * 24; // Move to next MRxNR in MCxNR (where MC >= MR)

        // -------------------------------------------------------------------------
        begin_asm()

        mov(var(mask), rdx)             // load mask
        kmovw(edx, k(2))                // move mask to k2 register
        mov(var(a), rax)                // load address of a
        mov(var(cs_a), r10)             // load cs_a
        mov(var(b), rbx)                // load address of b
        mov(var(rs_b), r8)              // load rs_b
        mov(var(cs_b), r9)              // load cs_b
        mov(var(c), rcx)                // load address of c
        mov(var(cs_c), rdi)             // load cs_c
        lea(mem(, r8, 8), r8)           // rs_b *= sizeof(double)
        lea(mem(, r9, 8), r9)           // cs_b *= sizeof(double)
        lea(mem(, r10, 8), r10)         // cs_a *= sizeof(double)
        lea(mem(, rdi, 8), rdi)         // cs_c *= sizeof(double)
        lea(mem(r9, r9, 2 ), r13)       // r13 = 3*cs_b
        lea(mem(rcx, 7*8), rdx)         // C for prefetching
        mov(var(ps_a8), r14)            // panel stride of A
        lea(mem(rax, r14, 1, 7*8), r14) // prefetch next panel of A
        lea(mem(rbx, r8, 8, 7*8), r11)  // r11 = rbx + 8*rs_b(B for prefetching)

        /* Register usage: zmm0-5 are used to load A matrix
         *                 zmm6-29 are used for accumulation
         *                 zmm30-31 are used for broadcasting B matrix
         */

        // zero out all accumulation registers
        vxorpd(zmm6, zmm6, zmm6)
        vxorpd(zmm7, zmm7, zmm7)
        vxorpd(zmm28, zmm28, zmm28)
        vxorpd(zmm8, zmm8, zmm8)
        vxorpd(zmm9, zmm9, zmm9)
        vxorpd(zmm29, zmm29, zmm29)
        vxorpd(zmm10, zmm10, zmm10)
        vxorpd(zmm11, zmm11, zmm11)
        vxorpd(zmm26, zmm26, zmm26)
        vxorpd(zmm12, zmm12, zmm12)
        vxorpd(zmm13, zmm13, zmm13)
        vxorpd(zmm27,zmm27, zmm27)
        vxorpd(zmm14, zmm14, zmm14)
        vxorpd(zmm15, zmm15, zmm15)
        vxorpd(zmm16, zmm16, zmm16)
        vxorpd(zmm17, zmm17, zmm17)
        vxorpd(zmm18, zmm18, zmm18)
        vxorpd(zmm19, zmm19, zmm19)
        vxorpd(zmm20, zmm20, zmm20)
        vxorpd(zmm21, zmm21, zmm21)
        vxorpd(zmm22, zmm22, zmm22)
        vxorpd(zmm23, zmm23, zmm23)
        vxorpd(zmm24, zmm24, zmm24)
        vxorpd(zmm25, zmm25, zmm25)

        // K is unrolled by 8 to facilitate prefetch of B
        // Assuming B to be col-stored, for each iteration of K,
        //one cacheline of B_next is prefetched where b_next = b + (unroll)*rs_b
        label(.DLOOPKITER)                                     // main loop
        mov(var(k_iter), rsi)                                  // i = k_iter
        sub(imm( 4+TAIL_NITER), rsi)                           // i -= NR + TAIL_NITER
        jle(.PREFETCHLOOP)                                     // jump if i <= 0

        label(.LOOP1)
            /**
             * This edge kernel uses two separate vector register bank
             * to hold fma result.
             * Once the K loop is completed these two vector register banks
             * are added together and final result is available in one
             * register bank.
             * Here odd iterations uses vector register zmm6, zmm7, zmm28,
             * zmm8, zmm9, zmm29, zmm10, zmm11, zmm26, zmm12, zmm13, zmm27
             * to hold fma result.
             * While even iterations uses zmm14, zmm15, zmm16, zmm17, zmm18
             * zmm19, zmm20, zmm21, zmm22, zmm23, zmm24, zmm25 to hold fma
             * result.
             * At the end of K loop, these two banks are added together and
             * final result is available in vector register zmm6, zmm7, zmm28,
             * zmm8, zmm9, zmm29, zmm10, zmm11, zmm26, zmm12, zmm13, zmm27.
            */
            // ---------------------------------- iteration 1

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )

            // ---------------------------------- iteration 2

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm23 )
            vfmadd231pd( zmm4,zmm31,zmm24 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 3

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )

            // ---------------------------------- iteration 4

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm23 )
            vfmadd231pd( zmm4,zmm31,zmm24 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 5

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )

            // ---------------------------------- iteration 6

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm23 )
            vfmadd231pd( zmm4,zmm31,zmm24 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 7

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )

            // ---------------------------------- iteration 8

            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm23 )
            vfmadd231pd( zmm4,zmm31,zmm24 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            dec(rsi)                                           // i -= 1
        jnz(.LOOP1)                                            // iterate again if i != 0.

        label(.PREFETCHLOOP)
        add(imm(4), rsi)                                       // i += NR
        jle(.TAILITER)                                         // jump if i <= 0.

        label(.LOOP2)

            // ---------------------------------- iteration 1
            prefetchw0( mem(rdx))                              // prefetch C
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )

            // ---------------------------------- iteration 2
            prefetchw0( mem(rdx, 64))                          // prefetch C
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm23 )
            vfmadd231pd( zmm4,zmm31,zmm24 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 3
            prefetchw0( mem(rdx, 128))                        // prefetch C
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )

            // ---------------------------------- iteration 4
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm23 )
            vfmadd231pd( zmm4,zmm31,zmm24 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 5
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )

            // ---------------------------------- iteration 6
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm23 )
            vfmadd231pd( zmm4,zmm31,zmm24 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 7
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )

            // ---------------------------------- iteration 8
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm23 )
            vfmadd231pd( zmm4,zmm31,zmm24 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            lea(mem(rdx, rdi, 1), rdx)                         // C += cs_c
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            sub(imm(1), rsi)                                   // i -= 1
        jnz(.LOOP2)                                            // iterate again if i != 0.
        label(.TAILITER)
        add(imm(TAIL_NITER), rsi)                              // i += TAIL_NITER
        jle(.TAIL)                                            // jump if i <= 0

        label(.LOOP3)

            // ---------------------------------- iteration 1
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )

            // ---------------------------------- iteration 2
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm23 )
            vfmadd231pd( zmm4,zmm31,zmm24 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 3
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )

            // ---------------------------------- iteration 4
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r13,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm23 )
            vfmadd231pd( zmm4,zmm31,zmm24 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 5
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )

            // ---------------------------------- iteration 6
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm23 )
            vfmadd231pd( zmm4,zmm31,zmm24 )
            vfmadd231pd( zmm5,zmm31,zmm25 )

            // ---------------------------------- iteration 7
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )

            // ---------------------------------- iteration 8
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm23 )
            vfmadd231pd( zmm4,zmm31,zmm24 )
            vfmadd231pd( zmm5,zmm31,zmm25 )
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            dec(rsi)                                           // i -= 1
        jnz(.LOOP3)                                            // iterate again if i != 0.

        vaddpd(zmm14, zmm6, zmm6)
        vaddpd(zmm15, zmm7, zmm7)
        vaddpd(zmm16, zmm28, zmm28)
        vaddpd(zmm17, zmm8, zmm8)
        vaddpd(zmm18, zmm9, zmm9)
        vaddpd(zmm19, zmm29, zmm29)
        vaddpd(zmm20, zmm10, zmm10)
        vaddpd(zmm21, zmm11, zmm11)
        vaddpd(zmm22, zmm26, zmm26)
        vaddpd(zmm23, zmm12, zmm12)
        vaddpd(zmm24, zmm13, zmm13)
        vaddpd(zmm25, zmm27, zmm27)

        label(.TAIL)
        mov(var(k_left), rsi)                                  // i = k_left
        test(rsi, rsi)                                         // check i via logical AND
        je(.DPOSTACCUM)                                        // if i == 0, jump to post-accumulation

        label(.DLOOPKLEFT)                                     // k_left loop
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            vbroadcastsd( mem(rbx,r13,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm12 )
            vfmadd231pd( zmm1,zmm31,zmm13 )
            vfmadd231pd( zmm2,zmm31,zmm27 )
            dec(rsi)                                           // i -= 1
        jne(.DLOOPKLEFT)                                       // iterate again if i != 0.


        label(.DPOSTACCUM)
        mov(var(alpha), rdx)                                   // load address of alpha
        vbroadcastsd(mem(rdx), zmm30)                           // broadcast alpha
        mov(var(beta), rax)                                    // load address of beta
        vbroadcastsd(mem(rax), zmm31)                           // broadcast beta

        // scale by alpha
        vmulpd( zmm30,zmm6,zmm6 )
        vmulpd( zmm30,zmm7,zmm7 )
        vmulpd( zmm30,zmm28,zmm28 )
        vmulpd( zmm30,zmm8,zmm8 )
        vmulpd( zmm30,zmm9,zmm9 )
        vmulpd( zmm30,zmm29,zmm29 )
        vmulpd( zmm30,zmm10,zmm10 )
        vmulpd( zmm30,zmm11,zmm11 )
        vmulpd( zmm30,zmm26,zmm26 )
        vmulpd( zmm30,zmm12,zmm12 )
        vmulpd( zmm30,zmm13,zmm13 )
        vmulpd( zmm30,zmm27,zmm27 )


        mov(var(rs_c), rsi)                                    // load rs_c
        lea(mem(, rsi, 8), rsi)                                // rsi = rs_c * sizeof(double)
        lea(mem(rdi, rdi, 2), r13)                             // r13 = 3*cs_c
        vxorpd(ymm2, ymm2, ymm2)
        vucomisd(xmm2, xmm31)                                   // set ZF if beta == 0
        je(.DBETAZERO)                                         // if ZF == 1, jump to beta == 0 case


        cmp(imm(8), rdi)                                       // set ZF if (8*cs_c) == 8


        jz(.DROWSTORED)                                        // jump to row storage case

        label(.DCOLSTORED)
        vfmadd231pd( mem(rcx),zmm31,zmm6)
        vmovupd( zmm6,(rcx))
        vfmadd231pd( 0x40(rcx),zmm31,zmm7)
        vmovupd( zmm7,0x40(rcx))
        vfmadd231pd( 0x80(rcx),zmm31,zmm28)
        vmovupd( zmm28,0x80(rcx))
        vfmadd231pd( mem(rcx,rdi,1),zmm31,zmm8)
        vmovupd( zmm8,(rcx,rdi,1))
        vfmadd231pd( 0x40(rcx,rdi,1),zmm31,zmm9)
        vmovupd( zmm9,0x40(rcx,rdi,1))
        vfmadd231pd( 0x80(rcx,rdi,1),zmm31,zmm29)
        vmovupd( zmm29,0x80(rcx,rdi,1))
        vfmadd231pd( mem(rcx,rdi,2),zmm31,zmm10)
        vmovupd( zmm10,(rcx,rdi,2))
        vfmadd231pd( 0x40(rcx,rdi,2),zmm31,zmm11)
        vmovupd( zmm11,0x40(rcx,rdi,2))
        vfmadd231pd( 0x80(rcx,rdi,2),zmm31,zmm26)
        vmovupd( zmm26,0x80(rcx,rdi,2))
        vfmadd231pd( mem(rcx,r13,1),zmm31,zmm12)
        vmovupd( zmm12,(rcx,r13,1))
        vfmadd231pd( 0x40(rcx,r13,1),zmm31,zmm13)
        vmovupd( zmm13,0x40(rcx,r13,1))
        vfmadd231pd( 0x80(rcx,r13,1),zmm31,zmm27)
        vmovupd( zmm27,0x80(rcx,r13,1))

        jmp(.DDONE)                                           // jump to end.

        label(.DROWSTORED)
        // r12 = 3*rs_c
        lea(mem(rsi,  rsi,  2), r12)
        // r13 = 5*rs_c
        lea(mem(r12, rsi,  2), r13)
        // rdx = 7*rs_c
        lea(mem(r12, rsi,  4), rdx)
        lea(mem(   , rsi, 8), r14)
        UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)

        vbroadcastsd(mem(rax), zmm31)
        UPDATE_MASKED_C
        //First 8x4 tile updated

        UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C
        //Second 8x4 tile updated

        UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C
        //Third 8x4 tile updated
        jmp(.DDONE)                                          // jump to end.


        label(.DBETAZERO)
        cmp(imm(8), rdi)                                     // set ZF if (8*cs_c) == 8

        jz(.DROWSTORBZ)                                      // jump to row storage case
        label(.DCOLSTORBZ)
        vmovupd( zmm6,(rcx))
        vmovupd( zmm7,0x40(rcx))
        vmovupd( zmm28,0x80(rcx))
        vmovupd( zmm8,(rcx,rdi,1))
        vmovupd( zmm9,0x40(rcx,rdi,1))
        vmovupd( zmm29,0x80(rcx,rdi,1))
        vmovupd( zmm10,(rcx,rdi,2))
        vmovupd( zmm11,0x40(rcx,rdi,2))
        vmovupd( zmm26,0x80(rcx,rdi,2))
        vmovupd( zmm12,(rcx,r13,1))
        vmovupd( zmm13,0x40(rcx,r13,1))
        vmovupd( zmm27,0x80(rcx,r13,1))

        jmp(.DDONE)                                          // jump to end.


        label(.DROWSTORBZ)
        // r12 = 3*rs_c
        lea(mem(rsi,  rsi,  2), r12)
        // r13 = 5*rs_c
        lea(mem(r12, rsi,  2), r13)
        // rdx = 7*rs_c
        lea(mem(r12, rsi,  4), rdx)
        lea(mem(   , rsi, 8), r14)
        UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)

        UPDATE_MASKED_C_BZ
        //First 8x5 tile updated

        UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C_BZ
        //Second 8x5 tile updated

        UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C_BZ
        //Third 8x5 tile updated
        label(.DDONE)


        vzeroupper()

        end_asm(
          : // output operands (none)
          : // input operands
            [k_iter] "m" (k_iter),
            [k_left] "m" (k_left),
            [a]      "m" (a),
            [rs_a]   "m" (rs_a),
            [cs_a]   "m" (cs_a),
            [ps_a8]  "m" (ps_a8),
            [b]      "m" (b),
            [rs_b]   "m" (rs_b),
            [cs_b]   "m" (cs_b),
            [alpha]  "m" (alpha),
            [beta]   "m" (beta),
            [c]      "m" (c),
            [rs_c]   "m" (rs_c),
            [cs_c]   "m" (cs_c),
            [mask]   "m" (mask)
          : // register clobber list
            "rax", "rbx", "rcx", "rdx", "rsi", "rdi",
            "r8", "r9", "r10", "r11", "r12", "r13", "r14",
            "xmm2", "xmm31",
            "ymm2",
            "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5", "zmm6",
            "zmm7", "zmm8", "zmm9", "zmm10", "zmm11", "zmm12", "zmm13",
            "zmm14", "zmm15", "zmm16", "zmm17", "zmm18", "zmm19",
            "zmm20", "zmm21", "zmm22", "zmm23", "zmm24", "zmm25",
            "zmm26", "zmm27", "zmm28", "zmm29", "zmm30", "zmm31",
            "k2",
            "memory"
        )
    } //mloop

    consider_edge_cases:

    // Handle edge cases in the m dimension, if they exist.
    if (m_left)
    {
        const dim_t nr_cur = 4;
        const dim_t i_edge = m0 - ( dim_t )m_left;
        double *restrict cij = cbuf + i_edge * rs_c;
        double *restrict ai  = abuf + m_iter * ps_a;
        double *restrict bj  = bbuf;
        // covers the range 16 < m_left <= 24 by using masked load/store instructions
        if( 16 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_24x4(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
        // covers the range 8 < m_left <= 16 by using masked load/store instructions
        else if( 8 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_16x4(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
        // covers the range 0 < m_left <= 8 by using masked load/store instructions
        else if( 0 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_8x4(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
    }
    AOCL_DTL_TRACE_EXIT(AOCL_DTL_LEVEL_TRACE_7);
}

void bli_dgemmsup_cv_zen4_asm_24x3m
(
       conj_t    conja,
       conj_t    conjb,
       dim_t     m0,
       dim_t     n0,
       dim_t     k0,
       double*    restrict alpha,
       double*    restrict a, inc_t rs_a0, inc_t cs_a0,
       double*    restrict b, inc_t rs_b0, inc_t cs_b0,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c0, inc_t cs_c0,
       auxinfo_t* restrict data,
       cntx_t*    restrict cntx
     )
{
    AOCL_DTL_TRACE_ENTRY(AOCL_DTL_LEVEL_TRACE_7);
    double *abuf = a;
    double *bbuf = b;
    double *cbuf = c;

    // Typecast local copies of integers in case dim_t and inc_t are a
    // different size than is expected by load instructions.
    uint64_t m_iter = (uint64_t)m0 / 24;
    uint64_t m_left = (uint64_t)m0 % 24;

    uint64_t rs_a   = rs_a0;
    uint64_t cs_a   = cs_a0;
    uint64_t rs_b   = rs_b0;
    uint64_t cs_b   = cs_b0;
    uint64_t rs_c   = rs_c0;
    uint64_t cs_c   = cs_c0;

    uint64_t ps_a = bli_auxinfo_ps_a( data );
    uint64_t ps_a8  = ps_a * sizeof( double );

    uint64_t k_iter = (uint64_t)k0 / 8;
    uint64_t k_left = (uint64_t)k0 % 8;

    uint8_t mask = (0xff >> (0x8 - (n0 & 7))); // calculate mask based on n_left

    if ( m_iter == 0 ) goto consider_edge_cases;

    /* For one iteration of this loop, a block of MRxNR is computed
     * This loop moves along m-dimension of c matrix with steps of MR*rs_c.
     */
    for(dim_t m=0; m < m_iter; m++)
    {

        a = abuf + m * ps_a ; // Move to next MRXKC in MCXKC (where MC>=MR)
        b = bbuf;  //Same KCXNR is used across different MRXKC in MCXKC
        c = cbuf + m * rs_c * 24; // Move to next MRxNR in MCxNR (where MC >= MR)

        // -------------------------------------------------------------------------
        begin_asm()

        mov(var(mask), rdx)             // load mask
        kmovw(edx, k(2))                // move mask to k2 register
        mov(var(a), rax)                // load address of a
        mov(var(cs_a), r10)             // load cs_a
        mov(var(b), rbx)                // load address of b
        mov(var(rs_b), r8)              // load rs_b
        mov(var(cs_b), r9)              // load cs_b
        mov(var(c), rcx)                // load address of c
        mov(var(cs_c), rdi)             // load cs_c
        lea(mem(, r8, 8), r8)           // rs_b *= sizeof(double)
        lea(mem(, r9, 8), r9)           // cs_b *= sizeof(double)
        lea(mem(, r10, 8), r10)         // cs_a *= sizeof(double)
        lea(mem(, rdi, 8), rdi)         // cs_c *= sizeof(double)
        lea(mem(rcx, 7*8), rdx)         // C for prefetching
        mov(var(ps_a8), r14)            // panel stride of A
        lea(mem(rax, r14, 1, 7*8), r14) // prefetch next panel of A
        lea(mem(rbx, r8, 8, 7*8), r11)  // r11 = rbx + 8*rs_b(B for prefetching)

        /* Register usage: zmm0-5 are used to load A matrix
         *                 zmm6-29 are used for accumulation
         *                 zmm30-31 are used for broadcasting B matrix
         */

        // zero out all accumulation registers
        vxorpd(zmm6, zmm6, zmm6)
        vxorpd(zmm7, zmm7, zmm7)
        vxorpd(zmm28, zmm28, zmm28)
        vxorpd(zmm8, zmm8, zmm8)
        vxorpd(zmm9, zmm9, zmm9)
        vxorpd(zmm29, zmm29, zmm29)
        vxorpd(zmm10, zmm10, zmm10)
        vxorpd(zmm11, zmm11, zmm11)
        vxorpd(zmm26, zmm26, zmm26)
        vxorpd(zmm14, zmm14, zmm14)
        vxorpd(zmm15, zmm15, zmm15)
        vxorpd(zmm16, zmm16, zmm16)
        vxorpd(zmm17, zmm17, zmm17)
        vxorpd(zmm18, zmm18, zmm18)
        vxorpd(zmm19, zmm19, zmm19)
        vxorpd(zmm20, zmm20, zmm20)
        vxorpd(zmm21, zmm21, zmm21)
        vxorpd(zmm22, zmm22, zmm22)

        // K is unrolled by 8 to facilitate prefetch of B
        // Assuming B to be col-stored, for each iteration of K,
        //one cacheline of B_next is prefetched where b_next = b + (unroll)*rs_b
        label(.DLOOPKITER)                                     // main loop
        mov(var(k_iter), rsi)                                  // i = k_iter
        sub(imm( 3+TAIL_NITER), rsi)                           // i -= NR + TAIL_NITER
        jle(.PREFETCHLOOP)                                     // jump if i <= 0

        /**
         * This edge kernel uses two separate vector register bank
         * to hold fma result.
         * Once the K loop is completed these two vector register banks
         * are added together and final result is available in one
         * register bank.
         * Here odd iterations uses vector register zmm6, zmm7, zmm28,
         * zmm8, zmm9, zmm29, zmm10, zmm11, zmm26 to hold fma result.
         * While even iterations uses zmm14, zmm15, zmm16, zmm17, zmm18
         * zmm19, zmm20, zmm21, zmm22 to hold fma
         * result.
         * At the end of K loop, these two banks are added together and
         * final result is available in vector register zmm6, zmm7, zmm28,
         * zmm8, zmm9, zmm29, zmm10, zmm11, zmm26.
         */

        label(.LOOP1)

            // ---------------------------------- iteration 1

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )

            // ---------------------------------- iteration 2

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 3

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )

            // ---------------------------------- iteration 4

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 5

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )

            // ---------------------------------- iteration 6

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 7

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )

            // ---------------------------------- iteration 8

            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            dec(rsi)                                           // i -= 1
        jnz(.LOOP1)                                            // iterate again if i != 0.

        label(.PREFETCHLOOP)
        add(imm(3), rsi)                                       // i += NR
        jle(.TAILITER)                                         // jump if i <= 0.

        label(.LOOP2)

            // ---------------------------------- iteration 1
            prefetchw0( mem(rdx))                              // prefetch C
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )

            // ---------------------------------- iteration 2
            prefetchw0( mem(rdx, 64))                          // prefetch C
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 3
            prefetchw0( mem(rdx, 128))                        // prefetch C
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )

            // ---------------------------------- iteration 4
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 5
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )

            // ---------------------------------- iteration 6
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 7
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )

            // ---------------------------------- iteration 8
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            lea(mem(rdx, rdi, 1), rdx)                         // C += cs_c
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            sub(imm(1), rsi)                                   // i -= 1
        jnz(.LOOP2)                                            // iterate again if i != 0.
        label(.TAILITER)
        add(imm(TAIL_NITER), rsi)                              // i += TAIL_NITER
        jle(.TAIL)                                             // jump if i <= 0

        label(.LOOP3)

            // ---------------------------------- iteration 1
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )

            // ---------------------------------- iteration 2
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 3
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,2) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )

            // ---------------------------------- iteration 4
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 5
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )

            // ---------------------------------- iteration 6
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )

            // ---------------------------------- iteration 7
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )

            // ---------------------------------- iteration 8
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm20 )
            vfmadd231pd( zmm4,zmm30,zmm21 )
            vfmadd231pd( zmm5,zmm30,zmm22 )
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            dec(rsi)                                           // i -= 1
        jnz(.LOOP3)                                            // iterate again if i != 0.

        vaddpd(zmm14, zmm6, zmm6)
        vaddpd(zmm15, zmm7, zmm7)
        vaddpd(zmm16, zmm28, zmm28)
        vaddpd(zmm17, zmm8, zmm8)
        vaddpd(zmm18, zmm9, zmm9)
        vaddpd(zmm19, zmm29, zmm29)
        vaddpd(zmm20, zmm10, zmm10)
        vaddpd(zmm21, zmm11, zmm11)
        vaddpd(zmm22, zmm26, zmm26)

        label(.TAIL)
        mov(var(k_left), rsi)                                  // i = k_left
        test(rsi, rsi)                                         // check i via logical AND
        je(.DPOSTACCUM)                                        // if i == 0, jump to post-accumulation

        label(.DLOOPKLEFT)                                     // k_left loop
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            vbroadcastsd( mem(rbx,r9,2),zmm30 )
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm10 )
            vfmadd231pd( zmm1,zmm30,zmm11 )
            vfmadd231pd( zmm2,zmm30,zmm26 )
            dec(rsi)                                           // i -= 1
        jne(.DLOOPKLEFT)                                       // iterate again if i != 0.


        label(.DPOSTACCUM)
        mov(var(alpha), rdx)                                   // load address of alpha
        vbroadcastsd(mem(rdx), zmm30)                           // broadcast alpha
        mov(var(beta), rax)                                    // load address of beta
        vbroadcastsd(mem(rax), zmm31)                           // broadcast beta

        // scale by alpha
        vmulpd( zmm30,zmm6,zmm6 )
        vmulpd( zmm30,zmm7,zmm7 )
        vmulpd( zmm30,zmm28,zmm28 )
        vmulpd( zmm30,zmm8,zmm8 )
        vmulpd( zmm30,zmm9,zmm9 )
        vmulpd( zmm30,zmm29,zmm29 )
        vmulpd( zmm30,zmm10,zmm10 )
        vmulpd( zmm30,zmm11,zmm11 )
        vmulpd( zmm30,zmm26,zmm26 )


        mov(var(rs_c), rsi)                                    // load rs_c
        lea(mem(, rsi, 8), rsi)                                // rsi = rs_c * sizeof(double)
        vxorpd(ymm2, ymm2, ymm2)
        vucomisd(xmm2, xmm31)                                   // set ZF if beta == 0
        je(.DBETAZERO)                                         // if ZF == 1, jump to beta == 0 case


        cmp(imm(8), rdi)                                       // set ZF if (8*cs_c) == 8


        jz(.DROWSTORED)                                        // jump to row storage case

        label(.DCOLSTORED)
        vfmadd231pd( mem(rcx),zmm31,zmm6)
        vmovupd( zmm6,(rcx))
        vfmadd231pd( 0x40(rcx),zmm31,zmm7)
        vmovupd( zmm7,0x40(rcx))
        vfmadd231pd( 0x80(rcx),zmm31,zmm28)
        vmovupd( zmm28,0x80(rcx))
        vfmadd231pd( mem(rcx,rdi,1),zmm31,zmm8)
        vmovupd( zmm8,(rcx,rdi,1))
        vfmadd231pd( 0x40(rcx,rdi,1),zmm31,zmm9)
        vmovupd( zmm9,0x40(rcx,rdi,1))
        vfmadd231pd( 0x80(rcx,rdi,1),zmm31,zmm29)
        vmovupd( zmm29,0x80(rcx,rdi,1))
        vfmadd231pd( mem(rcx,rdi,2),zmm31,zmm10)
        vmovupd( zmm10,(rcx,rdi,2))
        vfmadd231pd( 0x40(rcx,rdi,2),zmm31,zmm11)
        vmovupd( zmm11,0x40(rcx,rdi,2))
        vfmadd231pd( 0x80(rcx,rdi,2),zmm31,zmm26)
        vmovupd( zmm26,0x80(rcx,rdi,2))

        jmp(.DDONE)                                           // jump to end.

        label(.DROWSTORED)
        // r12 = 3*rs_c
        lea(mem(rsi,  rsi,  2), r12)
        // r13 = 5*rs_c
        lea(mem(r12, rsi,  2), r13)
        // rdx = 7*rs_c
        lea(mem(r12, rsi,  4), rdx)
        lea(mem(   , rsi, 8), r14)
        UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)

        vbroadcastsd(mem(rax), zmm31)
        UPDATE_MASKED_C
        //First 8x3 tile updated

        UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C
        //Second 8x3 tile updated

        UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C
        //Third 8x3 tile updated
        jmp(.DDONE)                                          // jump to end.


        label(.DBETAZERO)
        cmp(imm(8), rdi)                                     // set ZF if (8*cs_c) == 8

        jz(.DROWSTORBZ)                                      // jump to row storage case
        label(.DCOLSTORBZ)
        vmovupd( zmm6,(rcx))
        vmovupd( zmm7,0x40(rcx))
        vmovupd( zmm28,0x80(rcx))
        vmovupd( zmm8,(rcx,rdi,1))
        vmovupd( zmm9,0x40(rcx,rdi,1))
        vmovupd( zmm29,0x80(rcx,rdi,1))
        vmovupd( zmm10,(rcx,rdi,2))
        vmovupd( zmm11,0x40(rcx,rdi,2))
        vmovupd( zmm26,0x80(rcx,rdi,2))

        jmp(.DDONE)                                          // jump to end.


        label(.DROWSTORBZ)
        // r12 = 3*rs_c
        lea(mem(rsi,  rsi,  2), r12)
        // r13 = 5*rs_c
        lea(mem(r12, rsi,  2), r13)
        // rdx = 7*rs_c
        lea(mem(r12, rsi,  4), rdx)
        lea(mem(   , rsi, 8), r14)
        UNPACK_LO_HIGH(8, 6, 0, 1, 12, 10, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)

        vbroadcastsd(mem(rax), zmm31)
        UPDATE_MASKED_C_BZ
        //First 8x3 tile updated

        UNPACK_LO_HIGH(9, 7, 0, 1, 13, 11, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C_BZ
        //Second 8x3 tile updated

        UNPACK_LO_HIGH(29, 28, 0, 1, 27, 26, 2, 3)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C_BZ
        //Third 8x3 tile updated
        label(.DDONE)


        vzeroupper()

        end_asm(
          : // output operands (none)
          : // input operands
            [k_iter] "m" (k_iter),
            [k_left] "m" (k_left),
            [a]      "m" (a),
            [rs_a]   "m" (rs_a),
            [cs_a]   "m" (cs_a),
            [ps_a8]  "m" (ps_a8),
            [b]      "m" (b),
            [rs_b]   "m" (rs_b),
            [cs_b]   "m" (cs_b),
            [alpha]  "m" (alpha),
            [beta]   "m" (beta),
            [c]      "m" (c),
            [rs_c]   "m" (rs_c),
            [cs_c]   "m" (cs_c),
            [mask]   "m" (mask)
          : // register clobber list
            "rax", "rbx", "rcx", "rdx", "rsi", "rdi",
            "r8", "r9", "r10", "r11", "r12", "r13", "r14",
            "xmm2", "xmm31",
            "ymm2",
            "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5", "zmm6",
            "zmm7", "zmm8", "zmm9", "zmm10", "zmm11", "zmm12", "zmm13",
            "zmm14", "zmm15", "zmm16", "zmm17", "zmm18", "zmm19",
            "zmm20", "zmm21", "zmm22", "zmm26", "zmm27", "zmm28",
            "zmm29", "zmm30", "zmm31",
            "k2",
            "memory"
        )
    } //mloop

    consider_edge_cases:

    // Handle edge cases in the m dimension, if they exist.
    if (m_left)
    {
        const dim_t nr_cur = 3;
        const dim_t i_edge = m0 - ( dim_t )m_left;
        double *restrict cij = cbuf + i_edge * rs_c;
        double *restrict ai  = abuf + m_iter * ps_a;
        double *restrict bj  = bbuf;
        // covers the range 16 < m_left <= 24 by using masked load/store instructions
        if( 16 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_24x3(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
        // covers the range 8 < m_left <= 16 by using masked load/store instructions
        else if( 8 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_16x3(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
        // covers the range 0 < m_left <= 8 by using masked load/store instructions
        else if( 0 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_8x3(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
    }
    AOCL_DTL_TRACE_EXIT(AOCL_DTL_LEVEL_TRACE_7);
}

void bli_dgemmsup_cv_zen4_asm_24x2m
(
       conj_t    conja,
       conj_t    conjb,
       dim_t     m0,
       dim_t     n0,
       dim_t     k0,
       double*    restrict alpha,
       double*    restrict a, inc_t rs_a0, inc_t cs_a0,
       double*    restrict b, inc_t rs_b0, inc_t cs_b0,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c0, inc_t cs_c0,
       auxinfo_t* restrict data,
       cntx_t*    restrict cntx
     )
{
    AOCL_DTL_TRACE_ENTRY(AOCL_DTL_LEVEL_TRACE_7);
    double *abuf = a;
    double *bbuf = b;
    double *cbuf = c;

    // Typecast local copies of integers in case dim_t and inc_t are a
    // different size than is expected by load instructions.
    uint64_t m_iter = (uint64_t)m0 / 24;
    uint64_t m_left = (uint64_t)m0 % 24;

    uint64_t rs_a   = rs_a0;
    uint64_t cs_a   = cs_a0;
    uint64_t rs_b   = rs_b0;
    uint64_t cs_b   = cs_b0;
    uint64_t rs_c   = rs_c0;
    uint64_t cs_c   = cs_c0;

    uint64_t ps_a = bli_auxinfo_ps_a( data );
    uint64_t ps_a8  = ps_a * sizeof( double );

    uint64_t k_iter = (uint64_t)k0 / 8;
    uint64_t k_left = (uint64_t)k0 % 8;

    uint8_t mask = (0xff >> (0x8 - (n0 & 7))); // calculate mask based on n_left

    if ( m_iter == 0 ) goto consider_edge_cases;

    /* For one iteration of this loop, a block of MRxNR is computed
     * This loop moves along m-dimension of c matrix with steps of MR*rs_c.
     */
    for(dim_t m=0; m < m_iter; m++)
    {

        a = abuf + m * ps_a ; // Move to next MRXKC in MCXKC (where MC>=MR)
        b = bbuf;  //Same KCXNR is used across different MRXKC in MCXKC
        c = cbuf + m * rs_c * 24; // Move to next MRxNR in MCxNR (where MC >= MR)

        // -------------------------------------------------------------------------
        begin_asm()

        mov(var(mask), rdx)             // load mask
        kmovw(edx, k(2))                // move mask to k2 register
        mov(var(a), rax)                // load address of a
        mov(var(cs_a), r10)             // load cs_a
        mov(var(b), rbx)                // load address of b
        mov(var(rs_b), r8)              // load rs_b
        mov(var(cs_b), r9)              // load cs_b
        mov(var(c), rcx)                // load address of c
        mov(var(cs_c), rdi)             // load cs_c
        lea(mem(, r8, 8), r8)           // rs_b *= sizeof(double)
        lea(mem(, r9, 8), r9)           // cs_b *= sizeof(double)
        lea(mem(, r10, 8), r10)         // cs_a *= sizeof(double)
        lea(mem(, rdi, 8), rdi)         // cs_c *= sizeof(double)
        lea(mem(rcx, 7*8), rdx)         // C for prefetching
        mov(var(ps_a8), r14)            // panel stride of A
        lea(mem(rax, r14, 1, 7*8), r14) // prefetch next panel of A
        lea(mem(rbx, r8, 8, 7*8), r11)  // r11 = rbx + 8*rs_b(B for prefetching)

        /* Register usage: zmm0-5 are used to load A matrix
         *                 zmm6-29 are used for accumulation
         *                 zmm30-31 are used for broadcasting B matrix
         */

        // zero out all accumulation registers
        vxorpd(zmm6, zmm6, zmm6)
        vxorpd(zmm7, zmm7, zmm7)
        vxorpd(zmm28, zmm28, zmm28)
        vxorpd(zmm8, zmm8, zmm8)
        vxorpd(zmm9, zmm9, zmm9)
        vxorpd(zmm29, zmm29, zmm29)
        vxorpd(zmm14, zmm14, zmm14)
        vxorpd(zmm15, zmm15, zmm15)
        vxorpd(zmm16, zmm16, zmm16)
        vxorpd(zmm17, zmm17, zmm17)
        vxorpd(zmm18, zmm18, zmm18)
        vxorpd(zmm19, zmm19, zmm19)

        // K is unrolled by 8 to facilitate prefetch of B
        // Assuming B to be col-stored, for each iteration of K,
        //one cacheline of B_next is prefetched where b_next = b + (unroll)*rs_b
        label(.DLOOPKITER)                                     // main loop
        mov(var(k_iter), rsi)                                  // i = k_iter
        sub(imm( 2+TAIL_NITER), rsi)                           // i -= NR + TAIL_NITER
        jle(.PREFETCHLOOP)                                     // jump if i <= 0

        /**
         * This edge kernel uses two separate vector register bank
         * to hold fma result.
         * Once the K loop is completed these two vector register banks
         * are added together and final result is available in one
         * register bank.
         * Here odd iterations uses vector register zmm6, zmm7, zmm28,
         * zmm8, zmm9, zmm29 to hold fma result.
         * While even iterations uses zmm14, zmm15, zmm16, zmm17, zmm18
         * zmm19, zmm20, zmm21 to hold fma result.
         * At the end of K loop, these two banks are added together and
         * final result is available in vector register zmm6, zmm7, zmm28,
         * zmm8, zmm9, zmm29.
         */

        label(.LOOP1)

            // ---------------------------------- iteration 1

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )

            // ---------------------------------- iteration 2

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )

            // ---------------------------------- iteration 3

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )

            // ---------------------------------- iteration 4

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )

            // ---------------------------------- iteration 5

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )

            // ---------------------------------- iteration 6

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )

            // ---------------------------------- iteration 7

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )

            // ---------------------------------- iteration 8

            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            dec(rsi)                                           // i -= 1
        jnz(.LOOP1)                                            // iterate again if i != 0.

        label(.PREFETCHLOOP)
        add(imm(2), rsi)                                       // i += NR
        jle(.TAILITER)                                         // jump if i <= 0.

        label(.LOOP2)

            // ---------------------------------- iteration 1
            prefetchw0( mem(rdx))                              // prefetch C
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )

            // ---------------------------------- iteration 2
            prefetchw0( mem(rdx, 64))                          // prefetch C
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )

            // ---------------------------------- iteration 3
            prefetchw0( mem(rdx, 128))                        // prefetch C
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )

            // ---------------------------------- iteration 4
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )

            // ---------------------------------- iteration 5
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )

            // ---------------------------------- iteration 6
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )

            // ---------------------------------- iteration 7
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )

            // ---------------------------------- iteration 8
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            lea(mem(rdx, rdi, 1), rdx)                         // C += cs_c
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            sub(imm(1), rsi)                                   // i -= 1
        jnz(.LOOP2)                                            // iterate again if i != 0.
        label(.TAILITER)
        add(imm(TAIL_NITER), rsi)                              // i += TAIL_NITER
        jle(.TAIL)                                             // jump if i <= 0

        label(.LOOP3)

            // ---------------------------------- iteration 1
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )

            // ---------------------------------- iteration 2
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11,r9,1) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )

            // ---------------------------------- iteration 3
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )

            // ---------------------------------- iteration 4
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )

            // ---------------------------------- iteration 5
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )

            // ---------------------------------- iteration 6
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )

            // ---------------------------------- iteration 7
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )

            // ---------------------------------- iteration 8
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm31,zmm17 )
            vfmadd231pd( zmm4,zmm31,zmm18 )
            vfmadd231pd( zmm5,zmm31,zmm19 )
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            dec(rsi)                                           // i -= 1
        jnz(.LOOP3)                                            // iterate again if i != 0.

        vaddpd(zmm14, zmm6, zmm6)
        vaddpd(zmm15, zmm7, zmm7)
        vaddpd(zmm16, zmm28, zmm28)
        vaddpd(zmm17, zmm8, zmm8)
        vaddpd(zmm18, zmm9, zmm9)
        vaddpd(zmm19, zmm29, zmm29)

        label(.TAIL)
        mov(var(k_left), rsi)                                  // i = k_left
        test(rsi, rsi)                                         // check i via logical AND
        je(.DPOSTACCUM)                                        // if i == 0, jump to post-accumulation

        label(.DLOOPKLEFT)                                     // k_left loop
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            vbroadcastsd( mem(rbx,r9,1),zmm31 )
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm31,zmm8 )
            vfmadd231pd( zmm1,zmm31,zmm9 )
            vfmadd231pd( zmm2,zmm31,zmm29 )
            dec(rsi)                                           // i -= 1
        jne(.DLOOPKLEFT)                                       // iterate again if i != 0.


        label(.DPOSTACCUM)
        mov(var(alpha), rdx)                                   // load address of alpha
        vbroadcastsd(mem(rdx), zmm30)                           // broadcast alpha
        mov(var(beta), rax)                                    // load address of beta
        vbroadcastsd(mem(rax), zmm31)                           // broadcast beta

        // scale by alpha
        vmulpd( zmm30,zmm6,zmm6 )
        vmulpd( zmm30,zmm7,zmm7 )
        vmulpd( zmm30,zmm28,zmm28 )
        vmulpd( zmm30,zmm8,zmm8 )
        vmulpd( zmm30,zmm9,zmm9 )
        vmulpd( zmm30,zmm29,zmm29 )


        mov(var(rs_c), rsi)                                    // load rs_c
        lea(mem(, rsi, 8), rsi)                                // rsi = rs_c * sizeof(double)
        vxorpd(ymm2, ymm2, ymm2)
        vucomisd(xmm2, xmm31)                                   // set ZF if beta == 0
        je(.DBETAZERO)                                         // if ZF == 1, jump to beta == 0 case


        cmp(imm(8), rdi)                                       // set ZF if (8*cs_c) == 8


        jz(.DROWSTORED)                                        // jump to row storage case

        label(.DCOLSTORED)
        vfmadd231pd( mem(rcx),zmm31,zmm6)
        vmovupd( zmm6,(rcx))
        vfmadd231pd( 0x40(rcx),zmm31,zmm7)
        vmovupd( zmm7,0x40(rcx))
        vfmadd231pd( 0x80(rcx),zmm31,zmm28)
        vmovupd( zmm28,0x80(rcx))
        vfmadd231pd( mem(rcx,rdi,1),zmm31,zmm8)
        vmovupd( zmm8,(rcx,rdi,1))
        vfmadd231pd( 0x40(rcx,rdi,1),zmm31,zmm9)
        vmovupd( zmm9,0x40(rcx,rdi,1))
        vfmadd231pd( 0x80(rcx,rdi,1),zmm31,zmm29)
        vmovupd( zmm29,0x80(rcx,rdi,1))

        jmp(.DDONE)                                           // jump to end.

        label(.DROWSTORED)
        // r12 = 3*rs_c
        lea(mem(rsi,  rsi,  2), r12)
        // r13 = 5*rs_c
        lea(mem(r12, rsi,  2), r13)
        // rdx = 7*rs_c
        lea(mem(r12, rsi,  4), rdx)
        lea(mem(   , rsi, 8), r14)
        vunpcklpd( zmm8,  zmm6,  zmm0)
        vunpckhpd( zmm8,  zmm6,  zmm1)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)

        vbroadcastsd(mem(rax), zmm31)
        UPDATE_MASKED_C
        //First 8x2 tile updated

        vunpcklpd( zmm9,  zmm7,  zmm0)
        vunpckhpd( zmm9,  zmm7,  zmm1)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C
        //Second 8x2 tile updated

        vunpcklpd( zmm29,  zmm28,  zmm0)
        vunpckhpd( zmm29,  zmm28,  zmm1)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)
        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C
        //Third 8x2 tile updated
        jmp(.DDONE)                                          // jump to end.


        label(.DBETAZERO)
        cmp(imm(8), rdi)                                     // set ZF if (8*cs_c) == 8

        jz(.DROWSTORBZ)                                      // jump to row storage case
        label(.DCOLSTORBZ)
        vmovupd( zmm6,(rcx))
        vmovupd( zmm7,0x40(rcx))
        vmovupd( zmm28,0x80(rcx))
        vmovupd( zmm8,(rcx,rdi,1))
        vmovupd( zmm9,0x40(rcx,rdi,1))
        vmovupd( zmm29,0x80(rcx,rdi,1))

        jmp(.DDONE)                                          // jump to end.


        label(.DROWSTORBZ)
        // r12 = 3*rs_c
        lea(mem(rsi,  rsi,  2), r12)
        // r13 = 5*rs_c
        lea(mem(r12, rsi,  2), r13)
        // rdx = 7*rs_c
        lea(mem(r12, rsi,  4), rdx)
        lea(mem(   , rsi, 8), r14)
        vunpcklpd( zmm8,  zmm6,  zmm0)
        vunpckhpd( zmm8,  zmm6,  zmm1)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)

        UPDATE_MASKED_C_BZ
        //First 8x2 tile updated

        vunpcklpd( zmm9,  zmm7,  zmm0)
        vunpckhpd( zmm9,  zmm7,  zmm1)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C_BZ
        //Second 8x2 tile updated

        vunpcklpd( zmm29,  zmm28,  zmm0)
        vunpckhpd( zmm29,  zmm28,  zmm1)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C_BZ
        //Third 8x2 tile updated
        label(.DDONE)


        vzeroupper()

        end_asm(
          : // output operands (none)
          : // input operands
            [k_iter] "m" (k_iter),
            [k_left] "m" (k_left),
            [a]      "m" (a),
            [rs_a]   "m" (rs_a),
            [cs_a]   "m" (cs_a),
            [ps_a8]  "m" (ps_a8),
            [b]      "m" (b),
            [rs_b]   "m" (rs_b),
            [cs_b]   "m" (cs_b),
            [alpha]  "m" (alpha),
            [beta]   "m" (beta),
            [c]      "m" (c),
            [rs_c]   "m" (rs_c),
            [cs_c]   "m" (cs_c),
            [mask]   "m" (mask)
          : // register clobber list
            "rax", "rbx", "rcx", "rdx", "rsi", "rdi",
            "r8", "r9", "r10", "r11", "r12", "r13", "r14",
            "xmm2", "xmm31",
            "ymm2",
            "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5", "zmm6",
            "zmm7", "zmm8", "zmm9", "zmm10", "zmm12", "zmm14", "zmm15",
            "zmm16", "zmm17", "zmm18", "zmm19", "zmm28", "zmm29",
            "zmm30", "zmm31",
            "k2",
            "memory"
        )
    } //mloop

    consider_edge_cases:

    // Handle edge cases in the m dimension, if they exist.
    if (m_left)
    {
        const dim_t nr_cur = 2;
        const dim_t i_edge = m0 - ( dim_t )m_left;
        double *restrict cij = cbuf + i_edge * rs_c;
        double *restrict ai  = abuf + m_iter * ps_a;
        double *restrict bj  = bbuf;
        // covers the range 16 < m_left <= 24 by using masked load/store instructions
        if( 16 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_24x2(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
        // covers the range 8 < m_left <= 16 by using masked load/store instructions
        else if( 8 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_16x2(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
        // covers the range 0 < m_left <= 8 by using masked load/store instructions
        else if( 0 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_8x2(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
    }
    AOCL_DTL_TRACE_EXIT(AOCL_DTL_LEVEL_TRACE_7);
}

void bli_dgemmsup_cv_zen4_asm_24x1m
(
       conj_t    conja,
       conj_t    conjb,
       dim_t     m0,
       dim_t     n0,
       dim_t     k0,
       double*    restrict alpha,
       double*    restrict a, inc_t rs_a0, inc_t cs_a0,
       double*    restrict b, inc_t rs_b0, inc_t cs_b0,
       double*    restrict beta,
       double*    restrict c, inc_t rs_c0, inc_t cs_c0,
       auxinfo_t* restrict data,
       cntx_t*    restrict cntx
     )
{
    AOCL_DTL_TRACE_ENTRY(AOCL_DTL_LEVEL_TRACE_7);
    double *abuf = a;
    double *bbuf = b;
    double *cbuf = c;

    // Typecast local copies of integers in case dim_t and inc_t are a
    // different size than is expected by load instructions.
    uint64_t m_iter = (uint64_t)m0 / 24;
    uint64_t m_left = (uint64_t)m0 % 24;

    uint64_t rs_a   = rs_a0;
    uint64_t cs_a   = cs_a0;
    uint64_t rs_b   = rs_b0;
    uint64_t cs_b   = cs_b0;
    uint64_t rs_c   = rs_c0;
    uint64_t cs_c   = cs_c0;

    uint64_t ps_a = bli_auxinfo_ps_a( data );
    uint64_t ps_a8  = ps_a * sizeof( double );

    uint64_t k_iter = (uint64_t)k0 / 8;
    uint64_t k_left = (uint64_t)k0 % 8;

    uint8_t mask = (0xff >> (0x8 - (n0 & 7))); // calculate mask based on n_left

    if ( m_iter == 0 ) goto consider_edge_cases;

    /* For one iteration of this loop, a block of MRxNR is computed
     * This loop moves along m-dimension of c matrix with steps of MR*rs_c.
     */
    for(dim_t m=0; m < m_iter; m++)
    {

        a = abuf + m * ps_a ; // Move to next MRXKC in MCXKC (where MC>=MR)
        b = bbuf;  //Same KCXNR is used across different MRXKC in MCXKC
        c = cbuf + m * rs_c * 24; // Move to next MRxNR in MCxNR (where MC >= MR)

        // -------------------------------------------------------------------------
        begin_asm()

        mov(var(mask), rdx)             // load mask
        kmovw(edx, k(2))                // move mask to k2 register
        mov(var(a), rax)                // load address of a
        mov(var(cs_a), r10)             // load cs_a
        mov(var(b), rbx)                // load address of b
        mov(var(rs_b), r8)              // load rs_b
        mov(var(cs_b), r9)              // load cs_b
        mov(var(c), rcx)                // load address of c
        mov(var(cs_c), rdi)             // load cs_c
        lea(mem(, r8, 8), r8)           // rs_b *= sizeof(double)
        lea(mem(, r9, 8), r9)           // cs_b *= sizeof(double)
        lea(mem(, r10, 8), r10)         // cs_a *= sizeof(double)
        lea(mem(, rdi, 8), rdi)         // cs_c *= sizeof(double)
        lea(mem(rcx, 7*8), rdx)         // C for prefetching
        mov(var(ps_a8), r14)            // panel stride of A
        lea(mem(rax, r14, 1, 7*8), r14) // prefetch next panel of A
        lea(mem(rbx, r8, 8, 7*8), r11)  // r11 = rbx + 8*rs_b(B for prefetching)

        /* Register usage: zmm0-5 are used to load A matrix
         *                 zmm6-29 are used for accumulation
         *                 zmm30-31 are used for broadcasting B matrix
         */

        // zero out all accumulation registers
        vxorpd(zmm6, zmm6, zmm6)
        vxorpd(zmm7, zmm7, zmm7)
        vxorpd(zmm28, zmm28, zmm28)
        vxorpd(zmm14, zmm14, zmm14)
        vxorpd(zmm15, zmm15, zmm15)
        vxorpd(zmm16, zmm16, zmm16)
        // K is unrolled by 8 to facilitate prefetch of B
        // Assuming B to be col-stored, for each iteration of K,
        //one cacheline of B_next is prefetched where b_next = b + (unroll)*rs_b
        label(.DLOOPKITER)                                     // main loop
        mov(var(k_iter), rsi)                                  // i = k_iter
        sub(imm( 1+TAIL_NITER), rsi)                           // i -= NR + TAIL_NITER
        jle(.PREFETCHLOOP)                                     // jump if i <= 0

        /**
         * This edge kernel uses two separate vector register bank
         * to hold fma result.
         * Once the K loop is completed these two vector register banks
         * are added together and final result is available in one
         * register bank.
         * Here odd iterations uses vector register zmm6, zmm7, zmm28,
         * to hold fma result.
         * While even iterations uses zmm14, zmm15, zmm16 to hold fma
         * result.
         * At the end of K loop, these two banks are added together and
         * final result is available in vector register zmm6, zmm7, zmm28,
         */

        label(.LOOP1)

            // ---------------------------------- iteration 1

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )

            // ---------------------------------- iteration 2

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )

            // ---------------------------------- iteration 3

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )

            // ---------------------------------- iteration 4

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )

            // ---------------------------------- iteration 5

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )

            // ---------------------------------- iteration 6

            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )

            // ---------------------------------- iteration 7

            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )

            // ---------------------------------- iteration 8

            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            dec(rsi)                                           // i -= 1
        jnz(.LOOP1)                                            // iterate again if i != 0.

        label(.PREFETCHLOOP)
        add(imm(1), rsi)                                       // i += NR
        jle(.TAILITER)                                         // jump if i <= 0.

        label(.LOOP2)

            // ---------------------------------- iteration 1
            prefetchw0( mem(rdx))                              // prefetch C
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )

            // ---------------------------------- iteration 2
            prefetchw0( mem(rdx, 64))                          // prefetch C
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )

            // ---------------------------------- iteration 3
            prefetchw0( mem(rdx, 128))                        // prefetch C
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )

            // ---------------------------------- iteration 4
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )

            // ---------------------------------- iteration 5
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )

            // ---------------------------------- iteration 6
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )

            // ---------------------------------- iteration 7
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )

            // ---------------------------------- iteration 8
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            lea(mem(rdx, rdi, 1), rdx)                         // C += cs_c
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            sub(imm(1), rsi)                                   // i -= 1
        jnz(.LOOP2)                                            // iterate again if i != 0.
        label(.TAILITER)
        add(imm(TAIL_NITER), rsi)                              // i += TAIL_NITER
        jle(.TAIL)                                             // jump if i <= 0

        label(.LOOP3)

            // ---------------------------------- iteration 1
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            prefetch( 0,mem(r11) )                             // prefetch B
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )

            // ---------------------------------- iteration 2
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )

            // ---------------------------------- iteration 3
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )

            // ---------------------------------- iteration 4
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )

            // ---------------------------------- iteration 5
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )

            // ---------------------------------- iteration 6
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )

            // ---------------------------------- iteration 7
            vmovupd( mem(rax),zmm3 )                           // load A
            vmovupd( 0x40(rax),zmm4 )
            vmovupd( 0x80(rax),zmm5 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )

            // ---------------------------------- iteration 8
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm3,zmm30,zmm14 )
            vfmadd231pd( zmm4,zmm30,zmm15 )
            vfmadd231pd( zmm5,zmm30,zmm16 )
            lea(mem(r11,r8,8), r11)                            // b_next += 8*rs_b
            dec(rsi)                                           // i -= 1
        jnz(.LOOP3)                                            // iterate again if i != 0.

        vaddpd(zmm14, zmm6, zmm6)
        vaddpd(zmm15, zmm7, zmm7)
        vaddpd(zmm16, zmm28, zmm28)

        label(.TAIL)
        mov(var(k_left), rsi)                                  // i = k_left
        test(rsi, rsi)                                         // check i via logical AND
        je(.DPOSTACCUM)                                        // if i == 0, jump to post-accumulation

        label(.DLOOPKLEFT)                                     // k_left loop
            vmovupd( mem(rax),zmm0 )                           // load A
            vmovupd( 0x40(rax),zmm1 )
            vmovupd( 0x80(rax),zmm2 )
            add( r10,rax )                                     // a += cs_a
            //prefetch 24 elements(3 cachelines) of the corresponding column in next panel of A
            prefetch( 1,mem(r14) )
            prefetch( 1,0x40(r14) )
            prefetch( 1,0x80(r14) )
            add( r10,r14 )                                     // a_next += cs_a
            vbroadcastsd( mem(rbx),zmm30 )
            add( r8,rbx )                                     // b += rs_b
            vfmadd231pd( zmm0,zmm30,zmm6 )
            vfmadd231pd( zmm1,zmm30,zmm7 )
            vfmadd231pd( zmm2,zmm30,zmm28 )
            dec(rsi)                                           // i -= 1
        jne(.DLOOPKLEFT)                                       // iterate again if i != 0.


        label(.DPOSTACCUM)
        mov(var(alpha), rdx)                                   // load address of alpha
        vbroadcastsd(mem(rdx), zmm30)                           // broadcast alpha
        mov(var(beta), rax)                                    // load address of beta
        vbroadcastsd(mem(rax), zmm31)                           // broadcast beta

        // scale by alpha
        vmulpd( zmm30,zmm6,zmm6 )
        vmulpd( zmm30,zmm7,zmm7 )
        vmulpd( zmm30,zmm28,zmm28 )


        mov(var(rs_c), rsi)                                    // load rs_c
        lea(mem(, rsi, 8), rsi)                                // rsi = rs_c * sizeof(double)
        vxorpd(ymm2, ymm2, ymm2)
        vucomisd(xmm2, xmm31)                                   // set ZF if beta == 0
        je(.DBETAZERO)                                         // if ZF == 1, jump to beta == 0 case


        cmp(imm(8), rdi)                                       // set ZF if (8*cs_c) == 8


        jz(.DROWSTORED)                                        // jump to row storage case

        label(.DCOLSTORED)
        vfmadd231pd( mem(rcx),zmm31,zmm6)
        vmovupd( zmm6,(rcx))
        vfmadd231pd( 0x40(rcx),zmm31,zmm7)
        vmovupd( zmm7,0x40(rcx))
        vfmadd231pd( 0x80(rcx),zmm31,zmm28)
        vmovupd( zmm28,0x80(rcx))

        jmp(.DDONE)                                           // jump to end.

        label(.DROWSTORED)
        // r12 = 3*rs_c
        lea(mem(rsi,  rsi,  2), r12)
        // r13 = 5*rs_c
        lea(mem(r12, rsi,  2), r13)
        // rdx = 7*rs_c
        lea(mem(r12, rsi,  4), rdx)
        lea(mem(   , rsi, 8), r14)
        vunpcklpd( zmm8,  zmm6,  zmm0)
        vunpckhpd( zmm8,  zmm6,  zmm1)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)

        vbroadcastsd(mem(rax), zmm31)
        UPDATE_MASKED_C
        //First 8x1 tile updated

        vunpcklpd( zmm9,  zmm7,  zmm0)
        vunpckhpd( zmm9,  zmm7,  zmm1)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C
        //Second 8x1 tile updated

        vunpcklpd( zmm29,  zmm28,  zmm0)
        vunpckhpd( zmm29,  zmm28,  zmm1)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C
        //Third 8x1 tile updated
        jmp(.DDONE)                                          // jump to end.


        label(.DBETAZERO)
        cmp(imm(8), rdi)                                     // set ZF if (8*cs_c) == 8

        jz(.DROWSTORBZ)                                      // jump to row storage case
        label(.DCOLSTORBZ)
        vmovupd( zmm6,(rcx))
        vmovupd( zmm7,0x40(rcx))
        vmovupd( zmm28,0x80(rcx))

        jmp(.DDONE)                                          // jump to end.


        label(.DROWSTORBZ)
        // r12 = 3*rs_c
        lea(mem(rsi,  rsi,  2), r12)
        // r13 = 5*rs_c
        lea(mem(r12, rsi,  2), r13)
        // rdx = 7*rs_c
        lea(mem(r12, rsi,  4), rdx)
        lea(mem(   , rsi, 8), r14)
        vunpcklpd( zmm8,  zmm6,  zmm0)
        vunpckhpd( zmm8,  zmm6,  zmm1)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 30, 31)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 30, 4, 5, 12, 31, 6, 8)

        UPDATE_MASKED_C_BZ
        //First 8x1 tile updated

        vunpcklpd( zmm9,  zmm7,  zmm0)
        vunpckhpd( zmm9,  zmm7,  zmm1)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C_BZ
        //Second 8x1 tile updated

        vunpcklpd( zmm29,  zmm28,  zmm0)
        vunpckhpd( zmm29,  zmm28,  zmm1)
        SHUFFLE_DATA(2, 0, 4, 5, 3, 1, 7, 9)

        SHUFFLE_DATA(2, 0, 6, 8, 3, 1, 10, 12)

        SHUFFLE_DATA(6, 4, 0, 1, 8, 5, 2, 3)
        SHUFFLE_DATA(10, 7, 4, 5, 12, 9, 6, 8)

        UPDATE_MASKED_C_BZ
        //Third 8x1 tile updated
        label(.DDONE)


        vzeroupper()

        end_asm(
          : // output operands (none)
          : // input operands
            [k_iter] "m" (k_iter),
            [k_left] "m" (k_left),
            [a]      "m" (a),
            [rs_a]   "m" (rs_a),
            [cs_a]   "m" (cs_a),
            [ps_a8]  "m" (ps_a8),
            [b]      "m" (b),
            [rs_b]   "m" (rs_b),
            [cs_b]   "m" (cs_b),
            [alpha]  "m" (alpha),
            [beta]   "m" (beta),
            [c]      "m" (c),
            [rs_c]   "m" (rs_c),
            [cs_c]   "m" (cs_c),
            [mask]   "m" (mask)
          : // register clobber list
            "rax", "rbx", "rcx", "rdx", "rsi", "rdi",
            "r8", "r9", "r10", "r11", "r12", "r13", "r14",
            "xmm2", "xmm31",
            "ymm2",
            "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5", "zmm6",
            "zmm7", "zmm8", "zmm9", "zmm10", "zmm12", "zmm14", "zmm15",
            "zmm16", "zmm18", "zmm28", "zmm29", "zmm30", "zmm31",
            "k2",
            "memory"
        )
    } //mloop

    consider_edge_cases:

    // Handle edge cases in the m dimension, if they exist.
    if (m_left)
    {
        const dim_t nr_cur = 1;
        const dim_t i_edge = m0 - ( dim_t )m_left;
        double *restrict cij = cbuf + i_edge * rs_c;
        double *restrict ai  = abuf + m_iter * ps_a;
        double *restrict bj  = bbuf;
        // covers the range 16 < m_left <= 24 by using masked load/store instructions
        if( 16 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_24x1(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
        // covers the range 8 < m_left <= 16 by using masked load/store instructions
        else if( 8 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_16x1(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
        // covers the range 0 < m_left <= 8 by using masked load/store instructions
        else if( 0 < m_left )
        {
            bli_dgemmsup_rv_zen4_asm_8x1(
              conja, conjb, m_left, nr_cur, k0,
              alpha, ai, rs_a0, cs_a0, bj, rs_b0, cs_b0,
              beta, cij, rs_c0, cs_c0, data, cntx);
        }
    }
    AOCL_DTL_TRACE_EXIT(AOCL_DTL_LEVEL_TRACE_7);
}
